###################################
# to release transfer input file held jobs
condor_release fliphilipp -constraint "HoldReasonCode==13"

###################################
# to place running jobs on hold
condor_hold <cluster_id> -constraint "JobStatus==2" -reason "Job was held manually."

###################################
# to transfer data back
# make sure to combine stats files before and delete outs/logs/errs (zip before if want to keep) 
scp -r fliphilipp@ap21.uc.osg-htc.org:/home/fliphilipp/GlacierLakeDetectionICESat2 /Volumes/nox/Philipp/<folder_name>
scp fliphilipp@ap21.uc.osg-htc.org:/ospool/ap21/data/fliphilipp/gld_data/gld2except91granules.tar.gz /Volumes/nox/Philipp/IceLakesRun2

###################################
# to combine all output stats into one file
filename_out=stats_combined.csv; >| $filename_out; for f in detection_out_stat/stat*.csv; do cat "${f}" >> $filename_out; done

###################################
# to zip files to get them transferred
tar -czvf name-of-archive.tar.gz /path/to/directory-or-file
# unzip
tar -xvzf name-of-archive.tar.gz

# in home directory:
tar -czvf /ospool/ap21/data/fliphilipp/gld_data/gld2except91granules.tar.gz GlacierLakeDetectionICESat2

# on lumos / nox
tar -czvf gld2except91granules_renamed.tar.gz GlacierLakeDetectionICESat2

###################################
# to completely clean up outputs 
rm detection_out_data/* detection_out_stat/* detection_out_plot/* logs/* errs/* outs/*

# or if too many files
rm -r detection_out_data
rm -r detection_out_plot 
rm -r detection_out_stat
rm -r logs
rm -r errs
rm -r outs
mkdir detection_out_data
mkdir detection_out_plot 
mkdir detection_out_stat
mkdir logs
mkdir errs
mkdir outs


###################################
condor_q
grep 'Success!!!!!' outs/*-123983_*.out | wc -l
grep 'No success.....' outs/*-123983_*.out | wc -l

condor_q
grep 'Success!!!!!' outs/*-123993_*.out | wc -l
grep 'No success.....' outs/*-123993_*.out | wc -l

###################################

# specify cluster and name
clusterid=114211
clustername=greenland1

# condor_q overview, and check for unique hold resons
condor_q $clusterid
condor_q $clusterid -held -af HoldReason | wc -l | xargs printf "%s total jobs on hold\n"
condor_q $clusterid -held -af HoldReason | cut -c-11 | sort | uniq -c

# define hold reasons to grep for (might need to add others if they come up)
declare -a HoldReasons=(
 "memory usage exceeded request_memory"
 "The job (shadow) restarted too many times"
 "The job restarted too many times"
 "Transfer input files failure"
)

filename_out=held_jobs/$clustername-$clusterid.csv
>| $filename_out
# print number of jobs that fulfill the 
totalcounter=0
for holdreason in "${HoldReasons[@]}"; do
    counts=$(condor_q $clusterid -hold | head -n 100 | grep "$holdreason" | wc -l);
    printf "%5i - $holdreason\n" $counts;
    totalcounter=$(expr $totalcounter + $counts);
    condor_q $clusterid -held | head -n 100 | grep "$holdreason" | awk -F'[. ]+' '{printf "logs/*%s*-%s.*\n",$1,$2;}' | while read x; do ls $x | xargs printf "%s," >> $filename_out; echo "$holdreason" >> $filename_out; done  
done

printf "%5i - TOTAL\n\n" $totalcounter;
echo $filename_out
cat $filename_out | wc -l | xargs printf "%s lines in file\n"
cat $filename_out


############################################################


# specify cluster and name
clusterid=114211
clustername=greenland1

# condor_q overview, and check for unique hold resons
condor_q $clusterid
condor_q $clusterid -held -af HoldReason | wc -l | xargs printf "%s total jobs on hold\n"
condor_q $clusterid -held -af HoldReason | cut -c-11 | sort | uniq -c

# define hold reasons to grep for (might need to add others if they come up)
declare -a HoldReasons=(
 "memory usage exceeded request_memory"
 "The job (shadow) restarted too many times"
 "The job restarted too many times"
 "Transfer input files failure"
)

filename_out=held_jobs/$clustername-$clusterid.csv
>| $filename_out
# print number of jobs that fulfill the 
totalcounter=0
for holdreason in "${HoldReasons[@]}"; do
    counts=$(condor_q $clusterid -hold | head -n 100 | grep "$holdreason" | wc -l);
    printf "%5i - $holdreason\n" $counts;
    totalcounter=$(expr $totalcounter + $counts);
    condor_q $clusterid -held | head -n 100 | grep "$holdreason" | awk -F'[. ]+' '{printf "errs/*%s*-%s.*\n",$1,$2;}' | while read x; do cat $x | grep "/run_py.sh" | awk -F'['"'"']+' '{printf "%s,%s,",$4,$6}' >> $filename_out; echo $holdreason >> $filename_out; done  
done

printf "%5i - TOTAL\n\n" $totalcounter;
echo $filename_out
cat $filename_out | wc -l | xargs printf "%s lines in file\n"



# print number of jobs that fulfill the 
totalcounter=0
for holdreason in "${HoldReasons[@]}"; do
    counts=$(condor_q $clusterid -hold | head -n 100 | grep "$holdreason" | wc -l);
    printf "%5i - $holdreason\n" $counts;
    totalcounter=$(expr $totalcounter + $counts);
    condor_q $clusterid -hold | grep "$holdreason" | awk -F'[. ]+' '{printf "errs/*%s*-%s.*\n",$1,$2;}' | while read x; do cat $x | grep "/srv//run_py.sh" | awk -F'['"'"']+' '{printf "%s,%s,",$4,$6}' >> $filename_out; echo $holdreason >> $filename_out; done  
done
printf "%5i - TOTAL\n\n" $totalcounter;



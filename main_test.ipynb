{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220a514b-f6f6-464e-a1a1-2deed37ca3dd",
   "metadata": {},
   "source": [
    "# imports of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ef813e-efff-43d2-873a-02d59d3c9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Philipp Arndt, UC San Diego / Scripps Institution of Oceanography\n",
    "# \n",
    "# intended for use on OSG OSPool, called in run_py.sh, which is called in a submit file \n",
    "# submit file is based on a granule list queried locally in make_granule_list.ipynb \n",
    "# see examples for submit files in: HTCondor_submit/ \n",
    "# see examples for granule lists in:  granule_lists/\n",
    "# \n",
    "# run locally with: \n",
    "# $ conda activate icelakes-env\n",
    "# $ python3 detect_lakes.py --granule <granule_producer_id> --polygon geojsons/<polygon_name.geojson>\n",
    "# \n",
    "# a call that returns a bunch of lakes\n",
    "# $ python3 detect_lakes.py --granule ATL03_20220714010847_03381603_006_02.h5 --polygon geojsons/simplified_GRE_2500_CW.geojson\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import pickle\n",
    "import subprocess\n",
    "import traceback\n",
    "import numpy as np\n",
    "import icelakes\n",
    "from icelakes.utilities import encedc, decedc, get_size\n",
    "from icelakes.nsidc import download_granule, edc\n",
    "from icelakes.detection import read_atl03, detect_lakes, melt_lake\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Test script to print some stats for a given ICESat-2 ATL03 granule.')\n",
    "parser.add_argument('--granule', type=str, default='ATL03_20220714010847_03381603_006_02.h5',\n",
    "                    help='The producer_id of the input ATL03 granule')\n",
    "parser.add_argument('--polygon', type=str, default='geojsons/simplified_GRE_2000_CW.geojson',\n",
    "                    help='The file path of a geojson file for spatial subsetting') # geojsons/west_greenland.geojson\n",
    "parser.add_argument('--is2_data_dir', type=str, default='IS2data',\n",
    "                    help='The directory into which to download ICESat-2 granules')\n",
    "parser.add_argument('--download_gtxs', type=str, default='all',\n",
    "                    help='String value or list of gtx names to download, also accepts \"all\"')\n",
    "parser.add_argument('--out_data_dir', type=str, default='detection_out_data',\n",
    "                    help='The directory to which to write the output data')\n",
    "parser.add_argument('--out_plot_dir', type=str, default='detection_out_plot',\n",
    "                    help='The directory to which to write the output plots')\n",
    "parser.add_argument('--out_stat_dir', type=str, default='detection_out_stat',\n",
    "                    help='The directory to which to write the granule stats')\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70996468-8d28-4160-adce-72446f4731e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args.granule = 'ATL03_20200225170832_09310610_006_01.h5'\n",
    "#args.polygon = 'geojsons/simplified_ANT_1500_East_D-Dp.geojson'\n",
    "# ATL03_20200225170832_09310610_006_01.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47ae168-9318-4e02-9bee-3d8b1e8e7f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(granule='ATL03_20220714010847_03381603_006_02.h5', polygon='geojsons/simplified_GRE_2000_CW.geojson', is2_data_dir='IS2data', download_gtxs='all', out_data_dir='detection_out_data', out_plot_dir='detection_out_plot', out_stat_dir='detection_out_stat')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcaf4eb-ffde-4712-a8f6-5162b96eaa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This job is running at the following lat/lon location:32.8807000,-117.2359000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try to figure out where the script is being executed (just to show those maps at conferences, etc...)\n",
    "try:\n",
    "    with open('location-wrapper.sh', 'rb') as file: script = file.read()\n",
    "    geoip_out = subprocess.run(script, shell=True, capture_output=True)\n",
    "    compute_latlon = str(geoip_out.stdout)[str(geoip_out.stdout).find('<x><y><z>')+9 : str(geoip_out.stdout).find('<z><y><x>')]\n",
    "    print('\\nThis job is running at the following lat/lon location:%s\\n' % compute_latlon)\n",
    "except:\n",
    "    compute_latlon='0.0,0.0'\n",
    "    print('\\nUnable to determine compute location for this script.\\n')\n",
    "\n",
    "# # shuffling files around for HTCondor\n",
    "# for thispath in (args.is2_data_dir, args.out_data_dir, args.out_plot_dir):\n",
    "#     if not os.path.exists(thispath): os.makedirs(thispath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960783a3-fae9-4a6c-86f2-790e7c0498f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the specified ICESat-2 data from NSIDC\n",
    "# input_filename, request_status_code = download_granule(args.granule, args.download_gtxs, args.polygon, args.is2_data_dir, \n",
    "#                                              decedc(edc().u), decedc(edc().p))\n",
    "# input_filename, request_status_code = download_granule(args.granule, args.download_gtxs, args.polygon, args.is2_data_dir, \n",
    "#                                             decedc(edc().u), decedc(edc().p), vars_sub='all', spatial_sub=True)\n",
    "#input_filename = 'IS2data/processed_ATL03_20200225170832_09310610_006_01.h5'\n",
    "#request_status_code = 200\n",
    "input_filename = 'IS2data/processed_ATL03_20220714010847_03381603_006_02.h5'\n",
    "request_status_code = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6544e6-0262-4f26-bfd4-509d4716881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# poly_nonsimplified = args.polygon.replace('simplified_', '')\n",
    "# clip_shape = gpd.read_file(poly_nonsimplified)\n",
    "\n",
    "# # first just slice to the bounding box\n",
    "# lons = clip_shape.loc[0].geometry.exterior.coords.xy[0]\n",
    "# lats = clip_shape.loc[0].geometry.exterior.coords.xy[1]\n",
    "# print('%.5f,%.5f,%.5f,%.5f' % (np.min(lons), np.min(lats), np.max(lons), np.max(lats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9adb119-3fc6-45d9-8a99-286e437f0840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request status code: 200 True\n"
     ]
    }
   ],
   "source": [
    "# perform a bunch of checks to make sure everything went alright with the nsidc api\n",
    "print('Request status code:', request_status_code, request_status_code==200)\n",
    "if request_status_code != 200:\n",
    "    print('NSIDC API request failed.')\n",
    "    sys.exit(127)\n",
    "if request_status_code==200:\n",
    "    with open('success.txt', 'w') as f: print('we got some sweet data', file=f)\n",
    "    if input_filename == 'none': \n",
    "        print('granule seems to be empty. nothing more to do here.') \n",
    "        sys.exit(69)\n",
    "if os.path.exists(input_filename):\n",
    "    if os.path.getsize(input_filename) < 31457280:# 30 MB\n",
    "        print('granule seems to be empty. nothing more to do here.') \n",
    "        sys.exit(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ac1123-9efa-4742-aab2-2504b1d6fc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading in IS2data/processed_ATL03_20220714010847_03381603_006_02.h5\n",
      "  reading in beam:  --> done.\n"
     ]
    }
   ],
   "source": [
    "gtx_list, ancillary = read_atl03(input_filename, gtxs_to_read='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a206fb9f-caf9-484d-9322-4323e1f68cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading in IS2data/processed_ATL03_20220714010847_03381603_006_02.h5\n",
      "  reading in beam: gt1l  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt1l (weak)\n",
      "---> finding flat surfaces in photon data (73 / 2963 were flat)\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(19 / 2963 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:   19\n",
      "   --> iteration   1, number of lakes:   10\n",
      "   --> iteration   2, number of lakes:    6\n",
      "   --> iteration   3, number of lakes:    5\n",
      "   --> iteration   4, number of lakes:    4\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:<> 1: 2:> 3:<>>> \n",
      "---> calculating remaining photon densities\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(21 / 2963 pass lake quality test)\n",
      "RESULTS FOR : GT1L\n",
      "  lake    0 ( 68.45988°N, 49.45794°W) length:  0.6 km, surface elevation:  969.88 m, quality: 0.29850)\n",
      "  lake    1 ( 68.70059°N, 49.53780°W) length:  1.9 km, surface elevation:  947.84 m, quality: 0.73087)\n",
      "  lake    2 ( 69.39644°N, 49.77654°W) length:  0.3 km, surface elevation:  712.16 m, quality: 0.00113)\n",
      "  lake    3 ( 71.33472°N, 50.51435°W) length:  0.5 km, surface elevation: 1214.49 m, quality: 0.00402)\n",
      "  reading in IS2data/processed_ATL03_20220714010847_03381603_006_02.h5\n",
      "  reading in beam: gt1r  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt1r (strong)\n",
      "---> finding flat surfaces in photon data (68 / 2966 were flat)\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(22 / 2966 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:   22\n",
      "   --> iteration   1, number of lakes:   15\n",
      "   --> iteration   2, number of lakes:   10\n",
      "   --> iteration   3, number of lakes:    9\n",
      "   --> iteration   4, number of lakes:    7\n",
      "   --> iteration   5, number of lakes:    6\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:< 1:<> 2: 3:>> 4:<> 5: \n",
      "---> calculating remaining photon densities\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(23 / 2966 pass lake quality test)\n",
      "RESULTS FOR : GT1R\n",
      "  lake    0 ( 68.45959°N, 49.45566°W) length:  0.6 km, surface elevation:  969.84 m, quality: 0.30093)\n",
      "  lake    1 ( 68.70089°N, 49.53566°W) length:  1.8 km, surface elevation:  947.81 m, quality: 0.86291)\n",
      "  lake    2 ( 69.39676°N, 49.77433°W) length:  0.3 km, surface elevation:  712.11 m, quality: 0.02503)\n",
      "  lake    3 ( 70.44386°N, 50.15838°W) length:  0.3 km, surface elevation:  538.22 m, quality: 0.00000)\n",
      "  lake    4 ( 70.50315°N, 50.18107°W) length:  0.5 km, surface elevation:  628.51 m, quality: 0.28732)\n",
      "  reading in IS2data/processed_ATL03_20220714010847_03381603_006_02.h5\n",
      "  reading in beam: gt2l  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt2l (weak)\n",
      "---> finding flat surfaces in photon data (186 / 3048 were flat)\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(25 / 3048 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:   25\n",
      "   --> iteration   1, number of lakes:   17\n",
      "   --> iteration   2, number of lakes:   12\n",
      "   --> iteration   3, number of lakes:   11\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:> 1:> 2: 3: 4: 5:<> 6:> 7:<>>>>>>>>>>>>> 8:<<<<<>>>>>>>> 9:<<<<<<<>>>>>> 10:<<<<<<<<<>>>> \n",
      "---> calculating remaining photon densities\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(27 / 3048 pass lake quality test)\n",
      "RESULTS FOR : GT2L\n",
      "  lake    0 ( 68.36007°N, 49.34577°W) length:  1.1 km, surface elevation: 1031.88 m, quality: 0.22786)\n",
      "  lake    1 ( 68.72905°N, 49.46653°W) length:  0.6 km, surface elevation:  939.50 m, quality: 0.15629)\n",
      "  lake    2 ( 70.61163°N, 50.13685°W) length:  0.3 km, surface elevation:  918.16 m, quality: 0.01106)\n",
      "  lake    3 ( 70.66020°N, 50.15554°W) length:  0.4 km, surface elevation:  996.53 m, quality: 0.00000)\n",
      "  lake    4 ( 71.41578°N, 50.45573°W) length:  0.2 km, surface elevation: 1253.46 m, quality: 0.00223)\n",
      "  lake    5 ( 71.79286°N, 50.61314°W) length:  2.4 km, surface elevation: 1030.09 m, quality: 0.03323)\n",
      "  reading in IS2data/processed_ATL03_20220714010847_03381603_006_02.h5\n",
      "  reading in beam: gt2r  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt2r (strong)\n",
      "---> finding flat surfaces in photon data (182 / 3047 were flat)\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(40 / 3047 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:   40\n",
      "   --> iteration   1, number of lakes:   26\n",
      "   --> iteration   2, number of lakes:   20\n",
      "   --> iteration   3, number of lakes:   18\n",
      "   --> iteration   4, number of lakes:   15\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:>>>> 1:> 2:<> 3: 4:> 5:< 6:<> 7: 8:< 9: 10:>>>> 11:<<>>>>>>>>>>>> 12:<<<<<> 13: 14:< \n",
      "---> calculating remaining photon densities\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(40 / 3047 pass lake quality test)\n",
      "RESULTS FOR : GT2R\n",
      "  lake    0 ( 68.36231°N, 49.34420°W) length:  1.1 km, surface elevation: 1031.84 m, quality: 0.53661)\n",
      "  lake    1 ( 68.50951°N, 49.39194°W) length:  0.2 km, surface elevation: 1005.37 m, quality: 0.00000)\n",
      "  lake    2 ( 68.72873°N, 49.46403°W) length:  0.5 km, surface elevation:  939.43 m, quality: 0.31383)\n",
      "  lake    3 ( 70.29009°N, 50.01273°W) length:  0.2 km, surface elevation:  822.85 m, quality: 0.03727)\n",
      "  lake    4 ( 70.43781°N, 50.06822°W) length:  0.3 km, surface elevation:  695.83 m, quality: 0.00000)\n",
      "  lake    5 ( 70.61191°N, 50.13436°W) length:  0.4 km, surface elevation:  918.13 m, quality: 0.33437)\n",
      "  lake    6 ( 70.65987°N, 50.15283°W) length:  0.5 km, surface elevation:  996.50 m, quality: 0.20481)\n",
      "  lake    7 ( 70.80181°N, 50.20770°W) length:  0.6 km, surface elevation:  997.10 m, quality: 0.07433)\n",
      "  lake    8 ( 71.41547°N, 50.45291°W) length:  0.2 km, surface elevation: 1253.43 m, quality: 0.03294)\n",
      "  lake    9 ( 71.79188°N, 50.61000°W) length:  2.2 km, surface elevation: 1030.05 m, quality: 0.10765)\n",
      "  reading in IS2data/processed_ATL03_20220714010847_03381603_006_02.h5\n",
      "  reading in beam: gt3l  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt3l (weak)\n",
      "---> finding flat surfaces in photon data (98 / 3047 were flat)\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(30 / 3047 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:   30\n",
      "   --> iteration   1, number of lakes:   19\n",
      "   --> iteration   2, number of lakes:   14\n",
      "   --> iteration   3, number of lakes:   13\n",
      "   --> iteration   4, number of lakes:   11\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:< 1:> 2:<> 3: 4:> 5: 6:< 7:<> 8: 9: 10: \n",
      "---> calculating remaining photon densities\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(32 / 3047 pass lake quality test)\n",
      "RESULTS FOR : GT3L\n",
      "  lake    0 ( 68.28429°N, 49.24209°W) length:  0.2 km, surface elevation: 1122.27 m, quality: 0.05006)\n",
      "  lake    1 ( 68.63561°N, 49.35509°W) length:  0.2 km, surface elevation: 1023.86 m, quality: 0.28599)\n",
      "  lake    2 ( 68.78470°N, 49.40379°W) length:  0.7 km, surface elevation:  968.41 m, quality: 0.48036)\n",
      "  lake    3 ( 69.45416°N, 49.62954°W) length:  0.5 km, surface elevation:  870.89 m, quality: 0.20854)\n",
      "  lake    4 ( 69.49394°N, 49.64330°W) length:  0.6 km, surface elevation:  916.26 m, quality: 0.54308)\n",
      "  lake    5 ( 70.52417°N, 50.01529°W) length:  0.6 km, surface elevation:  792.09 m, quality: 0.01980)\n",
      "  lake    6 ( 70.79737°N, 50.11928°W) length:  0.5 km, surface elevation: 1096.74 m, quality: 0.16458)\n",
      "  lake    7 ( 70.81945°N, 50.12774°W) length:  0.9 km, surface elevation: 1073.93 m, quality: 0.56280)\n",
      "  reading in IS2data/processed_ATL03_20220714010847_03381603_006_02.h5\n",
      "  reading in beam: gt3r  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt3r (strong)\n",
      "---> finding flat surfaces in photon data (88 / 3048 were flat)\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(20 / 3048 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:   20\n",
      "   --> iteration   1, number of lakes:   10\n",
      "   --> iteration   2, number of lakes:    7\n",
      "   --> iteration   3, number of lakes:    6\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0: 1:<<< 2:<<< 3:> 4:> 5:<> \n",
      "---> calculating remaining photon densities\n",
      "---> removing afterpulses, calculating photon densities & looking for second density peaks below the surface\n",
      "(20 / 3048 pass lake quality test)\n",
      "RESULTS FOR : GT3R\n",
      "  lake    0 ( 68.63405°N, 49.35238°W) length:  0.2 km, surface elevation: 1023.80 m, quality: 0.28925)\n",
      "  lake    1 ( 68.78187°N, 49.40070°W) length:  1.3 km, surface elevation:  968.36 m, quality: 0.65625)\n",
      "  lake    2 ( 69.45324°N, 49.62696°W) length:  0.6 km, surface elevation:  870.85 m, quality: 0.13178)\n",
      "  lake    3 ( 69.47214°N, 49.63351°W) length:  0.3 km, surface elevation:  909.64 m, quality: 0.25974)\n",
      "  lake    4 ( 69.49364°N, 49.64100°W) length:  0.6 km, surface elevation:  916.20 m, quality: 0.70555)\n",
      "  lake    5 ( 70.52321°N, 50.01260°W) length:  0.7 km, surface elevation:  792.05 m, quality: 0.25070)\n"
     ]
    }
   ],
   "source": [
    "# detect melt lakes\n",
    "lake_list = []\n",
    "granule_stats = [0,0,0,0]\n",
    "for gtx in gtx_list:\n",
    "    lakes_found, gtx_stats = detect_lakes(input_filename, gtx, args.polygon, verbose=False)\n",
    "    for i in range(len(granule_stats)): granule_stats[i] += gtx_stats[i]\n",
    "    lake_list += lakes_found\n",
    "    del lakes_found, gtx_stats\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b2cf9c-095f-40fc-a679-623dc7a3eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully retrieved data from NSIDC!!\n",
      "\n",
      "GRANULE STATS (length total, length lakes, photons total, photons lakes):2594060.675,23995.000,25990020,370606\n",
      "\n",
      "---> determining depth for each lake\n",
      "   -->   1/ 39, gt1l |   68.460N,  -49.458E:   2.85m deep / quality:     3.69\n",
      "   -->   2/ 39, gt1l |   68.701N,  -49.538E:   8.59m deep / quality:    17.92\n",
      "   -->   3/ 39, gt1l |   69.396N,  -49.777E:   1.36m deep / quality:     0.57\n",
      "   -->   4/ 39, gt1l |   71.335N,  -50.514E:   3.79m deep / quality:     0.00\n",
      "   -->   5/ 39, gt1r |   68.460N,  -49.456E:   2.48m deep / quality:     1.20\n",
      "   -->   6/ 39, gt1r |   68.701N,  -49.536E:   8.43m deep / quality:    24.86\n",
      "   -->   7/ 39, gt1r |   69.397N,  -49.774E:   1.40m deep / quality:     0.86\n",
      "   -->   8/ 39, gt1r |   70.444N,  -50.158E:   7.67m deep / quality:     0.19\n",
      "   -->   9/ 39, gt1r |   70.503N,  -50.181E:  10.79m deep / quality:     0.61\n",
      "   -->  10/ 39, gt2l |   68.360N,  -49.346E:   5.43m deep / quality:     2.67\n",
      "   -->  11/ 39, gt2l |   68.729N,  -49.467E:   3.47m deep / quality:     7.11\n",
      "   -->  12/ 39, gt2l |   70.612N,  -50.137E:   3.07m deep / quality:     2.84\n",
      "   -->  13/ 39, gt2l |   70.660N,  -50.156E:   1.52m deep / quality:     0.00\n",
      "   -->  14/ 39, gt2l |   71.416N,  -50.456E:   1.08m deep / quality:     1.99\n",
      "   -->  15/ 39, gt2l |   71.793N,  -50.613E:   8.44m deep / quality:     0.01\n",
      "   -->  16/ 39, gt2r |   68.362N,  -49.344E:   5.72m deep / quality:     6.73\n",
      "   -->  17/ 39, gt2r |   68.510N,  -49.392E:   1.72m deep / quality:     0.00\n",
      "   -->  18/ 39, gt2r |   68.729N,  -49.464E:   2.40m deep / quality:     3.45\n",
      "   -->  19/ 39, gt2r |   70.290N,  -50.013E:   2.65m deep / quality:     0.00\n",
      "   -->  20/ 39, gt2r |   70.438N,  -50.068E:   1.94m deep / quality:     0.00\n",
      "   -->  21/ 39, gt2r |   70.612N,  -50.134E:   3.80m deep / quality:     8.23\n",
      "   -->  22/ 39, gt2r |   70.660N,  -50.153E:   2.19m deep / quality:     6.00\n",
      "   -->  23/ 39, gt2r |   70.802N,  -50.208E:   1.99m deep / quality:     0.00\n",
      "   -->  24/ 39, gt2r |   71.415N,  -50.453E:   1.53m deep / quality:     0.33\n",
      "   -->  25/ 39, gt2r |   71.792N,  -50.610E:   8.36m deep / quality:     0.00\n",
      "   -->  26/ 39, gt3l |   68.284N,  -49.242E:   1.37m deep / quality:     0.27\n",
      "   -->  27/ 39, gt3l |   68.636N,  -49.355E:   3.73m deep / quality:     6.61\n",
      "   -->  28/ 39, gt3l |   68.785N,  -49.404E:   6.76m deep / quality:    13.51\n",
      "   -->  29/ 39, gt3l |   69.454N,  -49.630E:   6.05m deep / quality:     9.57\n",
      "   -->  30/ 39, gt3l |   69.494N,  -49.643E:   5.78m deep / quality:    21.61\n",
      "   -->  31/ 39, gt3l |   70.524N,  -50.015E:   5.38m deep / quality:     1.13\n",
      "   -->  32/ 39, gt3l |   70.797N,  -50.119E:   3.71m deep / quality:     7.49\n",
      "   -->  33/ 39, gt3l |   70.819N,  -50.128E:   4.34m deep / quality:    10.45\n",
      "   -->  34/ 39, gt3r |   68.634N,  -49.352E:   3.43m deep / quality:     2.79\n",
      "   -->  35/ 39, gt3r |   68.782N,  -49.401E:   6.76m deep / quality:     4.47\n",
      "   -->  36/ 39, gt3r |   69.453N,  -49.627E:   7.45m deep / quality:    11.37\n",
      "   -->  37/ 39, gt3r |   69.472N,  -49.634E:   3.01m deep / quality:     2.59\n",
      "   -->  38/ 39, gt3r |   69.494N,  -49.641E:   5.79m deep / quality:    21.77\n",
      "   -->  39/ 39, gt3r |   70.523N,  -50.013E:  14.36m deep / quality:     1.89\n",
      "Wrote data file: detection_out_data/lake_09963_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt1l_0000.h5, 357.53 KB\n",
      "Wrote data file: detection_out_data/lake_09820_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt1l_0001.h5, 439.12 KB\n",
      "Wrote data file: detection_out_data/lake_09994_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt1l_0002.h5, 306.35 KB\n",
      "Wrote data file: detection_out_data/lake_10000_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt1l_0003.h5, 605.61 KB\n",
      "Wrote data file: detection_out_data/lake_09988_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt1r_0004.h5, 642.68 KB\n",
      "Wrote data file: detection_out_data/lake_09751_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt1r_0005.h5, 1.01 MB\n",
      "Wrote data file: detection_out_data/lake_09991_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt1r_0006.h5, 457.33 KB\n",
      "Wrote data file: detection_out_data/lake_09998_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt1r_0007.h5, 629.89 KB\n",
      "Wrote data file: detection_out_data/lake_09993_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt1r_0008.h5, 608.5 KB\n",
      "Wrote data file: detection_out_data/lake_09973_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2l_0009.h5, 556.37 KB\n",
      "Wrote data file: detection_out_data/lake_09928_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2l_0010.h5, 402.99 KB\n",
      "Wrote data file: detection_out_data/lake_09971_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2l_0011.h5, 309.56 KB\n",
      "Wrote data file: detection_out_data/lake_10000_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2l_0012.h5, 342.83 KB\n",
      "Wrote data file: detection_out_data/lake_09980_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2l_0013.h5, 446.67 KB\n",
      "Wrote data file: detection_out_data/lake_09999_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2l_0014.h5, 935.46 KB\n",
      "Wrote data file: detection_out_data/lake_09932_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0015.h5, 1.39 MB\n",
      "Wrote data file: detection_out_data/lake_10000_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0016.h5, 582.83 KB\n",
      "Wrote data file: detection_out_data/lake_09965_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0017.h5, 736.66 KB\n",
      "Wrote data file: detection_out_data/lake_10000_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0018.h5, 410.02 KB\n",
      "Wrote data file: detection_out_data/lake_10000_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0019.h5, 506.11 KB\n",
      "Wrote data file: detection_out_data/lake_09917_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0020.h5, 524.47 KB\n",
      "Wrote data file: detection_out_data/lake_09939_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0021.h5, 686.86 KB\n",
      "Wrote data file: detection_out_data/lake_10000_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0022.h5, 843.96 KB\n",
      "Wrote data file: detection_out_data/lake_09996_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0023.h5, 552.76 KB\n",
      "Wrote data file: detection_out_data/lake_10000_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt2r_0024.h5, 1.9 MB\n",
      "Wrote data file: detection_out_data/lake_09997_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3l_0025.h5, 237.5 KB\n",
      "Wrote data file: detection_out_data/lake_09933_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3l_0026.h5, 237.07 KB\n",
      "Wrote data file: detection_out_data/lake_09864_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3l_0027.h5, 314.71 KB\n",
      "Wrote data file: detection_out_data/lake_09904_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3l_0028.h5, 332.0 KB\n",
      "Wrote data file: detection_out_data/lake_09783_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3l_0029.h5, 275.31 KB\n",
      "Wrote data file: detection_out_data/lake_09988_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3l_0030.h5, 341.35 KB\n",
      "Wrote data file: detection_out_data/lake_09925_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3l_0031.h5, 325.64 KB\n",
      "Wrote data file: detection_out_data/lake_09895_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3l_0032.h5, 397.62 KB\n",
      "Wrote data file: detection_out_data/lake_09972_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3r_0033.h5, 424.67 KB\n",
      "Wrote data file: detection_out_data/lake_09955_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3r_0034.h5, 780.11 KB\n",
      "Wrote data file: detection_out_data/lake_09886_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3r_0035.h5, 591.21 KB\n",
      "Wrote data file: detection_out_data/lake_09974_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3r_0036.h5, 478.91 KB\n",
      "Wrote data file: detection_out_data/lake_09782_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3r_0037.h5, 485.44 KB\n",
      "Wrote data file: detection_out_data/lake_09981_GrIS_2022_simplified_GRE_2000_CW_ATL03_20220714010847_03381603_006_02_gt3r_0038.h5, 690.38 KB\n",
      "\n",
      "-------------------------------------------------\n",
      "----------->   Python script done!   <-----------\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if granule_stats[0] > 0:\n",
    "    with open('success.txt', 'w') as f: print('we got some data from NSIDC!!', file=f)\n",
    "    print('Sucessfully retrieved data from NSIDC!!')\n",
    "    \n",
    "# print stats for granule\n",
    "print('\\nGRANULE STATS (length total, length lakes, photons total, photons lakes):%.3f,%.3f,%i,%i\\n' % tuple(granule_stats))\n",
    "\n",
    "# for each lake call the surrf algorithm for depth determination\n",
    "# if it fails, just skip the lake, but print trackeback for the logs \n",
    "print('---> determining depth for each lake')\n",
    "for i, lake in enumerate(lake_list):\n",
    "    try: \n",
    "        lake.surrf()\n",
    "        print('   --> %3i/%3i, %s | %8.3fN, %8.3fE: %6.2fm deep / quality: %8.2f' % (i+1, len(lake_list), lake.gtx, lake.lat, \n",
    "                                                                                 lake.lon, lake.max_depth, lake.lake_quality))\n",
    "    except:\n",
    "        print('Error for lake %i (detection quality = %.5f) ... skipping:' % (i+1, lake.detection_quality))\n",
    "        traceback.print_exc()\n",
    "        lake.lake_quality = 0.0\n",
    "\n",
    "# remove zero quality lakes\n",
    "# lake_list[:] = [lake for lake in lake_list if lake.lake_quality > 0]\n",
    "\n",
    "# for each lake \n",
    "for i, lake in enumerate(lake_list):\n",
    "    lake.lake_id = '%s_%s_%s_%04i' % (lake.polygon_name, lake.granule_id[:-3], lake.gtx, i)\n",
    "    filename_base = 'lake_%05i_%s_%s_%s' % (np.clip(1000-lake.lake_quality,0,None)*10, \n",
    "                                                       lake.ice_sheet, lake.melt_season, \n",
    "                                                       lake.lake_id)\n",
    "    # plot each lake and save to image\n",
    "    fig = lake.plot_lake(closefig=True)\n",
    "    figname = args.out_plot_dir + '/%s.jpg' % filename_base\n",
    "    if fig is not None: fig.savefig(figname, dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # export each lake to h5 and pickle\n",
    "    try:\n",
    "        h5name = args.out_data_dir + '/%s.h5' % filename_base\n",
    "        datafile = lake.write_to_hdf5(h5name)\n",
    "        print('Wrote data file: %s, %s' % (datafile, get_size(datafile)))\n",
    "    except:\n",
    "        print('Could not write hdf5 file <%s>' % lake.lake_id)\n",
    "        # try:\n",
    "        #     pklname = args.out_data_dir + '/%s.pkl' % filename_base\n",
    "        #     with open(pklname, 'wb') as f: pickle.dump(vars(lake), f)\n",
    "        #     print('Wrote data file: %s, %s' % (pklname, get_size(pklname)))\n",
    "        # except:\n",
    "        #     print('Could not write pickle file.')\n",
    "\n",
    "statsfname = args.out_stat_dir + '/stats_%s_%s.csv' % (args.polygon[args.polygon.rfind('/')+1:].replace('.geojson',''), args.granule[:-4])\n",
    "with open(statsfname, 'w') as f: print('%.3f,%.3f,%i,%i,%s' % tuple(granule_stats+[compute_latlon]), file=f)\n",
    "\n",
    "# clean up the input data\n",
    "#########os.remove(input_filename)\n",
    "\n",
    "print('\\n-------------------------------------------------')\n",
    "print(  '----------->   Python script done!   <-----------')\n",
    "print(  '-------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb908c-36ed-47ad-b4e0-898c68d64e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "lake_list[0].photon_data\n",
    "fig, ax = plt.subplots(figsize=[6,3])\n",
    "dfl = lake_list[0].photon_data\n",
    "ax.scatter(dfl.xatc, dfl.h, s=1, c='k', alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced43f0b-4d10-47d4-94c0-df81b95c8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_list[0].photon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7057d-8f03-48e3-a856-ead89681673e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb0b6d-801c-4115-b37b-8ff59d4f1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import Process\n",
    "df = lake_list[0].photon_data.copy()\n",
    "start_mem = df.memory_usage().sum() / 1024\n",
    "print('Memory usage of dataframe is {:.2f} KB'.format(start_mem))\n",
    "print('Total: {:.2f} MB'.format(Process().memory_info().rss/1024**2))\n",
    "\n",
    "# df.xatc-=df.xatc.min()\n",
    "# df['xatc'] = df.xatc.astype(np.float32)\n",
    "df['geoid'] = df.geoid.astype(np.float16)\n",
    "df['h'] = df.h.astype(np.float32)\n",
    "# df['dt'] = df.dt.astype(np.int32)\n",
    "df['snr'] = df.snr.astype(pd.SparseDtype(np.float32, fill_value=0))\n",
    "df['is_afterpulse'] = df.is_afterpulse.astype(pd.SparseDtype(np.bool_, fill_value=False))\n",
    "df['prob_afterpulse'] = df.prob_afterpulse.astype(pd.SparseDtype(np.float16, fill_value=0))\n",
    "df['sat_ratio'] = df.sat_ratio.astype(pd.SparseDtype(np.float16, fill_value=0))\n",
    "df['sat_ratio_smooth'] = df.sat_ratio_smooth.astype(pd.SparseDtype(np.float16, fill_value=0))\n",
    "df['sat_elev'] = df.sat_elev.astype(pd.SparseDtype(np.float32, fill_value=np.nan))\n",
    "\n",
    "end_mem = df.memory_usage().sum() / 1024\n",
    "print('Memory usage of dataframe is {:.2f} KB'.format(end_mem))\n",
    "print('Total: {:.2f} MB'.format(Process().memory_info().rss/1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45941d9-cfdc-4886-8996-967c80d55610",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pd.arrays.SparseArray([False]*10, fill_value=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad720e-acfb-4383-92c6-0a9e453a4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = pd.DataFrame(pd.arrays.SparseArray(np.zeros(10).astype(np.float16), fill_value=0))\n",
    "bla.loc[3] = 1.9032093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c318065-5108-4967-bd75-8192a5d04abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtx = 'gt2l'\n",
    "polygon = args.polygon\n",
    "verbose = False\n",
    "print(input_filename, gtx, polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ffc0c-8db9-48f2-8dc1-cc595937d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtx_list, ancillary, photon_data = read_atl03(input_filename, geoid_h=True, gtxs_to_read=gtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6231f3-1b3f-4d83-9339-22bd6f6490bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import Process\n",
    "df = photon_data[gtx].copy()\n",
    "start_mem = df.memory_usage().sum() / 1024**2\n",
    "print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "print('Total: {:.2f} MB'.format(Process().memory_info().rss/1024**2))\n",
    "\n",
    "df.xatc-=df.xatc.min()\n",
    "df['xatc'] = df.xatc.astype(np.float32)\n",
    "df['geoid'] = df.geoid.astype(np.float32)\n",
    "df['h'] = df.h.astype(np.float32)\n",
    "df['dt'] = df.dt.astype(np.int32)\n",
    "\n",
    "end_mem = df.memory_usage().sum() / 1024**2\n",
    "print('Memory usage of dataframe is {:.2f} MB'.format(end_mem))\n",
    "print('Total: {:.2f} MB'.format(Process().memory_info().rss/1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81acc0f-4216-4375-b38b-ddc69a5f34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dt.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700fb4f-2fcc-4b58-9edf-e8c537de97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.xatc - df.xatc.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697fa64-7340-4264-8961-41a564ac14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690b938-deec-414c-b708-9f0d9cbd39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = input_filename\n",
    "geoid_h=True\n",
    "gtxs_to_read='gt3l'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63dcf0-8a00-4ee4-826a-dd54cae69923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "input_filename = 'IS2data/processed_ATL03_20200225170832_09310610_006_01.h5'\n",
    "gtx = 'gt2r'\n",
    "gdf_poly = gpd.read_file(args.polygon)\n",
    "dfs = [gdf_poly]\n",
    "for gtx in ['gt1r', 'gt2r', 'gt3r']:\n",
    "    with h5py.File(input_filename, 'r') as f:\n",
    "        lon_ph = f[gtx]['heights']['lon_ph'][:]\n",
    "        lat_ph = f[gtx]['heights']['lat_ph'][:]\n",
    "        df = pd.DataFrame({'lat_ph': lat_ph, 'lon': lon_ph})\n",
    "        df['lat'] = np.round(df.lat_ph,2)\n",
    "        df_gt = df.groupby('lat')[['lon']].median()\n",
    "        df_gt.reset_index(inplace=True)\n",
    "        gdf_gt = gpd.GeoDataFrame(geometry=gpd.points_from_xy(df_gt.lon, df_gt.lat), crs=\"EPSG:4326\")\n",
    "        dfs.append(gdf_gt)\n",
    "\n",
    "gdf = pd.concat(dfs)\n",
    "gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d895c4c3-b61b-49e1-b037-5fd48cd505c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'IS2data/processed_ATL03_20200225170832_09310610_006_01.h5'\n",
    "gtx = 'gt2l'\n",
    "with h5py.File(input_filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg = f[gtx]['geolocation']['ph_index_beg'][:]\n",
    "fig, ax = plt.subplots(figsize=[6,3])\n",
    "ax.scatter(segment_id, ph_index_beg, s=1, c='k', alpha=0.1)\n",
    "ax.set_xlabel('%s/geolocation/segment_id' % gtx)\n",
    "ax.set_ylabel('%s/geolocation/ph_index_beg' % gtx)\n",
    "fig.tight_layout()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a0e08-65cd-40de-9e13-065d2a17846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "beam = 'gt2l'\n",
    "gtx = beam\n",
    "\n",
    "with h5py.File(input_filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg = f[gtx]['geolocation']['ph_index_beg'][:]\n",
    "    segment_ph_cnt = f[gtx]['geolocation']['segment_ph_cnt'][:]\n",
    "    segment_dist_x = f[gtx]['geolocation']['segment_dist_x'][:]\n",
    "    dist_ph_along = f[gtx]['heights']['dist_ph_along'][:]\n",
    "    pce_mframe_cnt = f[gtx]['heights']['pce_mframe_cnt'][:]\n",
    "    x_atc = f[gtx]['heights']['dist_ph_along'][:]\n",
    "    dt = f[gtx]['heights']['delta_time'][:]\n",
    "    h = f[gtx]['heights']['h_ph'][:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(segment_id, ph_index_beg, s=1, c='k', alpha=0.1)\n",
    "\n",
    "ph_index_beg = np.concatenate(([0], np.cumsum(segment_ph_cnt[:-1])))\n",
    "\n",
    "print(ph_index_beg)\n",
    "print(segment_ph_cnt)\n",
    "print(np.sum(segment_ph_cnt) - len(dt))\n",
    "print(len(dt))\n",
    "print(np.sum(segment_ph_cnt))\n",
    "print(len(segment_id))\n",
    "print(pce_mframe_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab0e93-81bd-44b0-87db-4e7029bc0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "xmin = 181907.259119056\n",
    "xmax = 192586.55220003167\n",
    "xmin = 0.259119056\n",
    "xmax = 192586.55220003167\n",
    "idxs = np.array(range(len(x_atc)))\n",
    "idx_beg = ph_index_beg[(ph_index_beg > xmin) & (ph_index_beg < xmax)]\n",
    "sel = (idxs > xmin) & (idxs < xmax)\n",
    "ax.scatter(idxs[sel], x_atc[sel], s=1, c='k', alpha=0.5)\n",
    "# ax.scatter(segment_id, ph_index_beg_, s=1, c='k', alpha=0.5)\n",
    "for idx in idx_beg:\n",
    "    ax.plot([idx]*2, [0,20], 'r-', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea727c6-fb5b-496e-8a76-74b6307bb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(segment_id, ph_index_beg_, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c046643-06c9-4dd7-b837-a01ecbe5178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=[6,3])\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg = f[gtx]['geolocation']['ph_index_beg'][:] - 1\n",
    "ax.scatter(segment_id, ph_index_beg, s=1, c='k', alpha=0.1)\n",
    "ax.set_xlabel('%s/geolocation/segment_id' % gtx)\n",
    "ax.set_ylabel('%s/geolocation/ph_index_beg' % gtx)\n",
    "fig.tight_layout()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e77798-4b50-42b8-83ba-1d3405e55d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "input_filename = 'IS2data/processed_ATL03_20200225170832_09310610_006_01.h5'\n",
    "gtx = 'gt2l'\n",
    "fig, ax = plt.subplots(figsize=[6,3])\n",
    "with h5py.File(input_filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg = f[gtx]['geolocation']['ph_index_beg'][:] - 1\n",
    "    segment_ph_cnt = f[gtx]['geolocation']['segment_ph_cnt'][:]\n",
    "    segment_dist_x = f[gtx]['geolocation']['segment_dist_x'][:]\n",
    "    dist_ph_along = f[gtx]['heights']['dist_ph_along'][:]\n",
    "\n",
    "# ax.scatter(segment_id, ph_index_beg, s=1, c='k', alpha=0.1)\n",
    "\n",
    "idxs = np.concatenate(([0], np.cumsum(segment_ph_cnt[:-1])))\n",
    "# ax.scatter(segment_id, idxs, s=1, c='k', alpha=0.1)\n",
    "# ax.scatter(segment_id, ph_index_beg-idxs, s=1, c='k', alpha=0.1)\n",
    "ax.scatter(segment_id, segment_dist_x, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5272f90-3753-4f43-8000-b8c2c4a17f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6,3])\n",
    "ax.scatter(segment_id, ph_index_beg-idxs, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2f5cc-1d14-424a-83b3-f7d79ef3b2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728f3c4-8535-4724-bb94-91506e06f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam = 'gt2r'\n",
    "gtx = beam\n",
    "gtxs_to_read=beam\n",
    "\n",
    "print('  reading in', filename)\n",
    "granule_id = filename[filename.find('ATL03_'):(filename.find('.h5')+3)]\n",
    "\n",
    "# open file\n",
    "f = h5py.File(filename, 'r')\n",
    "\n",
    "# make dictionaries for beam data to be stored in\n",
    "dfs = {}\n",
    "dfs_bckgrd = {}\n",
    "all_beams = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "beams_available = [beam for beam in all_beams if \"/%s/heights/\" % beam in f]\n",
    "\n",
    "if gtxs_to_read=='all':\n",
    "    beamlist = beams_available\n",
    "elif gtxs_to_read=='none':\n",
    "    beamlist = []\n",
    "else:\n",
    "    if type(gtxs_to_read)==list: beamlist = list(set(gtxs_to_read).intersection(set(beams_available)))\n",
    "    elif type(gtxs_to_read)==str: beamlist = list(set([gtxs_to_read]).intersection(set(beams_available)))\n",
    "    else: beamlist = beams_available\n",
    "\n",
    "conf_landice = 3 # index for the land ice confidence\n",
    "\n",
    "orient = f['orbit_info']['sc_orient'][0]\n",
    "def orient_string(sc_orient):\n",
    "    if sc_orient == 0:\n",
    "        return 'backward'\n",
    "    elif sc_orient == 1:\n",
    "        return 'forward'\n",
    "    elif sc_orient == 2:\n",
    "        return 'transition'\n",
    "    else:\n",
    "        return 'error'\n",
    "    \n",
    "orient_str = orient_string(orient)\n",
    "gtl = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "beam_strength_dict = {k:['weak','strong'][k%2] for k in np.arange(1,7,1)}\n",
    "if orient_str == 'forward':\n",
    "    bl = np.arange(6,0,-1)\n",
    "    gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "    gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "elif orient_str == 'backward':\n",
    "    bl = np.arange(1,7,1)\n",
    "    gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "    gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "else:\n",
    "    gtx_beam_dict = {k:'undefined' for k in gtl}\n",
    "    gtx_strength_dict = {k:'undefined' for k in gtl}\n",
    "    \n",
    "\n",
    "ancillary = {'granule_id': granule_id,\n",
    "             'atlas_sdp_gps_epoch': f['ancillary_data']['atlas_sdp_gps_epoch'][0],\n",
    "             'rgt': f['orbit_info']['rgt'][0],\n",
    "             'cycle_number': f['orbit_info']['cycle_number'][0],\n",
    "             'sc_orient': orient_str,\n",
    "             'gtx_beam_dict': gtx_beam_dict,\n",
    "             'gtx_strength_dict': gtx_strength_dict,\n",
    "             'gtx_dead_time_dict': {}}\n",
    "\n",
    "# loop through all beams\n",
    "print('  reading in beam:', end=' ')\n",
    "for beam in beamlist:\n",
    "    \n",
    "    print(beam, end=' ')\n",
    "    try:\n",
    "        \n",
    "        if gtx_strength_dict[beam]=='strong':\n",
    "            ancillary['gtx_dead_time_dict'][beam] = np.mean(np.array(f['ancillary_data']['calibrations']['dead_time'][beam]['dead_time'])[:16])\n",
    "        else:\n",
    "            ancillary['gtx_dead_time_dict'][beam] = np.mean(np.array(f['ancillary_data']['calibrations']['dead_time'][beam]['dead_time'])[16:])\n",
    "           \n",
    "        #### get photon-level data\n",
    "        # if \"/%s/heights/\" not in f: break; # \n",
    "         \n",
    "        df = pd.DataFrame({'lat': np.array(f[beam]['heights']['lat_ph']),\n",
    "                           'lon': np.array(f[beam]['heights']['lon_ph']),\n",
    "                           'h': np.array(f[beam]['heights']['h_ph']),\n",
    "                           'dt': np.array(f[beam]['heights']['delta_time']),\n",
    "                           # 'conf': np.array(f[beam]['heights']['signal_conf_ph'][:,conf_landice]),\n",
    "                           # not using ATL03 confidences here\n",
    "                           'mframe': np.array(f[beam]['heights']['pce_mframe_cnt']),\n",
    "                           'ph_id_pulse': np.array(f[beam]['heights']['ph_id_pulse']),\n",
    "                           'qual': np.array(f[beam]['heights']['quality_ph'])}) \n",
    "        #### calculate along-track distances [meters from the equator crossing] from segment-level data\n",
    "        segment_id = f[beam]['geolocation']['segment_id'][:]\n",
    "        n_seg = len(segment_id)\n",
    "        segment_ph_cnt = f[beam]['geolocation']['segment_ph_cnt'][:]\n",
    "        ph_index_beg = np.concatenate(([0], np.cumsum(segment_ph_cnt[:-1])))\n",
    "        segment_dist_x = f[beam]['geolocation']['segment_dist_x'][:]\n",
    "        x_atc = f[beam]['heights']['dist_ph_along'][:]\n",
    "        # for each 20m segment\n",
    "        for j,_ in enumerate(segment_id):\n",
    "            idx = ph_index_beg[j]\n",
    "            cnt = segment_ph_cnt[j]\n",
    "            # skip segments with no photon events\n",
    "            if (cnt == 0):\n",
    "                continue\n",
    "            # add segment distance to along-track coordinates\n",
    "            x_atc[idx:idx+cnt] += segment_dist_x[j]\n",
    "        df['xatc'] = x_atc\n",
    "        # ph_index_beg = np.int64(f[beam]['geolocation']['ph_index_beg']) - 1\n",
    "        # segment_dist_x = np.array(f[beam]['geolocation']['segment_dist_x'])\n",
    "        # segment_length = np.array(f[beam]['geolocation']['segment_length'])\n",
    "        # valid = ph_index_beg>=0 # need to delete values where there's no photons in the segment (-1 value)\n",
    "        # df.loc[ph_index_beg[valid], 'xatc'] = segment_dist_x[valid]\n",
    "        # df.xatc.fillna(method='ffill',inplace=True)\n",
    "        # df.xatc += np.array(f[beam]['heights']['dist_ph_along'])\n",
    "\n",
    "        #### now we can filter out TEP (we don't do IRF / afterpulses because it seems to not be very good...)\n",
    "        df.query('qual < 3',inplace=True) \n",
    "        # df.drop(columns=['qual'], inplace=True)\n",
    "\n",
    "        #### sort by along-track distance (for interpolation to work smoothly)\n",
    "        df.sort_values(by='xatc',inplace=True)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # if geoid_h:\n",
    "        #     #### interpolate geoid to photon level using along-track distance, and add to elevation\n",
    "        #     geophys_geoid = np.array(f[beam]['geophys_corr']['geoid'])\n",
    "        #     geophys_geoid_x = segment_dist_x+0.5*segment_length\n",
    "        #     valid_geoid = geophys_geoid<1e10 # filter out INVALID_R4B fill values\n",
    "        #     geophys_geoid = geophys_geoid[valid_geoid]\n",
    "        #     geophys_geoid_x = geophys_geoid_x[valid_geoid]\n",
    "        #     # hacky fix for no weird stuff happening if geoid is undefined everywhere\n",
    "        #     if len(geophys_geoid>5):\n",
    "        #         geoid = np.interp(np.array(df.xatc), geophys_geoid_x, geophys_geoid)\n",
    "        #         df['h'] = df.h - geoid\n",
    "        #         df['geoid'] = geoid\n",
    "        #         del geoid\n",
    "        #     else:\n",
    "        #         df['geoid'] = 0.0\n",
    "\n",
    "        #### save to list of dataframes\n",
    "        dfs[beam] = df\n",
    "        del df \n",
    "        gc.collect()\n",
    "        #Mdfs_bckgrd[beam] = df_bckgrd\n",
    "    \n",
    "    except:\n",
    "        print('Error for {f:s} on {b:s} ... skipping:'.format(f=filename, b=beam))\n",
    "        traceback.print_exc()\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cddda0-80a3-4bcc-81a3-53ecd2bbf509",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam = 'gt1r'\n",
    "gtx = beam\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg_ = f[gtx]['geolocation']['ph_index_beg'][:]\n",
    "    segment_ph_cnt = f[gtx]['geolocation']['segment_ph_cnt'][:]\n",
    "    segment_dist_x = f[gtx]['geolocation']['segment_dist_x'][:]\n",
    "    dist_ph_along = f[gtx]['heights']['dist_ph_along'][:]\n",
    "    pce_mframe_cnt = f[gtx]['heights']['pce_mframe_cnt'][:]\n",
    "    x_atc = f[gtx]['heights']['dist_ph_along'][:]\n",
    "    dt = f[gtx]['heights']['delta_time'][:]\n",
    "    h = f[gtx]['heights']['h_ph'][:]\n",
    "\n",
    "ph_index_beg = np.concatenate(([0], np.cumsum(segment_ph_cnt[:-1])))\n",
    "\n",
    "print(ph_index_beg)\n",
    "print(ph_index_beg_)\n",
    "print(segment_ph_cnt)\n",
    "print(np.sum(segment_ph_cnt) - len(dt))\n",
    "print(len(dt))\n",
    "print(np.sum(segment_ph_cnt))\n",
    "print(len(segment_id))\n",
    "print(pce_mframe_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1417ee5-4325-4a77-955b-667f31eb2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "xmin = 181907.259119056\n",
    "xmax = 192586.55220003167\n",
    "xmin = 0.259119056\n",
    "xmax = 192586.55220003167\n",
    "idxs = np.array(range(len(x_atc)))\n",
    "idx_beg = ph_index_beg[(ph_index_beg > xmin) & (ph_index_beg < xmax)]\n",
    "sel = (idxs > xmin) & (idxs < xmax)\n",
    "ax.scatter(idxs[sel], x_atc[sel], s=1, c='k', alpha=0.5)\n",
    "# ax.scatter(segment_id, ph_index_beg_, s=1, c='k', alpha=0.5)\n",
    "for idx in idx_beg:\n",
    "    ax.plot([idx]*2, [0,20], 'r-', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d03ed6-1313-43c7-8d16-65952612d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(x_atc, h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77950d61-9468-4501-8736-7c8c022ba9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dt, h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b393f-146b-4eab-b412-23eeddd9565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "xmin = 181907.259119056\n",
    "xmax = 192586.55220003167\n",
    "idxs = np.array(range(len(x_atc)))\n",
    "idx_beg = ph_index_beg[(ph_index_beg > xmin) & (ph_index_beg < xmax)]\n",
    "sel = (idxs > xmin) & (idxs < xmax)\n",
    "ax.scatter(idxs[sel], x_atc[sel], s=1, c='k', alpha=0.1)\n",
    "for idx in idx_beg:\n",
    "    ax.plot([idx]*2, [0,20], 'r-', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cfacc9-55c7-41e8-b1ab-d97671a4d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_index_beg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7193a4d8-c795-4e72-882d-3fe1a3b3b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_atc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03106b-dff8-43b5-b1d3-a29003e83c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x_atc > xmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca2848-1b6b-498a-94a7-c08e600b5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x_atc < xmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce67f9-6272-465e-9209-056423478b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dfs[beam].xatc, dfs[beam].h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d7f06-7424-4752-98ca-b15c5838a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dfs[beam].dt, dfs[beam].h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d44df9-8df5-4e2b-b19d-ffca89c242a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "f = h5py.File(filename, 'r')\n",
    "beam = 'gt2r'\n",
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "# ax.plot(Segment_Distance)\n",
    "# ax.plot(Segment_Index_begin)\n",
    "# ax.plot(Segment_ID)\n",
    "# ax.plot(Segment_PE_count)\n",
    "ax.plot(f[beam]['geolocation']['ph_index_beg'][:] - 1)\n",
    "ax.plot(np.array(f[beam]['geolocation']['ph_index_beg'], dtype=np.int64) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54af79f-6c71-493f-ba05-3877410e9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba1ce2-fbb9-496b-ad8f-31ffd5df249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtx = 'gt1l'\n",
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dfs[gtx].dt, dfs[gtx].h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b283c6-fdf5-444b-81ff-5936cb22f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dfs[gtx].xatc, dfs[gtx].h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b46fa-5b5b-4d7b-8c37-52d615f11b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_atl03(filename, geoid_h=True, gtxs_to_read='all'):\n",
    "    \"\"\"\n",
    "    Read in an ATL03 granule. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        the file path of the granule to be read in\n",
    "    geoid_h : boolean\n",
    "        whether to include the ATL03-supplied geoid correction for photon heights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dfs : dict of pandas dataframes\n",
    "          photon-rate data with keys ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "          each dataframe contains the following variables\n",
    "          lat : float64, latitude of the photon, degrees\n",
    "          lon : float64, longitude of the photon, degrees\n",
    "          h : float64, elevation of the photon (geoid correction applied if geoid_h=True), meters\n",
    "          dt : float64, delta time of the photon, seconds from the ATLAS SDP GPS Epoch\n",
    "          mframe : uint32, the ICESat-2 major frame that the photon belongs to\n",
    "          qual : int8, quality flag 0=nominal,1=possible_afterpulse,2=possible_impulse_response_effect,3=possible_tep\n",
    "          xatc : float64, along-track distance of the photon, meters\n",
    "          geoid : float64, geoid correction that was applied to photon elevation (supplied if geoid_h=True), meters\n",
    "    dfs_bckgrd : dict of pandas dataframes\n",
    "                 photon-rate data with keys ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "                 each dataframe contains the following variables\n",
    "                 pce_mframe_cnt : int64, the major frame that the data belongs to\n",
    "                 bckgrd_counts : int32, number of background photons\n",
    "                 bckgrd_int_height : float32, height of the background window, meters\n",
    "                 delta_time : float64, Time at the start of ATLAS 50-shot sum, seconds from the ATLAS SDP GPS Epoch\n",
    "    ancillary : dictionary with the following keys:\n",
    "                granule_id : string, the producer granule id, extracted from filename\n",
    "                atlas_sdp_gps_epoch : float64, reference GPS time for ATLAS in seconds [1198800018.0]\n",
    "                rgt : int16, the reference ground track number\n",
    "                cycle_number : int8, the ICESat-2 cycle number of the granule\n",
    "                sc_orient : the spacecraft orientation (usually 'forward' or 'backward')\n",
    "                gtx_beam_dict : dictionary of the ground track / beam number configuration \n",
    "                                example: {'gt1l': 6, 'gt1r': 5, 'gt2l': 4, 'gt2r': 3, 'gt3l': 2, 'gt3r': 1}\n",
    "                gtx_strength_dict': dictionary of the ground track / beam strength configuration\n",
    "                                    example: {'gt1l': 'weak','gt1r': 'strong','gt2l': 'weak', ... }\n",
    "                                    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> read_atl03(filename='processed_ATL03_20210715182907_03381203_005_01.h5', geoid_h=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    print('  reading in', filename)\n",
    "    granule_id = filename[filename.find('ATL03_'):(filename.find('.h5')+3)]\n",
    "    \n",
    "    # open file\n",
    "    f = h5py.File(filename, 'r')\n",
    "    \n",
    "    # make dictionaries for beam data to be stored in\n",
    "    dfs = {}\n",
    "    dfs_bckgrd = {}\n",
    "    all_beams = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "    beams_available = [beam for beam in all_beams if \"/%s/heights/\" % beam in f]\n",
    "    \n",
    "    if gtxs_to_read=='all':\n",
    "        beamlist = beams_available\n",
    "    elif gtxs_to_read=='none':\n",
    "        beamlist = []\n",
    "    else:\n",
    "        if type(gtxs_to_read)==list: beamlist = list(set(gtxs_to_read).intersection(set(beams_available)))\n",
    "        elif type(gtxs_to_read)==str: beamlist = list(set([gtxs_to_read]).intersection(set(beams_available)))\n",
    "        else: beamlist = beams_available\n",
    "    \n",
    "    conf_landice = 3 # index for the land ice confidence\n",
    "    \n",
    "    orient = f['orbit_info']['sc_orient'][0]\n",
    "    def orient_string(sc_orient):\n",
    "        if sc_orient == 0:\n",
    "            return 'backward'\n",
    "        elif sc_orient == 1:\n",
    "            return 'forward'\n",
    "        elif sc_orient == 2:\n",
    "            return 'transition'\n",
    "        else:\n",
    "            return 'error'\n",
    "        \n",
    "    orient_str = orient_string(orient)\n",
    "    gtl = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "    beam_strength_dict = {k:['weak','strong'][k%2] for k in np.arange(1,7,1)}\n",
    "    if orient_str == 'forward':\n",
    "        bl = np.arange(6,0,-1)\n",
    "        gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "        gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "    elif orient_str == 'backward':\n",
    "        bl = np.arange(1,7,1)\n",
    "        gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "        gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "    else:\n",
    "        gtx_beam_dict = {k:'undefined' for k in gtl}\n",
    "        gtx_strength_dict = {k:'undefined' for k in gtl}\n",
    "        \n",
    "\n",
    "    ancillary = {'granule_id': granule_id,\n",
    "                 'atlas_sdp_gps_epoch': f['ancillary_data']['atlas_sdp_gps_epoch'][0],\n",
    "                 'rgt': f['orbit_info']['rgt'][0],\n",
    "                 'cycle_number': f['orbit_info']['cycle_number'][0],\n",
    "                 'sc_orient': orient_str,\n",
    "                 'gtx_beam_dict': gtx_beam_dict,\n",
    "                 'gtx_strength_dict': gtx_strength_dict,\n",
    "                 'gtx_dead_time_dict': {}}\n",
    "\n",
    "    # loop through all beams\n",
    "    print('  reading in beam:', end=' ')\n",
    "    for beam in beamlist:\n",
    "        \n",
    "        print(beam, end=' ')\n",
    "        try:\n",
    "            \n",
    "            if gtx_strength_dict[beam]=='strong':\n",
    "                ancillary['gtx_dead_time_dict'][beam] = np.mean(np.array(f['ancillary_data']['calibrations']['dead_time'][beam]['dead_time'])[:16])\n",
    "            else:\n",
    "                ancillary['gtx_dead_time_dict'][beam] = np.mean(np.array(f['ancillary_data']['calibrations']['dead_time'][beam]['dead_time'])[16:])\n",
    "               \n",
    "            #### get photon-level data\n",
    "            # if \"/%s/heights/\" not in f: break; # \n",
    "             \n",
    "            df = pd.DataFrame({'lat': np.array(f[beam]['heights']['lat_ph']),\n",
    "                               'lon': np.array(f[beam]['heights']['lon_ph']),\n",
    "                               'h': np.array(f[beam]['heights']['h_ph']),\n",
    "                               'dt': np.array(f[beam]['heights']['delta_time']),\n",
    "                               # 'conf': np.array(f[beam]['heights']['signal_conf_ph'][:,conf_landice]),\n",
    "                               # not using ATL03 confidences here\n",
    "                               'mframe': np.array(f[beam]['heights']['pce_mframe_cnt']),\n",
    "                               'ph_id_pulse': np.array(f[beam]['heights']['ph_id_pulse']),\n",
    "                               'qual': np.array(f[beam]['heights']['quality_ph'])}) \n",
    "                               # 0=nominal,1=afterpulse,2=impulse_response_effect,3=tep\n",
    "#            if 'weight_ph' in f[beam]['heights'].keys():\n",
    "#                 df['weight_ph'] = np.array(f[beam]['heights']['weight_ph'])\n",
    "# \n",
    "#             df_bckgrd = pd.DataFrame({'pce_mframe_cnt': np.array(f[beam]['bckgrd_atlas']['pce_mframe_cnt']),\n",
    "#                                       'bckgrd_counts': np.array(f[beam]['bckgrd_atlas']['bckgrd_counts']),\n",
    "#                                       'bckgrd_int_height': np.array(f[beam]['bckgrd_atlas']['bckgrd_int_height']),\n",
    "#                                       'delta_time': np.array(f[beam]['bckgrd_atlas']['delta_time'])})\n",
    "\n",
    "            #### calculate along-track distances [meters from the equator crossing] from segment-level data\n",
    "            df['xatc'] = np.full_like(df.lat, fill_value=np.nan)\n",
    "            ph_index_beg = np.int64(f[beam]['geolocation']['ph_index_beg']) - 1\n",
    "            segment_dist_x = np.array(f[beam]['geolocation']['segment_dist_x'])\n",
    "            segment_length = np.array(f[beam]['geolocation']['segment_length'])\n",
    "            valid = ph_index_beg>=0 # need to delete values where there's no photons in the segment (-1 value)\n",
    "            df.loc[ph_index_beg[valid], 'xatc'] = segment_dist_x[valid]\n",
    "            df.xatc.fillna(method='ffill',inplace=True)\n",
    "            df.xatc += np.array(f[beam]['heights']['dist_ph_along'])\n",
    "\n",
    "            #### now we can filter out TEP (we don't do IRF / afterpulses because it seems to not be very good...)\n",
    "            df.query('qual < 3',inplace=True) \n",
    "            # df.drop(columns=['qual'], inplace=True)\n",
    "\n",
    "            #### sort by along-track distance (for interpolation to work smoothly)\n",
    "            df.sort_values(by='xatc',inplace=True)\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            if geoid_h:\n",
    "                #### interpolate geoid to photon level using along-track distance, and add to elevation\n",
    "                geophys_geoid = np.array(f[beam]['geophys_corr']['geoid'])\n",
    "                geophys_geoid_x = segment_dist_x+0.5*segment_length\n",
    "                valid_geoid = geophys_geoid<1e10 # filter out INVALID_R4B fill values\n",
    "                geophys_geoid = geophys_geoid[valid_geoid]\n",
    "                geophys_geoid_x = geophys_geoid_x[valid_geoid]\n",
    "                # hacky fix for no weird stuff happening if geoid is undefined everywhere\n",
    "                if len(geophys_geoid>5):\n",
    "                    geoid = np.interp(np.array(df.xatc), geophys_geoid_x, geophys_geoid)\n",
    "                    df['h'] = df.h - geoid\n",
    "                    df['geoid'] = geoid\n",
    "                    del geoid\n",
    "                else:\n",
    "                    df['geoid'] = 0.0\n",
    "\n",
    "            #### save to list of dataframes\n",
    "            dfs[beam] = df\n",
    "            del df \n",
    "            gc.collect()\n",
    "            #Mdfs_bckgrd[beam] = df_bckgrd\n",
    "        \n",
    "        except:\n",
    "            print('Error for {f:s} on {b:s} ... skipping:'.format(f=filename, b=beam))\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    f.close()\n",
    "    print(' --> done.')\n",
    "    if len(beamlist)==0:\n",
    "        return beams_available, ancillary\n",
    "    else:\n",
    "        return beams_available, ancillary, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad021d-b614-41f6-8378-c759c394a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lakes(input_filename, gtx, polygon, verbose=False):\n",
    "    \n",
    "    gtx_list, ancillary, photon_data = read_atl03(input_filename, geoid_h=True, gtxs_to_read=gtx)\n",
    "    if len(photon_data)==0: return [], [0,0,0,0]\n",
    "    \n",
    "    print('\\n-----------------------------------------------------------------------------\\n')\n",
    "    print('PROCESSING GROUND TRACK: %s (%s)' % (gtx, ancillary['gtx_strength_dict'][gtx]))\n",
    "\n",
    "    # get the data frame for the gtx and aggregate info at major frame level\n",
    "    #df = photon_data[gtx]\n",
    "    #====================================================================================\n",
    "    #====================================================================================\n",
    "    #====================================================================================\n",
    "    #====================================================================================\n",
    "    #====================================================================================\n",
    "    # TODO: CLIP THE DATAFRAME TO THE NON-SIMPLIFIED POLYGON FOR THE REGION TO AVOID OVERLAP\n",
    "    poly_nonsimplified = polygon.replace('simplified_', '')\n",
    "    gdf = gpd.GeoDataFrame(photon_data[gtx], geometry=gpd.points_from_xy(photon_data[gtx].lon, photon_data[gtx].lat), crs=\"EPSG:4326\")\n",
    "    clip_shape = gpd.read_file(poly_nonsimplified)\n",
    "    gdf = gpd.clip(gdf, clip_shape).reset_index(drop=True)\n",
    "    df = pd.DataFrame(gdf.drop(columns='geometry'), copy=True)\n",
    "    photon_data = None\n",
    "    gdf = None\n",
    "    del gdf, photon_data, clip_shape\n",
    "    gc.collect()\n",
    "    \n",
    "    df_mframe = make_mframe_df(df)\n",
    "    \n",
    "    # get all the flat segments and select\n",
    "    df_mframe = find_flat_lake_surfaces(df_mframe, df)\n",
    "    df_selected = df_mframe[df_mframe.is_flat]\n",
    "    \n",
    "    # calculate densities and find second peaks (where surface is flat)\n",
    "    nsubsegs = 10\n",
    "    get_densities_and_2nd_peaks(df, df_mframe, df_selected, gtx, ancillary, n_subsegs=nsubsegs, print_results=verbose)\n",
    "    \n",
    "    # iteratively merge the detected segments into lakes \n",
    "    df_lakes = merge_lakes(df_mframe, print_progress=verbose, debug=verbose)\n",
    "    if df_lakes is None: \n",
    "        return [], [df.xatc.max()-df.xatc.min(), 0.0, df.h.count(), 0]\n",
    "    df_lakes = check_lake_surroundings(df_mframe, df_lakes)\n",
    "    calculate_remaining_densities(df, df_mframe, df_lakes, gtx, ancillary)\n",
    "    \n",
    "    # create a list of lake object, and calculate some stats for each\n",
    "    thelakes = []\n",
    "    if df_lakes is not None:\n",
    "        for i in range(len(df_lakes)):\n",
    "            lakedata = df_lakes.iloc[i]\n",
    "            thislake = melt_lake(lakedata.mframe_start, lakedata.mframe_end, lakedata.surf_elev, nsubsegs)\n",
    "            thislake.add_data(df, df_mframe, gtx, ancillary, polygon)\n",
    "            thislake.get_surface_elevation()\n",
    "            thislake.get_surface_extent()\n",
    "            thislake.calc_quality_lake()\n",
    "            thelakes.append(thislake)\n",
    "    \n",
    "    # remove any duplicates and make sure data segments don't overlap into other lakes' water surfaces\n",
    "    thelakes = remove_duplicate_lakes(thelakes, df, df_mframe, gtx, ancillary, polygon, nsubsegs, verbose=verbose)          \n",
    "    print_results(thelakes, gtx)\n",
    "    \n",
    "    # get gtx stats\n",
    "    gtx_stats = get_gtx_stats(df, thelakes)\n",
    "\n",
    "    del df, df_mframe, df_selected, df_lakes\n",
    "    gc.collect()\n",
    "    \n",
    "    return thelakes, gtx_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88ed82-bc43-469b-8347-d0514e19c361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69702c5a-80d8-4199-bd01-b113f7fbb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import icelakes\n",
    "from icelakes.utilities import encedc, decedc, get_size\n",
    "from icelakes.nsidc import download_granule, edc\n",
    "from icelakes.detection import read_atl03, detect_lakes, melt_lake\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Test script to print some stats for a given ICESat-2 ATL03 granule.')\n",
    "parser.add_argument('--granule', type=str, default='ATL03_20220714010847_03381603_006_02.h5',\n",
    "                    help='The producer_id of the input ATL03 granule')\n",
    "parser.add_argument('--polygon', type=str, default='geojsons/jakobshavn_small.geojson',\n",
    "                    help='The file path of a geojson file for spatial subsetting')\n",
    "parser.add_argument('--is2_data_dir', type=str, default='IS2data',\n",
    "                    help='The directory into which to download ICESat-2 granules')\n",
    "parser.add_argument('--download_gtxs', type=str, default='all',\n",
    "                    help='String value or list of gtx names to download, also accepts \"all\"')\n",
    "parser.add_argument('--out_data_dir', type=str, default='detection_out_data',\n",
    "                    help='The directory to which to write the output data')\n",
    "parser.add_argument('--out_plot_dir', type=str, default='detection_out_plot',\n",
    "                    help='The directory to which to write the output plots')\n",
    "parser.add_argument('--out_stat_dir', type=str, default='detection_out_stat',\n",
    "                    help='The directory to which to write the granule stats')\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4899f-93b7-46a8-a835-db906c340d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to figure out where the script is being executed (just to show those maps at conferences, etc...)\n",
    "try:\n",
    "    with open('location-wrapper.sh', 'rb') as file: script = file.read()\n",
    "    geoip_out = subprocess.run(script, shell=True, capture_output=True)\n",
    "    compute_latlon = str(geoip_out.stdout)[str(geoip_out.stdout).find('<x><y><z>')+9 : str(geoip_out.stdout).find('<z><y><x>')]\n",
    "    print('\\nThis job is running at the following lat/lon location:%s\\n' % compute_latlon)\n",
    "except:\n",
    "    compute_latlon='0.0,0.0'\n",
    "    print('\\nUnable to determine compute location for this script.\\n')\n",
    "\n",
    "# # shuffling files around for HTCondor\n",
    "# for thispath in (args.is2_data_dir, args.out_data_dir, args.out_plot_dir):\n",
    "#     if not os.path.exists(thispath): os.makedirs(thispath)\n",
    "\n",
    "# # download the specified ICESat-2 data from NSIDC\n",
    "# input_filename, request_status_code = download_granule(args.granule, args.download_gtxs, args.polygon, args.is2_data_dir, \n",
    "#                                              decedc(edc().u), decedc(edc().p))\n",
    "\n",
    "# # perform a bunch of checks to make sure everything went alright with the nsidc api\n",
    "# print('Request status code:', request_status_code, request_status_code==200)\n",
    "# if request_status_code != 200:\n",
    "#     print('NSIDC API request failed.')\n",
    "#     sys.exit(127)\n",
    "# if request_status_code==200:\n",
    "#     with open('success.txt', 'w') as f: print('we got some sweet data', file=f)\n",
    "#     if input_filename == 'none': \n",
    "#         print('granule seems to be empty. nothing more to do here.') \n",
    "#         sys.exit(69)\n",
    "# if os.path.exists(input_filename):\n",
    "#     if os.path.getsize(input_filename) < 31457280:# 30 MB\n",
    "#         print('granule seems to be empty. nothing more to do here.') \n",
    "#         sys.exit(69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a09ed9-73dc-43a7-82f7-c4f779aebb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'IS2data/processed_ATL03_20220714010847_03381603_006_02.h5'\n",
    "gtx_list, ancillary = read_atl03(input_filename, gtxs_to_read='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b823bc5-bff9-4452-bc31-9bb1e89f6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect melt lakes\n",
    "lake_list = []\n",
    "granule_stats = [0,0,0,0]\n",
    "\n",
    "for gtx in gtx_list:\n",
    "    lakes_found, gtx_stats = detect_lakes(input_filename, gtx, args.polygon, verbose=False)\n",
    "    for i in range(len(granule_stats)): granule_stats[i] += gtx_stats[i]\n",
    "    lake_list += lakes_found\n",
    "\n",
    "if granule_stats[0] > 0:\n",
    "    with open('success.txt', 'w') as f: print('we got some data from NSIDC!!', file=f)\n",
    "    print('Sucessfully retrieved data from NSIDC!!')\n",
    "    \n",
    "# print stats for granule\n",
    "print('\\nGRANULE STATS (length total, length lakes, photons total, photons lakes):%.3f,%.3f,%i,%i\\n' % tuple(granule_stats))\n",
    "\n",
    "# for each lake call the surrf algorithm for depth determination\n",
    "print('---> determining depth for each lake')\n",
    "for lake in lake_list:\n",
    "    lake.surrf()\n",
    "    print('   --> %8.3fN, %8.3fE: %6.2fm deep / quality: %8.2f' % (lake.lat,lake.lon,lake.max_depth,lake.lake_quality))\n",
    "\n",
    "# remove zero quality lakes\n",
    "lake_list[:] = [lake for lake in lake_list if lake.lake_quality > 0]\n",
    "\n",
    "for i, lake in enumerate(lake_list):\n",
    "    lake.lake_id = '%s_%s_%s_%04i' % (lake.polygon_name, lake.granule_id[:-3], lake.gtx, i)\n",
    "    filename_base = 'lake_%05i_%s_%s_%s' % (np.clip(1000-lake.lake_quality,0,None)*10, \n",
    "                                                       lake.ice_sheet, lake.melt_season, \n",
    "                                                       lake.lake_id)\n",
    "    # plot each lake and save to image\n",
    "    fig = lake.plot_lake(closefig=False)\n",
    "    figname = args.out_plot_dir + '/%s.jpg' % filename_base\n",
    "    if fig is not None: fig.savefig(figname, dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # export each lake to h5 and pickle\n",
    "    try:\n",
    "        h5name = args.out_data_dir + '/%s.h5' % filename_base\n",
    "        datafile = lake.write_to_hdf5(h5name)\n",
    "        print('Wrote data file: %s, %s' % (datafile, get_size(datafile)))\n",
    "    except:\n",
    "        print('Could not write hdf5 file.')\n",
    "        try:\n",
    "            pklname = args.out_data_dir + '/%s.pkl' % filename_base\n",
    "            with open(pklname, 'wb') as f: pickle.dump(vars(lake), f)\n",
    "            print('Wrote data file: %s, %s' % (pklname, get_size(pklname)))\n",
    "        except:\n",
    "            print('Could not write pickle file.')\n",
    "\n",
    "statsfname = args.out_stat_dir + '/stats_%s_%s.csv' % (args.polygon[args.polygon.rfind('/')+1:].replace('.geojson',''), args.granule[:-4])\n",
    "with open(statsfname, 'w') as f: print('%.3f,%.3f,%i,%i,%s' % tuple(granule_stats+[compute_latlon]), file=f)\n",
    "    \n",
    "# clean up the input data\n",
    "# os.remove(input_filename)\n",
    "\n",
    "print('\\n-------------------------------------------------')\n",
    "print(  '----------->   Python script done!   <-----------')\n",
    "print(  '-------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc32d59-f929-482f-beea-962493f5e633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ca9dc-a87f-424e-8efa-f95b2d08cf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e77f98-bf4e-438d-85fc-82b6a3812259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2ce86-20b6-4fc7-bc96-4f301f11ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pylab as plt\n",
    "from cmcrameri import cm as cmc\n",
    "lk = lake_list[2]\n",
    "dfp = lk.photon_data\n",
    "dfs = dfp[~dfp.is_afterpulse]\n",
    "dfap = dfp[dfp.is_afterpulse]\n",
    "fig, ax = plt.subplots(figsize=[8, 4.5], dpi=100)\n",
    "ax.scatter(dfs.xatc, dfs.h-lk.surface_elevation, s=1, c=dfs.snr, cmap=cmc.batlow_r, vmin=0, vmax=1)\n",
    "ax.scatter(dfap.xatc, dfap.h-lk.surface_elevation, s=1, c=dfap.snr, cmap=cmc.batlow_r, alpha=0.2, vmin=0, vmax=1)\n",
    "# ax.scatter(dfp.xatc, dfp.h, s=10, c='g')\n",
    "dfg = dfs.groupby('pulseid').mean()\n",
    "ax.scatter(dfg.xatc, dfg.sat_ratio.rolling(20,center=True).mean(), s=1, c=dfg.sat_ratio, cmap=cmc.roma_r, alpha=0.2, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f0e4c-2dea-4054-86bd-441895e292ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.groupby('pulseid').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d7583-f13a-4673-b921-7cb7e462b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224104a0-1116-43b1-ad97-e9233393c810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0153e-683c-41a7-9a73-d46a41584ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fc516-572f-4a62-a678-e8ee4bf016f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "import rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9553cd-2c69-40f8-811f-2da52f5cc2ef",
   "metadata": {},
   "source": [
    "# arguments for future script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32119103-fcf1-48f9-bb42-63cd9fb61e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule = 'ATL03_20210715182907_03381203_005_01.h5'\n",
    "shapefile = '/shapefiles/jakobshavn_small.shp'\n",
    "gtxs = 'gt1l'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23f7d2-54f8-48e1-9577-4d2d3a068333",
   "metadata": {},
   "source": [
    "# download the specified granule via NSIDC\n",
    "...and subset to the provided shapefile / only pull the variables needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08889d3-fb53-4e97-8899-306ca3187111",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/IS2data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75ad7b-1857-4370-b8ae-81f60ddb48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture celloutput \n",
    "download_granule_nsidc(granule, gtxs, shapefile, datadir, decedc(edc().u), decedc(edc().p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58859ed9-f9ec-4aa5-9b5e-abd1d5836adf",
   "metadata": {},
   "source": [
    "# read in the .h5 subsetted granule file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e324277-c1da-4634-be54-7b0c2455dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [datadir[1:]+'/'+f for f in listdir(datadir[1:]) if isfile(join(datadir[1:], f)) & ('.h5' in f)]\n",
    "print('\\nNumber of processed ATL03 granules to read in: ' + str(len(filelist)))\n",
    "    \n",
    "photon_data, bckgrd_data, ancillary = read_atl03(filelist[0], geoid_h=True)\n",
    "print_granule_stats(photon_data, bckgrd_data, ancillary, outfile='stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ca06e-4313-48dc-8811-090055eaf0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e13da2-edd2-41d6-8323-1e6769230eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icelakes-env",
   "language": "python",
   "name": "icelakes-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

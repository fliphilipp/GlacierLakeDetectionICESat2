{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220a514b-f6f6-464e-a1a1-2deed37ca3dd",
   "metadata": {},
   "source": [
    "# imports of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ef813e-efff-43d2-873a-02d59d3c9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Philipp Arndt, UC San Diego / Scripps Institution of Oceanography\n",
    "# \n",
    "# intended for use on OSG OSPool, called in run_py.sh, which is called in a submit file \n",
    "# submit file is based on a granule list queried locally in make_granule_list.ipynb \n",
    "# see examples for submit files in: HTCondor_submit/ \n",
    "# see examples for granule lists in:  granule_lists/\n",
    "# \n",
    "# run locally with: \n",
    "# $ conda activate icelakes-env\n",
    "# $ python3 detect_lakes.py --granule <granule_producer_id> --polygon geojsons/<polygon_name.geojson>\n",
    "# \n",
    "# a call that returns a bunch of lakes\n",
    "# $ python3 detect_lakes.py --granule ATL03_20220714010847_03381603_006_02.h5 --polygon geojsons/simplified_GRE_2500_CW.geojson\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import pickle\n",
    "import subprocess\n",
    "import traceback\n",
    "import numpy as np\n",
    "import icelakes\n",
    "from icelakes.utilities import encedc, decedc, get_size\n",
    "from icelakes.nsidc import download_granule, edc\n",
    "from icelakes.detection import read_atl03, detect_lakes, melt_lake\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Test script to print some stats for a given ICESat-2 ATL03 granule.')\n",
    "parser.add_argument('--granule', type=str, default='ATL03_20220714010847_03381603_006_02.h5',\n",
    "                    help='The producer_id of the input ATL03 granule')\n",
    "parser.add_argument('--polygon', type=str, default='geojsons/simplified_GRE_2500_CW.geojson',\n",
    "                    help='The file path of a geojson file for spatial subsetting') # geojsons/west_greenland.geojson\n",
    "parser.add_argument('--is2_data_dir', type=str, default='IS2data',\n",
    "                    help='The directory into which to download ICESat-2 granules')\n",
    "parser.add_argument('--download_gtxs', type=str, default='all',\n",
    "                    help='String value or list of gtx names to download, also accepts \"all\"')\n",
    "parser.add_argument('--out_data_dir', type=str, default='detection_out_data',\n",
    "                    help='The directory to which to write the output data')\n",
    "parser.add_argument('--out_plot_dir', type=str, default='detection_out_plot',\n",
    "                    help='The directory to which to write the output plots')\n",
    "parser.add_argument('--out_stat_dir', type=str, default='detection_out_stat',\n",
    "                    help='The directory to which to write the granule stats')\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70996468-8d28-4160-adce-72446f4731e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.granule = 'ATL03_20200225170832_09310610_006_01.h5'\n",
    "args.polygon = 'geojsons/simplified_ANT_1500_East_D-Dp.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47ae168-9318-4e02-9bee-3d8b1e8e7f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(granule='ATL03_20200225170832_09310610_006_01.h5', polygon='geojsons/simplified_ANT_1500_East_D-Dp.geojson', is2_data_dir='IS2data', download_gtxs='all', out_data_dir='detection_out_data', out_plot_dir='detection_out_plot', out_stat_dir='detection_out_stat')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcaf4eb-ffde-4712-a8f6-5162b96eaa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This job is running at the following lat/lon location:32.8807000,-117.2359000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try to figure out where the script is being executed (just to show those maps at conferences, etc...)\n",
    "try:\n",
    "    with open('location-wrapper.sh', 'rb') as file: script = file.read()\n",
    "    geoip_out = subprocess.run(script, shell=True, capture_output=True)\n",
    "    compute_latlon = str(geoip_out.stdout)[str(geoip_out.stdout).find('<x><y><z>')+9 : str(geoip_out.stdout).find('<z><y><x>')]\n",
    "    print('\\nThis job is running at the following lat/lon location:%s\\n' % compute_latlon)\n",
    "except:\n",
    "    compute_latlon='0.0,0.0'\n",
    "    print('\\nUnable to determine compute location for this script.\\n')\n",
    "\n",
    "# # shuffling files around for HTCondor\n",
    "# for thispath in (args.is2_data_dir, args.out_data_dir, args.out_plot_dir):\n",
    "#     if not os.path.exists(thispath): os.makedirs(thispath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b67c0a-0854-4a36-8c03-45db04bf2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "cont = str(request.content)\n",
    "url = cont[cont.find('<downloadUrl>')+len('<downloadUrl>'):cont.find('</downloadUrl>')]\n",
    "print(url)\n",
    "print(url[url.rfind('/')+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960783a3-fae9-4a6c-86f2-790e7c0498f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "\n",
      "Downloading ICESat-2 data. Found granules:\n",
      "  ATL03_20200225170832_09310610_006_01.h5, 701.47 MB\n",
      "\n",
      "Input geojson: geojsons/simplified_ANT_1500_East_D-Dp.geojson\n",
      "Simplified polygon coordinates based on geojson input: 156.17905874417477,-69.98476868862723,156.21804632553238,-69.84541272503796,154.7172995606475,-69.3070315790863,154.45927261364304,-68.55577235450231,154.04320753981082,-68.30466822170821,153.12756990622452,-68.16919736680683,151.89853484538625,-68.25429465741725,151.33591438898628,-68.53492807680641,151.05855792471132,-68.2609288390636,149.00326113569582,-68.32571821233006,147.86164805256337,-67.99706936961458,147.31076023854274,-68.0330738179102,145.85504978856434,-67.43522629559257,146.9743926042054,-66.81838677538441,146.43950746566978,-66.58791386932268,144.88579975972291,-67.06551520354104,143.5871859402733,-66.75939732455696,142.5455080732144,-66.92922802329122,141.93282188185194,-66.69753482833914,137.51222918773453,-66.25780925636022,136.5220846259414,-66.29809908135988,134.68697220012476,-65.90208821721448,133.0278187376516,-66.36228478504863,132.46437234646768,-66.82653528189199,132.51867035970636,-67.21572899285592,134.01575951178987,-67.40713192204124,136.03223355395968,-67.2990916366246,137.6785799433695,-67.35852961812665,139.81538831203824,-67.628483662192,142.71263423348654,-68.9661738568672,145.15492758683249,-69.70612815775382,147.67154265437642,-69.9601808272749,149.26928969271643,-70.3236624689814,151.9178878012581,-70.45354368594988,152.4928779384571,-70.60235029732092,156.17905874417477,-69.98476868862723\n",
      "\n",
      "API request URL:\n",
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=ATL03&producer_granule_id=ATL03_20200225170832_09310610_006_01.h5&version=006&page_size=100&request_mode=stream&agent=NO&email=yes&page_num=1\n",
      "\n",
      "Order:  1\n",
      "Requesting...\n",
      "HTTP response from order response URL:  200\n",
      "Downloading...\n",
      "Data request 1 is complete.\n",
      "\n",
      "Unzipped files and cleaned up directory.\n",
      "Output data saved in: IS2data\n",
      "File to process: IS2data/ATL03_20200225170832_09310610_006_01.h5 (701.47 MB)\n",
      "IS2data/ATL03_20200225170832_09310610_006_01.h5 status: 200\n"
     ]
    }
   ],
   "source": [
    "# download the specified ICESat-2 data from NSIDC\n",
    "# input_filename, request_status_code = download_granule(args.granule, args.download_gtxs, args.polygon, args.is2_data_dir, \n",
    "#                                              decedc(edc().u), decedc(edc().p))\n",
    "input_filename, request_status_code = download_granule(args.granule, args.download_gtxs, args.polygon, args.is2_data_dir, \n",
    "                                             decedc(edc().u), decedc(edc().p), vars_sub='all', spatial_sub=False)\n",
    "# input_filename = 'IS2data/processed_ATL03_20200225170832_09310610_006_01.h5'\n",
    "# request_status_code = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6544e6-0262-4f26-bfd4-509d4716881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "poly_nonsimplified = args.polygon.replace('simplified_', '')\n",
    "clip_shape = gpd.read_file(poly_nonsimplified)\n",
    "\n",
    "# first just slice to the bounding box\n",
    "lons = clip_shape.loc[0].geometry.exterior.coords.xy[0]\n",
    "lats = clip_shape.loc[0].geometry.exterior.coords.xy[1]\n",
    "print('%.5f,%.5f,%.5f,%.5f' % (np.min(lons), np.min(lats), np.max(lons), np.max(lats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9adb119-3fc6-45d9-8a99-286e437f0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a bunch of checks to make sure everything went alright with the nsidc api\n",
    "print('Request status code:', request_status_code, request_status_code==200)\n",
    "if request_status_code != 200:\n",
    "    print('NSIDC API request failed.')\n",
    "    sys.exit(127)\n",
    "if request_status_code==200:\n",
    "    with open('success.txt', 'w') as f: print('we got some sweet data', file=f)\n",
    "    if input_filename == 'none': \n",
    "        print('granule seems to be empty. nothing more to do here.') \n",
    "        sys.exit(69)\n",
    "if os.path.exists(input_filename):\n",
    "    if os.path.getsize(input_filename) < 31457280:# 30 MB\n",
    "        print('granule seems to be empty. nothing more to do here.') \n",
    "        sys.exit(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac1123-9efa-4742-aab2-2504b1d6fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtx_list, ancillary = read_atl03(input_filename, gtxs_to_read='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206fb9f-caf9-484d-9322-4323e1f68cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect melt lakes\n",
    "lake_list = []\n",
    "granule_stats = [0,0,0,0]\n",
    "# for gtx in gtx_list:\n",
    "#     lakes_found, gtx_stats = detect_lakes(input_filename, gtx, args.polygon, verbose=False)\n",
    "#     for i in range(len(granule_stats)): granule_stats[i] += gtx_stats[i]\n",
    "#     lake_list += lakes_found\n",
    "#     del lakes_found, gtx_stats\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c318065-5108-4967-bd75-8192a5d04abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtx = 'gt2l'\n",
    "polygon = args.polygon\n",
    "verbose = False\n",
    "print(input_filename, gtx, polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ffc0c-8db9-48f2-8dc1-cc595937d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtx_list, ancillary, photon_data = read_atl03(input_filename, geoid_h=True, gtxs_to_read=gtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6231f3-1b3f-4d83-9339-22bd6f6490bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import Process\n",
    "df = photon_data[gtx].copy()\n",
    "start_mem = df.memory_usage().sum() / 1024**2\n",
    "print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "print('Total: {:.2f} MB'.format(Process().memory_info().rss/1024**2))\n",
    "\n",
    "df.xatc-=df.xatc.min()\n",
    "df['xatc'] = df.xatc.astype(np.float32)\n",
    "df['geoid'] = df.geoid.astype(np.float32)\n",
    "df['h'] = df.h.astype(np.float32)\n",
    "df['dt'] = df.dt.astype(np.int32)\n",
    "\n",
    "end_mem = df.memory_usage().sum() / 1024**2\n",
    "print('Memory usage of dataframe is {:.2f} MB'.format(end_mem))\n",
    "print('Total: {:.2f} MB'.format(Process().memory_info().rss/1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81acc0f-4216-4375-b38b-ddc69a5f34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dt.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700fb4f-2fcc-4b58-9edf-e8c537de97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.xatc - df.xatc.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697fa64-7340-4264-8961-41a564ac14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690b938-deec-414c-b708-9f0d9cbd39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = input_filename\n",
    "geoid_h=True\n",
    "gtxs_to_read='gt3l'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63dcf0-8a00-4ee4-826a-dd54cae69923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "input_filename = 'IS2data/processed_ATL03_20200225170832_09310610_006_01.h5'\n",
    "gtx = 'gt2r'\n",
    "gdf_poly = gpd.read_file(args.polygon)\n",
    "dfs = [gdf_poly]\n",
    "for gtx in ['gt1r', 'gt2r', 'gt3r']:\n",
    "    with h5py.File(input_filename, 'r') as f:\n",
    "        lon_ph = f[gtx]['heights']['lon_ph'][:]\n",
    "        lat_ph = f[gtx]['heights']['lat_ph'][:]\n",
    "        df = pd.DataFrame({'lat_ph': lat_ph, 'lon': lon_ph})\n",
    "        df['lat'] = np.round(df.lat_ph,2)\n",
    "        df_gt = df.groupby('lat')[['lon']].median()\n",
    "        df_gt.reset_index(inplace=True)\n",
    "        gdf_gt = gpd.GeoDataFrame(geometry=gpd.points_from_xy(df_gt.lon, df_gt.lat), crs=\"EPSG:4326\")\n",
    "        dfs.append(gdf_gt)\n",
    "\n",
    "gdf = pd.concat(dfs)\n",
    "gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d895c4c3-b61b-49e1-b037-5fd48cd505c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'IS2data/processed_ATL03_20200225170832_09310610_006_01.h5'\n",
    "gtx = 'gt2l'\n",
    "with h5py.File(input_filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg = f[gtx]['geolocation']['ph_index_beg'][:]\n",
    "fig, ax = plt.subplots(figsize=[6,3])\n",
    "ax.scatter(segment_id, ph_index_beg, s=1, c='k', alpha=0.1)\n",
    "ax.set_xlabel('%s/geolocation/segment_id' % gtx)\n",
    "ax.set_ylabel('%s/geolocation/ph_index_beg' % gtx)\n",
    "fig.tight_layout()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a0e08-65cd-40de-9e13-065d2a17846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "beam = 'gt2l'\n",
    "gtx = beam\n",
    "\n",
    "with h5py.File(input_filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg = f[gtx]['geolocation']['ph_index_beg'][:]\n",
    "    segment_ph_cnt = f[gtx]['geolocation']['segment_ph_cnt'][:]\n",
    "    segment_dist_x = f[gtx]['geolocation']['segment_dist_x'][:]\n",
    "    dist_ph_along = f[gtx]['heights']['dist_ph_along'][:]\n",
    "    pce_mframe_cnt = f[gtx]['heights']['pce_mframe_cnt'][:]\n",
    "    x_atc = f[gtx]['heights']['dist_ph_along'][:]\n",
    "    dt = f[gtx]['heights']['delta_time'][:]\n",
    "    h = f[gtx]['heights']['h_ph'][:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(segment_id, ph_index_beg, s=1, c='k', alpha=0.1)\n",
    "\n",
    "ph_index_beg = np.concatenate(([0], np.cumsum(segment_ph_cnt[:-1])))\n",
    "\n",
    "print(ph_index_beg)\n",
    "print(segment_ph_cnt)\n",
    "print(np.sum(segment_ph_cnt) - len(dt))\n",
    "print(len(dt))\n",
    "print(np.sum(segment_ph_cnt))\n",
    "print(len(segment_id))\n",
    "print(pce_mframe_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab0e93-81bd-44b0-87db-4e7029bc0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "xmin = 181907.259119056\n",
    "xmax = 192586.55220003167\n",
    "xmin = 0.259119056\n",
    "xmax = 192586.55220003167\n",
    "idxs = np.array(range(len(x_atc)))\n",
    "idx_beg = ph_index_beg[(ph_index_beg > xmin) & (ph_index_beg < xmax)]\n",
    "sel = (idxs > xmin) & (idxs < xmax)\n",
    "ax.scatter(idxs[sel], x_atc[sel], s=1, c='k', alpha=0.5)\n",
    "# ax.scatter(segment_id, ph_index_beg_, s=1, c='k', alpha=0.5)\n",
    "for idx in idx_beg:\n",
    "    ax.plot([idx]*2, [0,20], 'r-', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea727c6-fb5b-496e-8a76-74b6307bb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(segment_id, ph_index_beg_, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c046643-06c9-4dd7-b837-a01ecbe5178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=[6,3])\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg = f[gtx]['geolocation']['ph_index_beg'][:] - 1\n",
    "ax.scatter(segment_id, ph_index_beg, s=1, c='k', alpha=0.1)\n",
    "ax.set_xlabel('%s/geolocation/segment_id' % gtx)\n",
    "ax.set_ylabel('%s/geolocation/ph_index_beg' % gtx)\n",
    "fig.tight_layout()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e77798-4b50-42b8-83ba-1d3405e55d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "input_filename = 'IS2data/processed_ATL03_20200225170832_09310610_006_01.h5'\n",
    "gtx = 'gt2l'\n",
    "fig, ax = plt.subplots(figsize=[6,3])\n",
    "with h5py.File(input_filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg = f[gtx]['geolocation']['ph_index_beg'][:] - 1\n",
    "    segment_ph_cnt = f[gtx]['geolocation']['segment_ph_cnt'][:]\n",
    "    segment_dist_x = f[gtx]['geolocation']['segment_dist_x'][:]\n",
    "    dist_ph_along = f[gtx]['heights']['dist_ph_along'][:]\n",
    "\n",
    "# ax.scatter(segment_id, ph_index_beg, s=1, c='k', alpha=0.1)\n",
    "\n",
    "idxs = np.concatenate(([0], np.cumsum(segment_ph_cnt[:-1])))\n",
    "# ax.scatter(segment_id, idxs, s=1, c='k', alpha=0.1)\n",
    "# ax.scatter(segment_id, ph_index_beg-idxs, s=1, c='k', alpha=0.1)\n",
    "ax.scatter(segment_id, segment_dist_x, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5272f90-3753-4f43-8000-b8c2c4a17f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6,3])\n",
    "ax.scatter(segment_id, ph_index_beg-idxs, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2f5cc-1d14-424a-83b3-f7d79ef3b2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728f3c4-8535-4724-bb94-91506e06f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam = 'gt2r'\n",
    "gtx = beam\n",
    "gtxs_to_read=beam\n",
    "\n",
    "print('  reading in', filename)\n",
    "granule_id = filename[filename.find('ATL03_'):(filename.find('.h5')+3)]\n",
    "\n",
    "# open file\n",
    "f = h5py.File(filename, 'r')\n",
    "\n",
    "# make dictionaries for beam data to be stored in\n",
    "dfs = {}\n",
    "dfs_bckgrd = {}\n",
    "all_beams = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "beams_available = [beam for beam in all_beams if \"/%s/heights/\" % beam in f]\n",
    "\n",
    "if gtxs_to_read=='all':\n",
    "    beamlist = beams_available\n",
    "elif gtxs_to_read=='none':\n",
    "    beamlist = []\n",
    "else:\n",
    "    if type(gtxs_to_read)==list: beamlist = list(set(gtxs_to_read).intersection(set(beams_available)))\n",
    "    elif type(gtxs_to_read)==str: beamlist = list(set([gtxs_to_read]).intersection(set(beams_available)))\n",
    "    else: beamlist = beams_available\n",
    "\n",
    "conf_landice = 3 # index for the land ice confidence\n",
    "\n",
    "orient = f['orbit_info']['sc_orient'][0]\n",
    "def orient_string(sc_orient):\n",
    "    if sc_orient == 0:\n",
    "        return 'backward'\n",
    "    elif sc_orient == 1:\n",
    "        return 'forward'\n",
    "    elif sc_orient == 2:\n",
    "        return 'transition'\n",
    "    else:\n",
    "        return 'error'\n",
    "    \n",
    "orient_str = orient_string(orient)\n",
    "gtl = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "beam_strength_dict = {k:['weak','strong'][k%2] for k in np.arange(1,7,1)}\n",
    "if orient_str == 'forward':\n",
    "    bl = np.arange(6,0,-1)\n",
    "    gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "    gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "elif orient_str == 'backward':\n",
    "    bl = np.arange(1,7,1)\n",
    "    gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "    gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "else:\n",
    "    gtx_beam_dict = {k:'undefined' for k in gtl}\n",
    "    gtx_strength_dict = {k:'undefined' for k in gtl}\n",
    "    \n",
    "\n",
    "ancillary = {'granule_id': granule_id,\n",
    "             'atlas_sdp_gps_epoch': f['ancillary_data']['atlas_sdp_gps_epoch'][0],\n",
    "             'rgt': f['orbit_info']['rgt'][0],\n",
    "             'cycle_number': f['orbit_info']['cycle_number'][0],\n",
    "             'sc_orient': orient_str,\n",
    "             'gtx_beam_dict': gtx_beam_dict,\n",
    "             'gtx_strength_dict': gtx_strength_dict,\n",
    "             'gtx_dead_time_dict': {}}\n",
    "\n",
    "# loop through all beams\n",
    "print('  reading in beam:', end=' ')\n",
    "for beam in beamlist:\n",
    "    \n",
    "    print(beam, end=' ')\n",
    "    try:\n",
    "        \n",
    "        if gtx_strength_dict[beam]=='strong':\n",
    "            ancillary['gtx_dead_time_dict'][beam] = np.mean(np.array(f['ancillary_data']['calibrations']['dead_time'][beam]['dead_time'])[:16])\n",
    "        else:\n",
    "            ancillary['gtx_dead_time_dict'][beam] = np.mean(np.array(f['ancillary_data']['calibrations']['dead_time'][beam]['dead_time'])[16:])\n",
    "           \n",
    "        #### get photon-level data\n",
    "        # if \"/%s/heights/\" not in f: break; # \n",
    "         \n",
    "        df = pd.DataFrame({'lat': np.array(f[beam]['heights']['lat_ph']),\n",
    "                           'lon': np.array(f[beam]['heights']['lon_ph']),\n",
    "                           'h': np.array(f[beam]['heights']['h_ph']),\n",
    "                           'dt': np.array(f[beam]['heights']['delta_time']),\n",
    "                           # 'conf': np.array(f[beam]['heights']['signal_conf_ph'][:,conf_landice]),\n",
    "                           # not using ATL03 confidences here\n",
    "                           'mframe': np.array(f[beam]['heights']['pce_mframe_cnt']),\n",
    "                           'ph_id_pulse': np.array(f[beam]['heights']['ph_id_pulse']),\n",
    "                           'qual': np.array(f[beam]['heights']['quality_ph'])}) \n",
    "        #### calculate along-track distances [meters from the equator crossing] from segment-level data\n",
    "        segment_id = f[beam]['geolocation']['segment_id'][:]\n",
    "        n_seg = len(segment_id)\n",
    "        segment_ph_cnt = f[beam]['geolocation']['segment_ph_cnt'][:]\n",
    "        ph_index_beg = np.concatenate(([0], np.cumsum(segment_ph_cnt[:-1])))\n",
    "        segment_dist_x = f[beam]['geolocation']['segment_dist_x'][:]\n",
    "        x_atc = f[beam]['heights']['dist_ph_along'][:]\n",
    "        # for each 20m segment\n",
    "        for j,_ in enumerate(segment_id):\n",
    "            idx = ph_index_beg[j]\n",
    "            cnt = segment_ph_cnt[j]\n",
    "            # skip segments with no photon events\n",
    "            if (cnt == 0):\n",
    "                continue\n",
    "            # add segment distance to along-track coordinates\n",
    "            x_atc[idx:idx+cnt] += segment_dist_x[j]\n",
    "        df['xatc'] = x_atc\n",
    "        # ph_index_beg = np.int64(f[beam]['geolocation']['ph_index_beg']) - 1\n",
    "        # segment_dist_x = np.array(f[beam]['geolocation']['segment_dist_x'])\n",
    "        # segment_length = np.array(f[beam]['geolocation']['segment_length'])\n",
    "        # valid = ph_index_beg>=0 # need to delete values where there's no photons in the segment (-1 value)\n",
    "        # df.loc[ph_index_beg[valid], 'xatc'] = segment_dist_x[valid]\n",
    "        # df.xatc.fillna(method='ffill',inplace=True)\n",
    "        # df.xatc += np.array(f[beam]['heights']['dist_ph_along'])\n",
    "\n",
    "        #### now we can filter out TEP (we don't do IRF / afterpulses because it seems to not be very good...)\n",
    "        df.query('qual < 3',inplace=True) \n",
    "        # df.drop(columns=['qual'], inplace=True)\n",
    "\n",
    "        #### sort by along-track distance (for interpolation to work smoothly)\n",
    "        df.sort_values(by='xatc',inplace=True)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # if geoid_h:\n",
    "        #     #### interpolate geoid to photon level using along-track distance, and add to elevation\n",
    "        #     geophys_geoid = np.array(f[beam]['geophys_corr']['geoid'])\n",
    "        #     geophys_geoid_x = segment_dist_x+0.5*segment_length\n",
    "        #     valid_geoid = geophys_geoid<1e10 # filter out INVALID_R4B fill values\n",
    "        #     geophys_geoid = geophys_geoid[valid_geoid]\n",
    "        #     geophys_geoid_x = geophys_geoid_x[valid_geoid]\n",
    "        #     # hacky fix for no weird stuff happening if geoid is undefined everywhere\n",
    "        #     if len(geophys_geoid>5):\n",
    "        #         geoid = np.interp(np.array(df.xatc), geophys_geoid_x, geophys_geoid)\n",
    "        #         df['h'] = df.h - geoid\n",
    "        #         df['geoid'] = geoid\n",
    "        #         del geoid\n",
    "        #     else:\n",
    "        #         df['geoid'] = 0.0\n",
    "\n",
    "        #### save to list of dataframes\n",
    "        dfs[beam] = df\n",
    "        del df \n",
    "        gc.collect()\n",
    "        #Mdfs_bckgrd[beam] = df_bckgrd\n",
    "    \n",
    "    except:\n",
    "        print('Error for {f:s} on {b:s} ... skipping:'.format(f=filename, b=beam))\n",
    "        traceback.print_exc()\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cddda0-80a3-4bcc-81a3-53ecd2bbf509",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam = 'gt1r'\n",
    "gtx = beam\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    segment_id = f[gtx]['geolocation']['segment_id'][:]\n",
    "    ph_index_beg_ = f[gtx]['geolocation']['ph_index_beg'][:]\n",
    "    segment_ph_cnt = f[gtx]['geolocation']['segment_ph_cnt'][:]\n",
    "    segment_dist_x = f[gtx]['geolocation']['segment_dist_x'][:]\n",
    "    dist_ph_along = f[gtx]['heights']['dist_ph_along'][:]\n",
    "    pce_mframe_cnt = f[gtx]['heights']['pce_mframe_cnt'][:]\n",
    "    x_atc = f[gtx]['heights']['dist_ph_along'][:]\n",
    "    dt = f[gtx]['heights']['delta_time'][:]\n",
    "    h = f[gtx]['heights']['h_ph'][:]\n",
    "\n",
    "ph_index_beg = np.concatenate(([0], np.cumsum(segment_ph_cnt[:-1])))\n",
    "\n",
    "print(ph_index_beg)\n",
    "print(ph_index_beg_)\n",
    "print(segment_ph_cnt)\n",
    "print(np.sum(segment_ph_cnt) - len(dt))\n",
    "print(len(dt))\n",
    "print(np.sum(segment_ph_cnt))\n",
    "print(len(segment_id))\n",
    "print(pce_mframe_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1417ee5-4325-4a77-955b-667f31eb2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "xmin = 181907.259119056\n",
    "xmax = 192586.55220003167\n",
    "xmin = 0.259119056\n",
    "xmax = 192586.55220003167\n",
    "idxs = np.array(range(len(x_atc)))\n",
    "idx_beg = ph_index_beg[(ph_index_beg > xmin) & (ph_index_beg < xmax)]\n",
    "sel = (idxs > xmin) & (idxs < xmax)\n",
    "ax.scatter(idxs[sel], x_atc[sel], s=1, c='k', alpha=0.5)\n",
    "# ax.scatter(segment_id, ph_index_beg_, s=1, c='k', alpha=0.5)\n",
    "for idx in idx_beg:\n",
    "    ax.plot([idx]*2, [0,20], 'r-', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d03ed6-1313-43c7-8d16-65952612d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(x_atc, h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77950d61-9468-4501-8736-7c8c022ba9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dt, h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b393f-146b-4eab-b412-23eeddd9565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "xmin = 181907.259119056\n",
    "xmax = 192586.55220003167\n",
    "idxs = np.array(range(len(x_atc)))\n",
    "idx_beg = ph_index_beg[(ph_index_beg > xmin) & (ph_index_beg < xmax)]\n",
    "sel = (idxs > xmin) & (idxs < xmax)\n",
    "ax.scatter(idxs[sel], x_atc[sel], s=1, c='k', alpha=0.1)\n",
    "for idx in idx_beg:\n",
    "    ax.plot([idx]*2, [0,20], 'r-', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cfacc9-55c7-41e8-b1ab-d97671a4d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_index_beg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7193a4d8-c795-4e72-882d-3fe1a3b3b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_atc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03106b-dff8-43b5-b1d3-a29003e83c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x_atc > xmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca2848-1b6b-498a-94a7-c08e600b5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x_atc < xmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce67f9-6272-465e-9209-056423478b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dfs[beam].xatc, dfs[beam].h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d7f06-7424-4752-98ca-b15c5838a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dfs[beam].dt, dfs[beam].h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d44df9-8df5-4e2b-b19d-ffca89c242a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "f = h5py.File(filename, 'r')\n",
    "beam = 'gt2r'\n",
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "# ax.plot(Segment_Distance)\n",
    "# ax.plot(Segment_Index_begin)\n",
    "# ax.plot(Segment_ID)\n",
    "# ax.plot(Segment_PE_count)\n",
    "ax.plot(f[beam]['geolocation']['ph_index_beg'][:] - 1)\n",
    "ax.plot(np.array(f[beam]['geolocation']['ph_index_beg'], dtype=np.int64) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54af79f-6c71-493f-ba05-3877410e9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba1ce2-fbb9-496b-ad8f-31ffd5df249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtx = 'gt1l'\n",
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dfs[gtx].dt, dfs[gtx].h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b283c6-fdf5-444b-81ff-5936cb22f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9,5])\n",
    "ax.scatter(dfs[gtx].xatc, dfs[gtx].h, s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b46fa-5b5b-4d7b-8c37-52d615f11b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_atl03(filename, geoid_h=True, gtxs_to_read='all'):\n",
    "    \"\"\"\n",
    "    Read in an ATL03 granule. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        the file path of the granule to be read in\n",
    "    geoid_h : boolean\n",
    "        whether to include the ATL03-supplied geoid correction for photon heights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dfs : dict of pandas dataframes\n",
    "          photon-rate data with keys ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "          each dataframe contains the following variables\n",
    "          lat : float64, latitude of the photon, degrees\n",
    "          lon : float64, longitude of the photon, degrees\n",
    "          h : float64, elevation of the photon (geoid correction applied if geoid_h=True), meters\n",
    "          dt : float64, delta time of the photon, seconds from the ATLAS SDP GPS Epoch\n",
    "          mframe : uint32, the ICESat-2 major frame that the photon belongs to\n",
    "          qual : int8, quality flag 0=nominal,1=possible_afterpulse,2=possible_impulse_response_effect,3=possible_tep\n",
    "          xatc : float64, along-track distance of the photon, meters\n",
    "          geoid : float64, geoid correction that was applied to photon elevation (supplied if geoid_h=True), meters\n",
    "    dfs_bckgrd : dict of pandas dataframes\n",
    "                 photon-rate data with keys ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "                 each dataframe contains the following variables\n",
    "                 pce_mframe_cnt : int64, the major frame that the data belongs to\n",
    "                 bckgrd_counts : int32, number of background photons\n",
    "                 bckgrd_int_height : float32, height of the background window, meters\n",
    "                 delta_time : float64, Time at the start of ATLAS 50-shot sum, seconds from the ATLAS SDP GPS Epoch\n",
    "    ancillary : dictionary with the following keys:\n",
    "                granule_id : string, the producer granule id, extracted from filename\n",
    "                atlas_sdp_gps_epoch : float64, reference GPS time for ATLAS in seconds [1198800018.0]\n",
    "                rgt : int16, the reference ground track number\n",
    "                cycle_number : int8, the ICESat-2 cycle number of the granule\n",
    "                sc_orient : the spacecraft orientation (usually 'forward' or 'backward')\n",
    "                gtx_beam_dict : dictionary of the ground track / beam number configuration \n",
    "                                example: {'gt1l': 6, 'gt1r': 5, 'gt2l': 4, 'gt2r': 3, 'gt3l': 2, 'gt3r': 1}\n",
    "                gtx_strength_dict': dictionary of the ground track / beam strength configuration\n",
    "                                    example: {'gt1l': 'weak','gt1r': 'strong','gt2l': 'weak', ... }\n",
    "                                    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> read_atl03(filename='processed_ATL03_20210715182907_03381203_005_01.h5', geoid_h=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    print('  reading in', filename)\n",
    "    granule_id = filename[filename.find('ATL03_'):(filename.find('.h5')+3)]\n",
    "    \n",
    "    # open file\n",
    "    f = h5py.File(filename, 'r')\n",
    "    \n",
    "    # make dictionaries for beam data to be stored in\n",
    "    dfs = {}\n",
    "    dfs_bckgrd = {}\n",
    "    all_beams = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "    beams_available = [beam for beam in all_beams if \"/%s/heights/\" % beam in f]\n",
    "    \n",
    "    if gtxs_to_read=='all':\n",
    "        beamlist = beams_available\n",
    "    elif gtxs_to_read=='none':\n",
    "        beamlist = []\n",
    "    else:\n",
    "        if type(gtxs_to_read)==list: beamlist = list(set(gtxs_to_read).intersection(set(beams_available)))\n",
    "        elif type(gtxs_to_read)==str: beamlist = list(set([gtxs_to_read]).intersection(set(beams_available)))\n",
    "        else: beamlist = beams_available\n",
    "    \n",
    "    conf_landice = 3 # index for the land ice confidence\n",
    "    \n",
    "    orient = f['orbit_info']['sc_orient'][0]\n",
    "    def orient_string(sc_orient):\n",
    "        if sc_orient == 0:\n",
    "            return 'backward'\n",
    "        elif sc_orient == 1:\n",
    "            return 'forward'\n",
    "        elif sc_orient == 2:\n",
    "            return 'transition'\n",
    "        else:\n",
    "            return 'error'\n",
    "        \n",
    "    orient_str = orient_string(orient)\n",
    "    gtl = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "    beam_strength_dict = {k:['weak','strong'][k%2] for k in np.arange(1,7,1)}\n",
    "    if orient_str == 'forward':\n",
    "        bl = np.arange(6,0,-1)\n",
    "        gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "        gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "    elif orient_str == 'backward':\n",
    "        bl = np.arange(1,7,1)\n",
    "        gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "        gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "    else:\n",
    "        gtx_beam_dict = {k:'undefined' for k in gtl}\n",
    "        gtx_strength_dict = {k:'undefined' for k in gtl}\n",
    "        \n",
    "\n",
    "    ancillary = {'granule_id': granule_id,\n",
    "                 'atlas_sdp_gps_epoch': f['ancillary_data']['atlas_sdp_gps_epoch'][0],\n",
    "                 'rgt': f['orbit_info']['rgt'][0],\n",
    "                 'cycle_number': f['orbit_info']['cycle_number'][0],\n",
    "                 'sc_orient': orient_str,\n",
    "                 'gtx_beam_dict': gtx_beam_dict,\n",
    "                 'gtx_strength_dict': gtx_strength_dict,\n",
    "                 'gtx_dead_time_dict': {}}\n",
    "\n",
    "    # loop through all beams\n",
    "    print('  reading in beam:', end=' ')\n",
    "    for beam in beamlist:\n",
    "        \n",
    "        print(beam, end=' ')\n",
    "        try:\n",
    "            \n",
    "            if gtx_strength_dict[beam]=='strong':\n",
    "                ancillary['gtx_dead_time_dict'][beam] = np.mean(np.array(f['ancillary_data']['calibrations']['dead_time'][beam]['dead_time'])[:16])\n",
    "            else:\n",
    "                ancillary['gtx_dead_time_dict'][beam] = np.mean(np.array(f['ancillary_data']['calibrations']['dead_time'][beam]['dead_time'])[16:])\n",
    "               \n",
    "            #### get photon-level data\n",
    "            # if \"/%s/heights/\" not in f: break; # \n",
    "             \n",
    "            df = pd.DataFrame({'lat': np.array(f[beam]['heights']['lat_ph']),\n",
    "                               'lon': np.array(f[beam]['heights']['lon_ph']),\n",
    "                               'h': np.array(f[beam]['heights']['h_ph']),\n",
    "                               'dt': np.array(f[beam]['heights']['delta_time']),\n",
    "                               # 'conf': np.array(f[beam]['heights']['signal_conf_ph'][:,conf_landice]),\n",
    "                               # not using ATL03 confidences here\n",
    "                               'mframe': np.array(f[beam]['heights']['pce_mframe_cnt']),\n",
    "                               'ph_id_pulse': np.array(f[beam]['heights']['ph_id_pulse']),\n",
    "                               'qual': np.array(f[beam]['heights']['quality_ph'])}) \n",
    "                               # 0=nominal,1=afterpulse,2=impulse_response_effect,3=tep\n",
    "#            if 'weight_ph' in f[beam]['heights'].keys():\n",
    "#                 df['weight_ph'] = np.array(f[beam]['heights']['weight_ph'])\n",
    "# \n",
    "#             df_bckgrd = pd.DataFrame({'pce_mframe_cnt': np.array(f[beam]['bckgrd_atlas']['pce_mframe_cnt']),\n",
    "#                                       'bckgrd_counts': np.array(f[beam]['bckgrd_atlas']['bckgrd_counts']),\n",
    "#                                       'bckgrd_int_height': np.array(f[beam]['bckgrd_atlas']['bckgrd_int_height']),\n",
    "#                                       'delta_time': np.array(f[beam]['bckgrd_atlas']['delta_time'])})\n",
    "\n",
    "            #### calculate along-track distances [meters from the equator crossing] from segment-level data\n",
    "            df['xatc'] = np.full_like(df.lat, fill_value=np.nan)\n",
    "            ph_index_beg = np.int64(f[beam]['geolocation']['ph_index_beg']) - 1\n",
    "            segment_dist_x = np.array(f[beam]['geolocation']['segment_dist_x'])\n",
    "            segment_length = np.array(f[beam]['geolocation']['segment_length'])\n",
    "            valid = ph_index_beg>=0 # need to delete values where there's no photons in the segment (-1 value)\n",
    "            df.loc[ph_index_beg[valid], 'xatc'] = segment_dist_x[valid]\n",
    "            df.xatc.fillna(method='ffill',inplace=True)\n",
    "            df.xatc += np.array(f[beam]['heights']['dist_ph_along'])\n",
    "\n",
    "            #### now we can filter out TEP (we don't do IRF / afterpulses because it seems to not be very good...)\n",
    "            df.query('qual < 3',inplace=True) \n",
    "            # df.drop(columns=['qual'], inplace=True)\n",
    "\n",
    "            #### sort by along-track distance (for interpolation to work smoothly)\n",
    "            df.sort_values(by='xatc',inplace=True)\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            if geoid_h:\n",
    "                #### interpolate geoid to photon level using along-track distance, and add to elevation\n",
    "                geophys_geoid = np.array(f[beam]['geophys_corr']['geoid'])\n",
    "                geophys_geoid_x = segment_dist_x+0.5*segment_length\n",
    "                valid_geoid = geophys_geoid<1e10 # filter out INVALID_R4B fill values\n",
    "                geophys_geoid = geophys_geoid[valid_geoid]\n",
    "                geophys_geoid_x = geophys_geoid_x[valid_geoid]\n",
    "                # hacky fix for no weird stuff happening if geoid is undefined everywhere\n",
    "                if len(geophys_geoid>5):\n",
    "                    geoid = np.interp(np.array(df.xatc), geophys_geoid_x, geophys_geoid)\n",
    "                    df['h'] = df.h - geoid\n",
    "                    df['geoid'] = geoid\n",
    "                    del geoid\n",
    "                else:\n",
    "                    df['geoid'] = 0.0\n",
    "\n",
    "            #### save to list of dataframes\n",
    "            dfs[beam] = df\n",
    "            del df \n",
    "            gc.collect()\n",
    "            #Mdfs_bckgrd[beam] = df_bckgrd\n",
    "        \n",
    "        except:\n",
    "            print('Error for {f:s} on {b:s} ... skipping:'.format(f=filename, b=beam))\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    f.close()\n",
    "    print(' --> done.')\n",
    "    if len(beamlist)==0:\n",
    "        return beams_available, ancillary\n",
    "    else:\n",
    "        return beams_available, ancillary, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad021d-b614-41f6-8378-c759c394a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lakes(input_filename, gtx, polygon, verbose=False):\n",
    "    \n",
    "    gtx_list, ancillary, photon_data = read_atl03(input_filename, geoid_h=True, gtxs_to_read=gtx)\n",
    "    if len(photon_data)==0: return [], [0,0,0,0]\n",
    "    \n",
    "    print('\\n-----------------------------------------------------------------------------\\n')\n",
    "    print('PROCESSING GROUND TRACK: %s (%s)' % (gtx, ancillary['gtx_strength_dict'][gtx]))\n",
    "\n",
    "    # get the data frame for the gtx and aggregate info at major frame level\n",
    "    #df = photon_data[gtx]\n",
    "    #====================================================================================\n",
    "    #====================================================================================\n",
    "    #====================================================================================\n",
    "    #====================================================================================\n",
    "    #====================================================================================\n",
    "    # TODO: CLIP THE DATAFRAME TO THE NON-SIMPLIFIED POLYGON FOR THE REGION TO AVOID OVERLAP\n",
    "    poly_nonsimplified = polygon.replace('simplified_', '')\n",
    "    gdf = gpd.GeoDataFrame(photon_data[gtx], geometry=gpd.points_from_xy(photon_data[gtx].lon, photon_data[gtx].lat), crs=\"EPSG:4326\")\n",
    "    clip_shape = gpd.read_file(poly_nonsimplified)\n",
    "    gdf = gpd.clip(gdf, clip_shape).reset_index(drop=True)\n",
    "    df = pd.DataFrame(gdf.drop(columns='geometry'), copy=True)\n",
    "    photon_data = None\n",
    "    gdf = None\n",
    "    del gdf, photon_data, clip_shape\n",
    "    gc.collect()\n",
    "    \n",
    "    df_mframe = make_mframe_df(df)\n",
    "    \n",
    "    # get all the flat segments and select\n",
    "    df_mframe = find_flat_lake_surfaces(df_mframe, df)\n",
    "    df_selected = df_mframe[df_mframe.is_flat]\n",
    "    \n",
    "    # calculate densities and find second peaks (where surface is flat)\n",
    "    nsubsegs = 10\n",
    "    get_densities_and_2nd_peaks(df, df_mframe, df_selected, gtx, ancillary, n_subsegs=nsubsegs, print_results=verbose)\n",
    "    \n",
    "    # iteratively merge the detected segments into lakes \n",
    "    df_lakes = merge_lakes(df_mframe, print_progress=verbose, debug=verbose)\n",
    "    if df_lakes is None: \n",
    "        return [], [df.xatc.max()-df.xatc.min(), 0.0, df.h.count(), 0]\n",
    "    df_lakes = check_lake_surroundings(df_mframe, df_lakes)\n",
    "    calculate_remaining_densities(df, df_mframe, df_lakes, gtx, ancillary)\n",
    "    \n",
    "    # create a list of lake object, and calculate some stats for each\n",
    "    thelakes = []\n",
    "    if df_lakes is not None:\n",
    "        for i in range(len(df_lakes)):\n",
    "            lakedata = df_lakes.iloc[i]\n",
    "            thislake = melt_lake(lakedata.mframe_start, lakedata.mframe_end, lakedata.surf_elev, nsubsegs)\n",
    "            thislake.add_data(df, df_mframe, gtx, ancillary, polygon)\n",
    "            thislake.get_surface_elevation()\n",
    "            thislake.get_surface_extent()\n",
    "            thislake.calc_quality_lake()\n",
    "            thelakes.append(thislake)\n",
    "    \n",
    "    # remove any duplicates and make sure data segments don't overlap into other lakes' water surfaces\n",
    "    thelakes = remove_duplicate_lakes(thelakes, df, df_mframe, gtx, ancillary, polygon, nsubsegs, verbose=verbose)          \n",
    "    print_results(thelakes, gtx)\n",
    "    \n",
    "    # get gtx stats\n",
    "    gtx_stats = get_gtx_stats(df, thelakes)\n",
    "\n",
    "    del df, df_mframe, df_selected, df_lakes\n",
    "    gc.collect()\n",
    "    \n",
    "    return thelakes, gtx_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88ed82-bc43-469b-8347-d0514e19c361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69702c5a-80d8-4199-bd01-b113f7fbb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import icelakes\n",
    "from icelakes.utilities import encedc, decedc, get_size\n",
    "from icelakes.nsidc import download_granule, edc\n",
    "from icelakes.detection import read_atl03, detect_lakes, melt_lake\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Test script to print some stats for a given ICESat-2 ATL03 granule.')\n",
    "parser.add_argument('--granule', type=str, default='ATL03_20220714010847_03381603_006_02.h5',\n",
    "                    help='The producer_id of the input ATL03 granule')\n",
    "parser.add_argument('--polygon', type=str, default='geojsons/jakobshavn_small.geojson',\n",
    "                    help='The file path of a geojson file for spatial subsetting')\n",
    "parser.add_argument('--is2_data_dir', type=str, default='IS2data',\n",
    "                    help='The directory into which to download ICESat-2 granules')\n",
    "parser.add_argument('--download_gtxs', type=str, default='all',\n",
    "                    help='String value or list of gtx names to download, also accepts \"all\"')\n",
    "parser.add_argument('--out_data_dir', type=str, default='detection_out_data',\n",
    "                    help='The directory to which to write the output data')\n",
    "parser.add_argument('--out_plot_dir', type=str, default='detection_out_plot',\n",
    "                    help='The directory to which to write the output plots')\n",
    "parser.add_argument('--out_stat_dir', type=str, default='detection_out_stat',\n",
    "                    help='The directory to which to write the granule stats')\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4899f-93b7-46a8-a835-db906c340d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to figure out where the script is being executed (just to show those maps at conferences, etc...)\n",
    "try:\n",
    "    with open('location-wrapper.sh', 'rb') as file: script = file.read()\n",
    "    geoip_out = subprocess.run(script, shell=True, capture_output=True)\n",
    "    compute_latlon = str(geoip_out.stdout)[str(geoip_out.stdout).find('<x><y><z>')+9 : str(geoip_out.stdout).find('<z><y><x>')]\n",
    "    print('\\nThis job is running at the following lat/lon location:%s\\n' % compute_latlon)\n",
    "except:\n",
    "    compute_latlon='0.0,0.0'\n",
    "    print('\\nUnable to determine compute location for this script.\\n')\n",
    "\n",
    "# # shuffling files around for HTCondor\n",
    "# for thispath in (args.is2_data_dir, args.out_data_dir, args.out_plot_dir):\n",
    "#     if not os.path.exists(thispath): os.makedirs(thispath)\n",
    "\n",
    "# # download the specified ICESat-2 data from NSIDC\n",
    "# input_filename, request_status_code = download_granule(args.granule, args.download_gtxs, args.polygon, args.is2_data_dir, \n",
    "#                                              decedc(edc().u), decedc(edc().p))\n",
    "\n",
    "# # perform a bunch of checks to make sure everything went alright with the nsidc api\n",
    "# print('Request status code:', request_status_code, request_status_code==200)\n",
    "# if request_status_code != 200:\n",
    "#     print('NSIDC API request failed.')\n",
    "#     sys.exit(127)\n",
    "# if request_status_code==200:\n",
    "#     with open('success.txt', 'w') as f: print('we got some sweet data', file=f)\n",
    "#     if input_filename == 'none': \n",
    "#         print('granule seems to be empty. nothing more to do here.') \n",
    "#         sys.exit(69)\n",
    "# if os.path.exists(input_filename):\n",
    "#     if os.path.getsize(input_filename) < 31457280:# 30 MB\n",
    "#         print('granule seems to be empty. nothing more to do here.') \n",
    "#         sys.exit(69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a09ed9-73dc-43a7-82f7-c4f779aebb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'IS2data/processed_ATL03_20220714010847_03381603_006_02.h5'\n",
    "gtx_list, ancillary = read_atl03(input_filename, gtxs_to_read='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b823bc5-bff9-4452-bc31-9bb1e89f6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect melt lakes\n",
    "lake_list = []\n",
    "granule_stats = [0,0,0,0]\n",
    "\n",
    "for gtx in gtx_list:\n",
    "    lakes_found, gtx_stats = detect_lakes(input_filename, gtx, args.polygon, verbose=False)\n",
    "    for i in range(len(granule_stats)): granule_stats[i] += gtx_stats[i]\n",
    "    lake_list += lakes_found\n",
    "\n",
    "if granule_stats[0] > 0:\n",
    "    with open('success.txt', 'w') as f: print('we got some data from NSIDC!!', file=f)\n",
    "    print('Sucessfully retrieved data from NSIDC!!')\n",
    "    \n",
    "# print stats for granule\n",
    "print('\\nGRANULE STATS (length total, length lakes, photons total, photons lakes):%.3f,%.3f,%i,%i\\n' % tuple(granule_stats))\n",
    "\n",
    "# for each lake call the surrf algorithm for depth determination\n",
    "print('---> determining depth for each lake')\n",
    "for lake in lake_list:\n",
    "    lake.surrf()\n",
    "    print('   --> %8.3fN, %8.3fE: %6.2fm deep / quality: %8.2f' % (lake.lat,lake.lon,lake.max_depth,lake.lake_quality))\n",
    "\n",
    "# remove zero quality lakes\n",
    "lake_list[:] = [lake for lake in lake_list if lake.lake_quality > 0]\n",
    "\n",
    "for i, lake in enumerate(lake_list):\n",
    "    lake.lake_id = '%s_%s_%s_%04i' % (lake.polygon_name, lake.granule_id[:-3], lake.gtx, i)\n",
    "    filename_base = 'lake_%05i_%s_%s_%s' % (np.clip(1000-lake.lake_quality,0,None)*10, \n",
    "                                                       lake.ice_sheet, lake.melt_season, \n",
    "                                                       lake.lake_id)\n",
    "    # plot each lake and save to image\n",
    "    fig = lake.plot_lake(closefig=False)\n",
    "    figname = args.out_plot_dir + '/%s.jpg' % filename_base\n",
    "    if fig is not None: fig.savefig(figname, dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # export each lake to h5 and pickle\n",
    "    try:\n",
    "        h5name = args.out_data_dir + '/%s.h5' % filename_base\n",
    "        datafile = lake.write_to_hdf5(h5name)\n",
    "        print('Wrote data file: %s, %s' % (datafile, get_size(datafile)))\n",
    "    except:\n",
    "        print('Could not write hdf5 file.')\n",
    "        try:\n",
    "            pklname = args.out_data_dir + '/%s.pkl' % filename_base\n",
    "            with open(pklname, 'wb') as f: pickle.dump(vars(lake), f)\n",
    "            print('Wrote data file: %s, %s' % (pklname, get_size(pklname)))\n",
    "        except:\n",
    "            print('Could not write pickle file.')\n",
    "\n",
    "statsfname = args.out_stat_dir + '/stats_%s_%s.csv' % (args.polygon[args.polygon.rfind('/')+1:].replace('.geojson',''), args.granule[:-4])\n",
    "with open(statsfname, 'w') as f: print('%.3f,%.3f,%i,%i,%s' % tuple(granule_stats+[compute_latlon]), file=f)\n",
    "    \n",
    "# clean up the input data\n",
    "# os.remove(input_filename)\n",
    "\n",
    "print('\\n-------------------------------------------------')\n",
    "print(  '----------->   Python script done!   <-----------')\n",
    "print(  '-------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc32d59-f929-482f-beea-962493f5e633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ca9dc-a87f-424e-8efa-f95b2d08cf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e77f98-bf4e-438d-85fc-82b6a3812259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2ce86-20b6-4fc7-bc96-4f301f11ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pylab as plt\n",
    "from cmcrameri import cm as cmc\n",
    "lk = lake_list[2]\n",
    "dfp = lk.photon_data\n",
    "dfs = dfp[~dfp.is_afterpulse]\n",
    "dfap = dfp[dfp.is_afterpulse]\n",
    "fig, ax = plt.subplots(figsize=[8, 4.5], dpi=100)\n",
    "ax.scatter(dfs.xatc, dfs.h-lk.surface_elevation, s=1, c=dfs.snr, cmap=cmc.batlow_r, vmin=0, vmax=1)\n",
    "ax.scatter(dfap.xatc, dfap.h-lk.surface_elevation, s=1, c=dfap.snr, cmap=cmc.batlow_r, alpha=0.2, vmin=0, vmax=1)\n",
    "# ax.scatter(dfp.xatc, dfp.h, s=10, c='g')\n",
    "dfg = dfs.groupby('pulseid').mean()\n",
    "ax.scatter(dfg.xatc, dfg.sat_ratio.rolling(20,center=True).mean(), s=1, c=dfg.sat_ratio, cmap=cmc.roma_r, alpha=0.2, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f0e4c-2dea-4054-86bd-441895e292ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.groupby('pulseid').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d7583-f13a-4673-b921-7cb7e462b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224104a0-1116-43b1-ad97-e9233393c810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0153e-683c-41a7-9a73-d46a41584ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fc516-572f-4a62-a678-e8ee4bf016f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "import rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9553cd-2c69-40f8-811f-2da52f5cc2ef",
   "metadata": {},
   "source": [
    "# arguments for future script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32119103-fcf1-48f9-bb42-63cd9fb61e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule = 'ATL03_20210715182907_03381203_005_01.h5'\n",
    "shapefile = '/shapefiles/jakobshavn_small.shp'\n",
    "gtxs = 'gt1l'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23f7d2-54f8-48e1-9577-4d2d3a068333",
   "metadata": {},
   "source": [
    "# download the specified granule via NSIDC\n",
    "...and subset to the provided shapefile / only pull the variables needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08889d3-fb53-4e97-8899-306ca3187111",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/IS2data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75ad7b-1857-4370-b8ae-81f60ddb48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture celloutput \n",
    "download_granule_nsidc(granule, gtxs, shapefile, datadir, decedc(edc().u), decedc(edc().p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58859ed9-f9ec-4aa5-9b5e-abd1d5836adf",
   "metadata": {},
   "source": [
    "# read in the .h5 subsetted granule file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e324277-c1da-4634-be54-7b0c2455dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [datadir[1:]+'/'+f for f in listdir(datadir[1:]) if isfile(join(datadir[1:], f)) & ('.h5' in f)]\n",
    "print('\\nNumber of processed ATL03 granules to read in: ' + str(len(filelist)))\n",
    "    \n",
    "photon_data, bckgrd_data, ancillary = read_atl03(filelist[0], geoid_h=True)\n",
    "print_granule_stats(photon_data, bckgrd_data, ancillary, outfile='stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ca06e-4313-48dc-8811-090055eaf0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e13da2-edd2-41d6-8323-1e6769230eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icelakes-env",
   "language": "python",
   "name": "icelakes-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2df2d51-5780-457b-9fe7-f5e12c5ae65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry.polygon import orient\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from xml.etree import ElementTree as ET\n",
    "from icelakes.utilities import get_size\n",
    "\n",
    "from icelakes.utilities import encedc, decedc, get_size\n",
    "from icelakes.nsidc import download_granule, edc\n",
    "from icelakes.detection import read_atl03, detect_lakes, melt_lake\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import io\n",
    "\n",
    "def download_granule(granule_id, gtxs, geojson, granule_output_path, uid, pwd, vars_sub='default', spatial_sub=True, request_mode='async'): \n",
    "    \n",
    "    print('--> parameters: granule_id = %s' % granule_id)\n",
    "    print('                gtxs = %s' % gtxs)\n",
    "    print('                geojson = %s' % geojson)\n",
    "    print('                granule_output_path = %s' % granule_output_path)\n",
    "    print('                vars_sub = %s' % vars_sub)\n",
    "    print('                spatial_sub = %s\\n' % spatial_sub)\n",
    "    \n",
    "    short_name = 'ATL03'\n",
    "    version = granule_id[30:33]\n",
    "    granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "    capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{version}.xml'\n",
    "    base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "    \n",
    "    geojson_filepath = str(os.getcwd() + '/' + geojson)\n",
    "    \n",
    "    # set the variables for subsetting\n",
    "    if vars_sub == 'default':\n",
    "        vars_sub = ['/ancillary_data/atlas_sdp_gps_epoch',\n",
    "                    '/ancillary_data/calibrations/dead_time/gtx',\n",
    "                    '/orbit_info/rgt',\n",
    "                    '/orbit_info/cycle_number',\n",
    "                    '/orbit_info/sc_orient',\n",
    "                    '/gtx/geolocation/segment_id',\n",
    "                    '/gtx/geolocation/ph_index_beg',\n",
    "                    '/gtx/geolocation/segment_dist_x',\n",
    "                    '/gtx/geolocation/segment_length',\n",
    "                    '/gtx/geolocation/segment_ph_cnt',\n",
    "                    # '/gtx/geophys_corr/dem_h',\n",
    "                    '/gtx/geophys_corr/geoid',\n",
    "                    '/gtx/bckgrd_atlas/pce_mframe_cnt',\n",
    "                    '/gtx/bckgrd_atlas/tlm_height_band1',\n",
    "                    '/gtx/bckgrd_atlas/tlm_height_band2',\n",
    "                    '/gtx/bckgrd_atlas/tlm_top_band1',\n",
    "                    '/gtx/bckgrd_atlas/tlm_top_band2',\n",
    "                    # '/gtx/bckgrd_atlas/bckgrd_counts',\n",
    "                    # '/gtx/bckgrd_atlas/bckgrd_int_height',\n",
    "                    # '/gtx/bckgrd_atlas/delta_time',\n",
    "                    '/gtx/heights/lat_ph',\n",
    "                    '/gtx/heights/lon_ph',\n",
    "                    '/gtx/heights/h_ph',\n",
    "                    '/gtx/heights/delta_time',\n",
    "                    '/gtx/heights/dist_ph_along',\n",
    "                    '/gtx/heights/quality_ph',\n",
    "                    # '/gtx/heights/signal_conf_ph',\n",
    "                    '/gtx/heights/pce_mframe_cnt',\n",
    "                    '/gtx/heights/ph_id_pulse'\n",
    "                    ]\n",
    "        if int(version) > 5:\n",
    "            vars_sub.append('/gtx/heights/weight_ph')\n",
    "    beam_list = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "    \n",
    "    if gtxs == 'all':\n",
    "        var_list = sum([[v.replace('/gtx','/'+bm) for bm in beam_list] if '/gtx' in v else [v] for v in vars_sub],[])\n",
    "    elif type(gtxs) == str:\n",
    "        var_list = [v.replace('/gtx','/'+gtxs.lower()) if '/gtx' in v else v for v in vars_sub]\n",
    "    elif type(gtxs) == list:\n",
    "        var_list = sum([[v.replace('/gtx','/'+bm.lower()) for bm in gtxs] if '/gtx' in v else [v] for v in vars_sub],[])\n",
    "    else: # default to requesting all beams\n",
    "        var_list = sum([[v.replace('/gtx','/'+bm) for bm in beam_list] if '/gtx' in v else [v] for v in vars_sub],[])\n",
    "    \n",
    "    # search for the given granule\n",
    "    search_params = {\n",
    "        'short_name': short_name,\n",
    "        'page_size': 100,\n",
    "        'page_num': 1,\n",
    "        'producer_granule_id': granule_id}\n",
    "    \n",
    "    granules = []\n",
    "    headers={'Accept': 'application/json'}\n",
    "    while True:\n",
    "        response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "        results = json.loads(response.content)\n",
    "    \n",
    "        if len(results['feed']['entry']) == 0:\n",
    "            # Out of results, so break out of loop\n",
    "            break\n",
    "    \n",
    "        # Collect results and increment page_num\n",
    "        granules.extend(results['feed']['entry'])\n",
    "        search_params['page_num'] += 1\n",
    "        \n",
    "    granule_list, idx_unique = np.unique(np.array([g['producer_granule_id'] for g in granules]), return_index=True)\n",
    "    granules = [g for i,g in enumerate(granules) if i in idx_unique] # keeps double counting, not sure why\n",
    "    print('\\nDownloading ICESat-2 data. Found granules:')\n",
    "    if len(granules) == 0:\n",
    "        print('None')\n",
    "        return 'none', 404\n",
    "    for result in granules:\n",
    "        print('  '+result['producer_granule_id'], f', {float(result[\"granule_size\"]):.2f} MB',sep='')\n",
    "        \n",
    "    \n",
    "    gdf = gpd.read_file(geojson_filepath)\n",
    "    poly = orient(gdf.loc[0].geometry,sign=1.0)\n",
    "    geojson_data = gpd.GeoSeries(poly).to_json() # Convert to geojson\n",
    "    geojson_data = geojson_data.replace(' ', '') #remove spaces for API call\n",
    "    \n",
    "    #Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "    polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "    \n",
    "    print('\\nInput geojson:', geojson)\n",
    "    print('Simplified polygon coordinates based on geojson input:', polygon)\n",
    "    \n",
    "    # Create session to store cookie and pass credentials to capabilities url\n",
    "    session = requests.session()\n",
    "    s = session.get(capability_url)\n",
    "    response = session.get(s.url,auth=(uid,pwd))\n",
    "    \n",
    "    try:\n",
    "        root = ET.fromstring(response.content)\n",
    "    except:\n",
    "        try:\n",
    "            cont = str(request._content)\n",
    "            print('request status code:', request.status_code)\n",
    "            the_code = cont[cont.find('<Code>')+6:cont.find('</Code>')]\n",
    "            if len(the_code) < 1000:\n",
    "                print(the_code)\n",
    "            the_message = cont[cont.find('<Message>')+9:cont.find('</Message>')]\n",
    "            if len(the_message) < 5000:\n",
    "                print(the_message)\n",
    "            print('')\n",
    "            return 'none', response.status_code\n",
    "        except:\n",
    "            return 'none', response.status_code\n",
    "    \n",
    "    #collect lists with each service option\n",
    "    subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "    \n",
    "    # this is for getting possible variable values from the granule search\n",
    "    if len(subagent) > 0 :\n",
    "        # variable subsetting\n",
    "        variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "        variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "        variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "        variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "    \n",
    "    # make sure to only request the variables that are available\n",
    "    def intersection(lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3\n",
    "    if vars_sub == 'all':\n",
    "        var_list_subsetting = ''\n",
    "    else:\n",
    "        var_list_subsetting = intersection(variable_vals,var_list)\n",
    "    \n",
    "    if len(subagent) < 1 :\n",
    "        print('No services exist for', short_name, 'version', latest_version)\n",
    "        agent = 'NO'\n",
    "        coverage,Boundingshape,polygon = '','',''\n",
    "    else:\n",
    "        agent = ''\n",
    "        subdict = subagent[0]\n",
    "        if (subdict['spatialSubsettingShapefile'] == 'true') and spatial_sub:\n",
    "            ######################################## Boundingshape = geojson_data\n",
    "            Boundingshape = polygon\n",
    "        else:\n",
    "            Boundingshape, polygon = '',''\n",
    "        coverage = ','.join(var_list_subsetting)\n",
    "    if (vars_sub=='all') & (not spatial_sub):\n",
    "        agent = 'NO'\n",
    "        \n",
    "    page_size = 100\n",
    "    page_num = int(np.ceil(len(granules)/page_size))\n",
    "    \n",
    "    param_dict = {'short_name': short_name, \n",
    "                  'producer_granule_id': granule_id,\n",
    "                  'version': version,  \n",
    "                  'polygon': polygon,\n",
    "                  'Boundingshape': Boundingshape,  \n",
    "                  'Coverage': coverage, \n",
    "                  'page_size': page_size, \n",
    "                  'request_mode': request_mode, \n",
    "                  'agent': agent, \n",
    "                  'email': 'yes'}\n",
    "    \n",
    "    #Remove blank key-value-pairs\n",
    "    param_dict = {k: v for k, v in param_dict.items() if v != ''}\n",
    "    \n",
    "    #Convert to string\n",
    "    param_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in param_dict.items())\n",
    "    param_string = param_string.replace(\"'\",\"\")\n",
    "    \n",
    "    #Print API base URL + request parameters\n",
    "    endpoint_list = [] \n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        API_request = api_request = f'{base_url}?{param_string}&page_num={page_val}'\n",
    "        endpoint_list.append(API_request)\n",
    "    \n",
    "    print('\\nAPI request URL:')\n",
    "    print(*endpoint_list, sep = \"\\n\") \n",
    "    \n",
    "    # Create an output folder if the folder does not already exist.\n",
    "    path = str(os.getcwd() + '/' + granule_output_path)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    # if asynchronous request\n",
    "    if request_mode=='async':\n",
    "        # Request data service for each page number, and unzip outputs\n",
    "        for i in range(page_num):\n",
    "            page_val = i + 1\n",
    "            print('Order: ', page_val)\n",
    "    \n",
    "        # For all requests other than spatial file upload, use get function\n",
    "            param_dict['page_num'] = page_val\n",
    "            request = session.get(base_url, params=param_dict)\n",
    "    \n",
    "            print('Request HTTP response: ', request.status_code)\n",
    "    \n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "            request.raise_for_status()\n",
    "            # print('Order request URL: ', request.url)\n",
    "            esir_root = ET.fromstring(request.content)\n",
    "            # print('Order request response XML content: ', request.content)\n",
    "    \n",
    "        #Look up order ID\n",
    "            orderlist = []   \n",
    "            for order in esir_root.findall(\"./order/\"):\n",
    "                orderlist.append(order.text)\n",
    "            orderID = orderlist[0]\n",
    "            print('order ID: ', orderID)\n",
    "    \n",
    "        #Create status URL\n",
    "            statusURL = base_url + '/' + orderID\n",
    "            print('status URL: ', statusURL)\n",
    "    \n",
    "        #Find order status\n",
    "            request_response = session.get(statusURL)    \n",
    "            print('HTTP response from order response URL: ', request_response.status_code)\n",
    "    \n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "            request_response.raise_for_status()\n",
    "            request_root = ET.fromstring(request_response.content)\n",
    "            statuslist = []\n",
    "            for status in request_root.findall(\"./requestStatus/\"):\n",
    "                statuslist.append(status.text)\n",
    "            status = statuslist[0]\n",
    "            print('Data request ', page_val, ' is submitting...')\n",
    "            print('Initial request status is ', status)\n",
    "    \n",
    "        #Continue loop while request is still processing\n",
    "            ith_loop = 0\n",
    "            while ((status == 'pending') or (status == 'processing')) and (ith_loop < 720): \n",
    "                ith_loop += 1\n",
    "                print('  Status is not complete. Trying again.')\n",
    "                time.sleep(10)\n",
    "                loop_response = session.get(statusURL)\n",
    "    \n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "                loop_response.raise_for_status()\n",
    "                loop_root = ET.fromstring(loop_response.content)\n",
    "    \n",
    "        #find status\n",
    "                statuslist = []\n",
    "                for status in loop_root.findall(\"./requestStatus/\"):\n",
    "                    statuslist.append(status.text)\n",
    "                status = statuslist[0]\n",
    "                print('  Retry request status is: ', status)\n",
    "                if status == 'pending' or status == 'processing':\n",
    "                    continue\n",
    "    \n",
    "        #Order can either complete, complete_with_errors, or fail:\n",
    "        # Provide complete_with_errors error message:\n",
    "            if status == 'complete_with_errors' or status == 'failed':\n",
    "                messagelist = []\n",
    "                for message in loop_root.findall(\"./processInfo/\"):\n",
    "                    messagelist.append(message.text)\n",
    "                print('error messages:')\n",
    "                print(messagelist)\n",
    "    \n",
    "        # Download zipped order if status is complete or complete_with_errors\n",
    "            downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "            zip_response = session.get(downloadURL)\n",
    "            this_status_code = zip_response.status_code\n",
    "            if status == 'complete' or status == 'complete_with_errors':\n",
    "                print('Zip download URL: ', downloadURL)\n",
    "                print('Beginning download of zipped output...')\n",
    "                # Raise bad request: Loop will stop for bad response code.\n",
    "                zip_response.raise_for_status()\n",
    "                with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "                    z.extractall(path)\n",
    "                print('Data request', page_val, 'is complete.')\n",
    "            else: \n",
    "                print('Request failed.')\n",
    "    \n",
    "    # if stream (synchronous) request\n",
    "    else:\n",
    "        for i in range(page_num):\n",
    "            page_val = i + 1\n",
    "            print('\\nOrder: ', page_val)\n",
    "            print('Requesting...')\n",
    "            request = session.get(base_url, params=param_dict)\n",
    "            this_status_code = request.status_code\n",
    "            print('HTTP response from order response URL: ', this_status_code)\n",
    "            request.raise_for_status()\n",
    "            d = request.headers['content-disposition']\n",
    "            fname = re.findall('filename=(.+)', d)\n",
    "            dirname = os.path.join(path,fname[0].strip('\\\"'))\n",
    "            print('Downloading...')\n",
    "            open(dirname, 'wb').write(request.content)\n",
    "            print('Data request', page_val, 'is complete.')\n",
    "    \n",
    "    # Unzip outputs\n",
    "    for z in os.listdir(path): \n",
    "        if z.endswith('.zip'): \n",
    "            zip_name = path + \"/\" + z \n",
    "            zip_ref = zipfile.ZipFile(zip_name) \n",
    "            zip_ref.extractall(path) \n",
    "            zip_ref.close() \n",
    "            os.remove(zip_name) \n",
    "    \n",
    "    # Clean up Outputs folder by removing individual granule folders \n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for file in files:\n",
    "            try:\n",
    "                shutil.move(os.path.join(root, file), os.path.join(path, file))\n",
    "            except OSError:\n",
    "                pass\n",
    "        for name in dirs:\n",
    "            # os.rmdir(os.path.join(root, name))\n",
    "            shutil.rmtree(os.path.join(root, name))\n",
    "            \n",
    "    print('\\nUnzipped files and cleaned up directory.')\n",
    "    print('Output data saved in:', granule_output_path)\n",
    "    \n",
    "    filelist = [granule_output_path+'/'+f for f in os.listdir(granule_output_path) \\\n",
    "                if os.path.isfile(os.path.join(granule_output_path, f)) & (granule_id in f)]\n",
    "    \n",
    "    if len(filelist) == 0: \n",
    "        return 'none'\n",
    "    else:\n",
    "        filename = filelist[0]\n",
    "    print('File to process: %s (%s)' % (filename, get_size(filename)))\n",
    "    \n",
    "    print('status:', this_status_code)\n",
    "    \n",
    "    return filename, this_status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee752606-3296-4d9b-886d-54b10a7f5db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> parameters: granule_id = ATL03_20230806063138_07192003_006_02.h5\n",
      "                gtxs = all\n",
      "                geojson = geojsons/teslake_gris_bounding_box.geojson\n",
      "                granule_output_path = zzzzz_test_IS2data\n",
      "                vars_sub = default\n",
      "                spatial_sub = True\n",
      "\n",
      "\n",
      "Downloading ICESat-2 data. Found granules:\n",
      "  ATL03_20230806063138_07192003_006_02.h5, 1222.72 MB\n",
      "\n",
      "Input geojson: geojsons/teslake_gris_bounding_box.geojson\n",
      "Simplified polygon coordinates based on geojson input: -48.19451,68.13085,-48.1538,68.13085,-48.1538,68.25982,-48.19451,68.25982,-48.19451,68.13085\n",
      "\n",
      "API request URL:\n",
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=ATL03&producer_granule_id=ATL03_20230806063138_07192003_006_02.h5&version=006&polygon=-48.19451,68.13085,-48.1538,68.13085,-48.1538,68.25982,-48.19451,68.25982,-48.19451,68.13085&Boundingshape=-48.19451,68.13085,-48.1538,68.13085,-48.1538,68.25982,-48.19451,68.25982,-48.19451,68.13085&Coverage=/ancillary_data/atlas_sdp_gps_epoch,/ancillary_data/calibrations/dead_time/gt1l,/ancillary_data/calibrations/dead_time/gt1l,/ancillary_data/calibrations/dead_time/gt1r,/ancillary_data/calibrations/dead_time/gt1r,/ancillary_data/calibrations/dead_time/gt2l,/ancillary_data/calibrations/dead_time/gt2l,/ancillary_data/calibrations/dead_time/gt2r,/ancillary_data/calibrations/dead_time/gt2r,/ancillary_data/calibrations/dead_time/gt3l,/ancillary_data/calibrations/dead_time/gt3l,/ancillary_data/calibrations/dead_time/gt3r,/ancillary_data/calibrations/dead_time/gt3r,/gt1l/bckgrd_atlas/pce_mframe_cnt,/gt1l/bckgrd_atlas/tlm_height_band1,/gt1l/bckgrd_atlas/tlm_height_band2,/gt1l/bckgrd_atlas/tlm_top_band1,/gt1l/bckgrd_atlas/tlm_top_band2,/gt1l/geolocation/ph_index_beg,/gt1l/geolocation/segment_dist_x,/gt1l/geolocation/segment_id,/gt1l/geolocation/segment_length,/gt1l/geolocation/segment_ph_cnt,/gt1l/geophys_corr/geoid,/gt1l/heights/delta_time,/gt1l/heights/dist_ph_along,/gt1l/heights/h_ph,/gt1l/heights/lat_ph,/gt1l/heights/lon_ph,/gt1l/heights/pce_mframe_cnt,/gt1l/heights/ph_id_pulse,/gt1l/heights/quality_ph,/gt1l/heights/weight_ph,/gt1r/bckgrd_atlas/pce_mframe_cnt,/gt1r/bckgrd_atlas/tlm_height_band1,/gt1r/bckgrd_atlas/tlm_height_band2,/gt1r/bckgrd_atlas/tlm_top_band1,/gt1r/bckgrd_atlas/tlm_top_band2,/gt1r/geolocation/ph_index_beg,/gt1r/geolocation/segment_dist_x,/gt1r/geolocation/segment_id,/gt1r/geolocation/segment_length,/gt1r/geolocation/segment_ph_cnt,/gt1r/geophys_corr/geoid,/gt1r/heights/delta_time,/gt1r/heights/dist_ph_along,/gt1r/heights/h_ph,/gt1r/heights/lat_ph,/gt1r/heights/lon_ph,/gt1r/heights/pce_mframe_cnt,/gt1r/heights/ph_id_pulse,/gt1r/heights/quality_ph,/gt1r/heights/weight_ph,/gt2l/bckgrd_atlas/pce_mframe_cnt,/gt2l/bckgrd_atlas/tlm_height_band1,/gt2l/bckgrd_atlas/tlm_height_band2,/gt2l/bckgrd_atlas/tlm_top_band1,/gt2l/bckgrd_atlas/tlm_top_band2,/gt2l/geolocation/ph_index_beg,/gt2l/geolocation/segment_dist_x,/gt2l/geolocation/segment_id,/gt2l/geolocation/segment_length,/gt2l/geolocation/segment_ph_cnt,/gt2l/geophys_corr/geoid,/gt2l/heights/delta_time,/gt2l/heights/dist_ph_along,/gt2l/heights/h_ph,/gt2l/heights/lat_ph,/gt2l/heights/lon_ph,/gt2l/heights/pce_mframe_cnt,/gt2l/heights/ph_id_pulse,/gt2l/heights/quality_ph,/gt2l/heights/weight_ph,/gt2r/bckgrd_atlas/pce_mframe_cnt,/gt2r/bckgrd_atlas/tlm_height_band1,/gt2r/bckgrd_atlas/tlm_height_band2,/gt2r/bckgrd_atlas/tlm_top_band1,/gt2r/bckgrd_atlas/tlm_top_band2,/gt2r/geolocation/ph_index_beg,/gt2r/geolocation/segment_dist_x,/gt2r/geolocation/segment_id,/gt2r/geolocation/segment_length,/gt2r/geolocation/segment_ph_cnt,/gt2r/geophys_corr/geoid,/gt2r/heights/delta_time,/gt2r/heights/dist_ph_along,/gt2r/heights/h_ph,/gt2r/heights/lat_ph,/gt2r/heights/lon_ph,/gt2r/heights/pce_mframe_cnt,/gt2r/heights/ph_id_pulse,/gt2r/heights/quality_ph,/gt2r/heights/weight_ph,/gt3l/bckgrd_atlas/pce_mframe_cnt,/gt3l/bckgrd_atlas/tlm_height_band1,/gt3l/bckgrd_atlas/tlm_height_band2,/gt3l/bckgrd_atlas/tlm_top_band1,/gt3l/bckgrd_atlas/tlm_top_band2,/gt3l/geolocation/ph_index_beg,/gt3l/geolocation/segment_dist_x,/gt3l/geolocation/segment_id,/gt3l/geolocation/segment_length,/gt3l/geolocation/segment_ph_cnt,/gt3l/geophys_corr/geoid,/gt3l/heights/delta_time,/gt3l/heights/dist_ph_along,/gt3l/heights/h_ph,/gt3l/heights/lat_ph,/gt3l/heights/lon_ph,/gt3l/heights/pce_mframe_cnt,/gt3l/heights/ph_id_pulse,/gt3l/heights/quality_ph,/gt3l/heights/weight_ph,/gt3r/bckgrd_atlas/pce_mframe_cnt,/gt3r/bckgrd_atlas/tlm_height_band1,/gt3r/bckgrd_atlas/tlm_height_band2,/gt3r/bckgrd_atlas/tlm_top_band1,/gt3r/bckgrd_atlas/tlm_top_band2,/gt3r/geolocation/ph_index_beg,/gt3r/geolocation/segment_dist_x,/gt3r/geolocation/segment_id,/gt3r/geolocation/segment_length,/gt3r/geolocation/segment_ph_cnt,/gt3r/geophys_corr/geoid,/gt3r/heights/delta_time,/gt3r/heights/dist_ph_along,/gt3r/heights/h_ph,/gt3r/heights/lat_ph,/gt3r/heights/lon_ph,/gt3r/heights/pce_mframe_cnt,/gt3r/heights/ph_id_pulse,/gt3r/heights/quality_ph,/gt3r/heights/weight_ph,/orbit_info/cycle_number,/orbit_info/rgt,/orbit_info/sc_orient&page_size=100&request_mode=async&email=yes&page_num=1\n",
      "Order:  1\n",
      "Request HTTP response:  201\n",
      "order ID:  5000005590472\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000005590472\n",
      "HTTP response from order response URL:  201\n",
      "Data request  1  is submitting...\n",
      "Initial request status is  processing\n",
      "  Status is not complete. Trying again.\n",
      "  Retry request status is:  complete\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000005590472.zip\n",
      "Beginning download of zipped output...\n",
      "Data request 1 is complete.\n",
      "\n",
      "Unzipped files and cleaned up directory.\n",
      "Output data saved in: zzzzz_test_IS2data\n",
      "File to process: zzzzz_test_IS2data/processed_ATL03_20230806063138_07192003_006_02.h5 (3.61 MB)\n",
      "status: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('zzzzz_test_IS2data/processed_ATL03_20230806063138_07192003_006_02.h5', 200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "granule_id = 'ATL03_20230806063138_07192003_006_02.h5'\n",
    "geojson = 'geojsons/teslake_gris_bounding_box.geojson'\n",
    "granule_output_path = 'zzzzz_test_IS2data'\n",
    "gtxs = 'all'\n",
    "uid = decedc(edc().u)\n",
    "pwd = decedc(edc().p)\n",
    "vars_sub='default'\n",
    "spatial_sub=True\n",
    "request_mode='async'\n",
    "\n",
    "download_granule(granule_id, gtxs, geojson, granule_output_path, uid, pwd, vars_sub='default', spatial_sub=True, request_mode='async')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d34fb131-a8d3-4639-a7d1-9dd2af0e2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> parameters: granule_id = ATL03_20230806063138_07192003_006_02.h5\n",
      "                gtxs = all\n",
      "                geojson = geojsons/teslake_gris_bounding_box.geojson\n",
      "                granule_output_path = zzzzz_test_IS2data\n",
      "                vars_sub = default\n",
      "                spatial_sub = True\n",
      "\n",
      "\n",
      "Downloading ICESat-2 data. Found granules:\n",
      "  ATL03_20230806063138_07192003_006_02.h5, 1222.72 MB\n",
      "\n",
      "Input geojson: geojsons/teslake_gris_bounding_box.geojson\n",
      "Simplified polygon coordinates based on geojson input: -48.19451,68.13085,-48.1538,68.13085,-48.1538,68.25982,-48.19451,68.25982,-48.19451,68.13085\n",
      "\n",
      "API request URL:\n",
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=ATL03&producer_granule_id=ATL03_20230806063138_07192003_006_02.h5&version=006&polygon=-48.19451,68.13085,-48.1538,68.13085,-48.1538,68.25982,-48.19451,68.25982,-48.19451,68.13085&Boundingshape=-48.19451,68.13085,-48.1538,68.13085,-48.1538,68.25982,-48.19451,68.25982,-48.19451,68.13085&Coverage=/ancillary_data/atlas_sdp_gps_epoch,/ancillary_data/calibrations/dead_time/gt1l,/ancillary_data/calibrations/dead_time/gt1l,/ancillary_data/calibrations/dead_time/gt1r,/ancillary_data/calibrations/dead_time/gt1r,/ancillary_data/calibrations/dead_time/gt2l,/ancillary_data/calibrations/dead_time/gt2l,/ancillary_data/calibrations/dead_time/gt2r,/ancillary_data/calibrations/dead_time/gt2r,/ancillary_data/calibrations/dead_time/gt3l,/ancillary_data/calibrations/dead_time/gt3l,/ancillary_data/calibrations/dead_time/gt3r,/ancillary_data/calibrations/dead_time/gt3r,/gt1l/bckgrd_atlas/pce_mframe_cnt,/gt1l/bckgrd_atlas/tlm_height_band1,/gt1l/bckgrd_atlas/tlm_height_band2,/gt1l/bckgrd_atlas/tlm_top_band1,/gt1l/bckgrd_atlas/tlm_top_band2,/gt1l/geolocation/ph_index_beg,/gt1l/geolocation/segment_dist_x,/gt1l/geolocation/segment_id,/gt1l/geolocation/segment_length,/gt1l/geolocation/segment_ph_cnt,/gt1l/geophys_corr/geoid,/gt1l/heights/delta_time,/gt1l/heights/dist_ph_along,/gt1l/heights/h_ph,/gt1l/heights/lat_ph,/gt1l/heights/lon_ph,/gt1l/heights/pce_mframe_cnt,/gt1l/heights/ph_id_pulse,/gt1l/heights/quality_ph,/gt1l/heights/weight_ph,/gt1r/bckgrd_atlas/pce_mframe_cnt,/gt1r/bckgrd_atlas/tlm_height_band1,/gt1r/bckgrd_atlas/tlm_height_band2,/gt1r/bckgrd_atlas/tlm_top_band1,/gt1r/bckgrd_atlas/tlm_top_band2,/gt1r/geolocation/ph_index_beg,/gt1r/geolocation/segment_dist_x,/gt1r/geolocation/segment_id,/gt1r/geolocation/segment_length,/gt1r/geolocation/segment_ph_cnt,/gt1r/geophys_corr/geoid,/gt1r/heights/delta_time,/gt1r/heights/dist_ph_along,/gt1r/heights/h_ph,/gt1r/heights/lat_ph,/gt1r/heights/lon_ph,/gt1r/heights/pce_mframe_cnt,/gt1r/heights/ph_id_pulse,/gt1r/heights/quality_ph,/gt1r/heights/weight_ph,/gt2l/bckgrd_atlas/pce_mframe_cnt,/gt2l/bckgrd_atlas/tlm_height_band1,/gt2l/bckgrd_atlas/tlm_height_band2,/gt2l/bckgrd_atlas/tlm_top_band1,/gt2l/bckgrd_atlas/tlm_top_band2,/gt2l/geolocation/ph_index_beg,/gt2l/geolocation/segment_dist_x,/gt2l/geolocation/segment_id,/gt2l/geolocation/segment_length,/gt2l/geolocation/segment_ph_cnt,/gt2l/geophys_corr/geoid,/gt2l/heights/delta_time,/gt2l/heights/dist_ph_along,/gt2l/heights/h_ph,/gt2l/heights/lat_ph,/gt2l/heights/lon_ph,/gt2l/heights/pce_mframe_cnt,/gt2l/heights/ph_id_pulse,/gt2l/heights/quality_ph,/gt2l/heights/weight_ph,/gt2r/bckgrd_atlas/pce_mframe_cnt,/gt2r/bckgrd_atlas/tlm_height_band1,/gt2r/bckgrd_atlas/tlm_height_band2,/gt2r/bckgrd_atlas/tlm_top_band1,/gt2r/bckgrd_atlas/tlm_top_band2,/gt2r/geolocation/ph_index_beg,/gt2r/geolocation/segment_dist_x,/gt2r/geolocation/segment_id,/gt2r/geolocation/segment_length,/gt2r/geolocation/segment_ph_cnt,/gt2r/geophys_corr/geoid,/gt2r/heights/delta_time,/gt2r/heights/dist_ph_along,/gt2r/heights/h_ph,/gt2r/heights/lat_ph,/gt2r/heights/lon_ph,/gt2r/heights/pce_mframe_cnt,/gt2r/heights/ph_id_pulse,/gt2r/heights/quality_ph,/gt2r/heights/weight_ph,/gt3l/bckgrd_atlas/pce_mframe_cnt,/gt3l/bckgrd_atlas/tlm_height_band1,/gt3l/bckgrd_atlas/tlm_height_band2,/gt3l/bckgrd_atlas/tlm_top_band1,/gt3l/bckgrd_atlas/tlm_top_band2,/gt3l/geolocation/ph_index_beg,/gt3l/geolocation/segment_dist_x,/gt3l/geolocation/segment_id,/gt3l/geolocation/segment_length,/gt3l/geolocation/segment_ph_cnt,/gt3l/geophys_corr/geoid,/gt3l/heights/delta_time,/gt3l/heights/dist_ph_along,/gt3l/heights/h_ph,/gt3l/heights/lat_ph,/gt3l/heights/lon_ph,/gt3l/heights/pce_mframe_cnt,/gt3l/heights/ph_id_pulse,/gt3l/heights/quality_ph,/gt3l/heights/weight_ph,/gt3r/bckgrd_atlas/pce_mframe_cnt,/gt3r/bckgrd_atlas/tlm_height_band1,/gt3r/bckgrd_atlas/tlm_height_band2,/gt3r/bckgrd_atlas/tlm_top_band1,/gt3r/bckgrd_atlas/tlm_top_band2,/gt3r/geolocation/ph_index_beg,/gt3r/geolocation/segment_dist_x,/gt3r/geolocation/segment_id,/gt3r/geolocation/segment_length,/gt3r/geolocation/segment_ph_cnt,/gt3r/geophys_corr/geoid,/gt3r/heights/delta_time,/gt3r/heights/dist_ph_along,/gt3r/heights/h_ph,/gt3r/heights/lat_ph,/gt3r/heights/lon_ph,/gt3r/heights/pce_mframe_cnt,/gt3r/heights/ph_id_pulse,/gt3r/heights/quality_ph,/gt3r/heights/weight_ph,/orbit_info/cycle_number,/orbit_info/rgt,/orbit_info/sc_orient&page_size=100&request_mode=stream&email=yes&page_num=1\n",
      "\n",
      "Order:  1\n",
      "Requesting...\n",
      "HTTP response from order response URL:  200\n",
      "Downloading...\n",
      "Data request 1 is complete.\n",
      "\n",
      "Unzipped files and cleaned up directory.\n",
      "Output data saved in: zzzzz_test_IS2data\n",
      "File to process: zzzzz_test_IS2data/processed_ATL03_20230806063138_07192003_006_02.h5 (3.61 MB)\n",
      "status: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('zzzzz_test_IS2data/processed_ATL03_20230806063138_07192003_006_02.h5', 200)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_granule(granule_id, gtxs, geojson, granule_output_path, uid, pwd, vars_sub='default', spatial_sub=True, request_mode='stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a414da4f-5f93-4206-8bc8-35ecee0b398f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'processed_ATL03_20230806063138_07192003_006_02.h5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79fbc212-3b94-4397-bb8b-d962d65e8c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a75333e-bfb7-4669-85c3-f42f2a3283af",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Session' object has no attribute 'status_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Session' object has no attribute 'status_code'"
     ]
    }
   ],
   "source": [
    "session.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c7c5687-348e-4ba8-8c1b-6738619e3ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea859e-232e-4f26-a6cf-6a4b6f23c883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eabe51-b025-40fe-bacf-8a9ee7f54afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db074cdc-9a69-47d2-9382-4a7b4006a94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8ba36-4d67-4f0f-b5b5-76335d1e62a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83132c-76fc-4103-91a7-c1b0a1ac44f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18ad76-e890-4350-8bb1-2fa2d3401a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9aac50-d927-4900-9fa2-cf47a1ba3fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e01a5-1b55-4a06-918a-76d59c15fc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9f5fc-f2f8-466a-81e9-e46762d18952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba219adc-0ed6-4a2b-9b83-78a875cec0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_granule(granule_id, gtxs, geojson, granule_output_path, uid, pwd, vars_sub='default', spatial_sub=False): \n",
    "    \"\"\"\n",
    "    Download a single ICESat-2 ATL03 granule based on its producer ID,\n",
    "    subsets it to a given geojson file, and puts it into the specified\n",
    "    output directory as a .h5 file. A NASA earthdata user id (uid), and\n",
    "    the associated password are required. \n",
    "    (Can also provide a shapefile instead of geojson.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    granule_id : string\n",
    "        the producer_granule_id for CMR search\n",
    "    gtxs : string or list\n",
    "        the ground tracks to request\n",
    "        possible values:\n",
    "            'gt1l' or 'gt1r' or 'gt2l', ... (single gtx)\n",
    "            ['gt1l', 'gt3r', ...] (list of gtxs)\n",
    "    geojson : string\n",
    "        filepath to the geojson file used for spatial subsetting\n",
    "    granule_output_path : string\n",
    "        folder in which to save the subsetted granule\n",
    "    uid : string\n",
    "        earthdata user id\n",
    "    pwd : string\n",
    "        the associated password\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nothing\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> download_granule_nsidc(granule_id='ATL03_20210715182907_03381203_005_01.h5', \n",
    "                               geojson='geojsons/jakobshavn.geojson', \n",
    "                               gtxs='all'\n",
    "                               granule_output_path='IS2data', \n",
    "                               uid='myuserid', \n",
    "                               pwd='mypasword')\n",
    "    \"\"\"\n",
    "    print('--> parameters: granule_id = %s' % granule_id)\n",
    "    print('                gtxs = %s' % gtxs)\n",
    "    print('                geojson = %s' % geojson)\n",
    "    print('                granule_output_path = %s' % granule_output_path)\n",
    "    print('                vars_sub = %s' % vars_sub)\n",
    "    print('                spatial_sub = %s\\n' % spatial_sub)\n",
    "    \n",
    "    short_name = 'ATL03'\n",
    "    version = granule_id[30:33]\n",
    "    granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "    capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{version}.xml'\n",
    "    base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "    \n",
    "    geojson_filepath = str(os.getcwd() + '/' + geojson)\n",
    "    \n",
    "    # set the variables for subsetting\n",
    "    if vars_sub == 'default':\n",
    "        vars_sub = ['/ancillary_data/atlas_sdp_gps_epoch',\n",
    "                    '/ancillary_data/calibrations/dead_time/gtx',\n",
    "                    '/orbit_info/rgt',\n",
    "                    '/orbit_info/cycle_number',\n",
    "                    '/orbit_info/sc_orient',\n",
    "                    '/gtx/geolocation/segment_id',\n",
    "                    '/gtx/geolocation/ph_index_beg',\n",
    "                    '/gtx/geolocation/segment_dist_x',\n",
    "                    '/gtx/geolocation/segment_length',\n",
    "                    '/gtx/geolocation/segment_ph_cnt',\n",
    "                    # '/gtx/geophys_corr/dem_h',\n",
    "                    '/gtx/geophys_corr/geoid',\n",
    "                    '/gtx/bckgrd_atlas/pce_mframe_cnt',\n",
    "                    '/gtx/bckgrd_atlas/tlm_height_band1',\n",
    "                    '/gtx/bckgrd_atlas/tlm_height_band2',\n",
    "                    '/gtx/bckgrd_atlas/tlm_top_band1',\n",
    "                    '/gtx/bckgrd_atlas/tlm_top_band2',\n",
    "                    # '/gtx/bckgrd_atlas/bckgrd_counts',\n",
    "                    # '/gtx/bckgrd_atlas/bckgrd_int_height',\n",
    "                    # '/gtx/bckgrd_atlas/delta_time',\n",
    "                    '/gtx/heights/lat_ph',\n",
    "                    '/gtx/heights/lon_ph',\n",
    "                    '/gtx/heights/h_ph',\n",
    "                    '/gtx/heights/delta_time',\n",
    "                    '/gtx/heights/dist_ph_along',\n",
    "                    '/gtx/heights/quality_ph',\n",
    "                    # '/gtx/heights/signal_conf_ph',\n",
    "                    '/gtx/heights/pce_mframe_cnt',\n",
    "                    '/gtx/heights/ph_id_pulse'\n",
    "                    ]\n",
    "        if int(version) > 5:\n",
    "            vars_sub.append('/gtx/heights/weight_ph')\n",
    "    beam_list = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "    \n",
    "    if gtxs == 'all':\n",
    "        var_list = sum([[v.replace('/gtx','/'+bm) for bm in beam_list] if '/gtx' in v else [v] for v in vars_sub],[])\n",
    "    elif type(gtxs) == str:\n",
    "        var_list = [v.replace('/gtx','/'+gtxs.lower()) if '/gtx' in v else v for v in vars_sub]\n",
    "    elif type(gtxs) == list:\n",
    "        var_list = sum([[v.replace('/gtx','/'+bm.lower()) for bm in gtxs] if '/gtx' in v else [v] for v in vars_sub],[])\n",
    "    else: # default to requesting all beams\n",
    "        var_list = sum([[v.replace('/gtx','/'+bm) for bm in beam_list] if '/gtx' in v else [v] for v in vars_sub],[])\n",
    "    \n",
    "    # search for the given granule\n",
    "    search_params = {\n",
    "        'short_name': short_name,\n",
    "        'page_size': 100,\n",
    "        'page_num': 1,\n",
    "        'producer_granule_id': granule_id}\n",
    "\n",
    "    granules = []\n",
    "    headers={'Accept': 'application/json'}\n",
    "    while True:\n",
    "        response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "        results = json.loads(response.content)\n",
    "\n",
    "        if len(results['feed']['entry']) == 0:\n",
    "            # Out of results, so break out of loop\n",
    "            break\n",
    "\n",
    "        # Collect results and increment page_num\n",
    "        granules.extend(results['feed']['entry'])\n",
    "        search_params['page_num'] += 1\n",
    "        \n",
    "    granule_list, idx_unique = np.unique(np.array([g['producer_granule_id'] for g in granules]), return_index=True)\n",
    "    granules = [g for i,g in enumerate(granules) if i in idx_unique] # keeps double counting, not sure why\n",
    "    print('\\nDownloading ICESat-2 data. Found granules:')\n",
    "    if len(granules) == 0:\n",
    "        print('None')\n",
    "        return 'none', 404\n",
    "    for result in granules:\n",
    "        print('  '+result['producer_granule_id'], f', {float(result[\"granule_size\"]):.2f} MB',sep='')\n",
    "        \n",
    "    # Use geopandas to read in polygon file as GeoDataFrame object \n",
    "    # Note: a shapefile, KML, or almost any other vector-based spatial data format could be substituted here.\n",
    "    gdf = gpd.read_file(geojson_filepath)\n",
    "\n",
    "    # make sure the two regions that go over the date line are adjusted \n",
    "    # if ('West_Ep-F.geojson' in geojson_filepath) or ('East_E-Ep.geojson' in geojson_filepath): \n",
    "    #     lon180 = np.array(gdf.geometry.iloc[0].exterior.coords.xy[0])\n",
    "    #     lon180[lon180 < 0] = lon180[lon180 < 0]  + 360\n",
    "    #     gdf['geometry'] = Polygon(list(zip(lon180, gdf.geometry.iloc[0].exterior.coords.xy[1])))\n",
    "    #     poly = orient(gdf.loc[0].geometry,sign=1.0)\n",
    "    #     lon180 = np.array(poly.exterior.coords.xy[0])\n",
    "    #     # lon180[lon180 >= 180] = lon180[lon180 >= 180] - 360\n",
    "    #     gdf['geometry'] = Polygon(list(zip(lon180, gdf.geometry.iloc[0].exterior.coords.xy[1])))\n",
    "    #     poly = gdf.loc[0].geometry\n",
    "    \n",
    "    # Simplify polygon for complex shapes in order to pass a reasonable request length to CMR. \n",
    "    # The larger the tolerance value, the more simplified the polygon.\n",
    "    # Orient counter-clockwise: CMR polygon points need to be provided in counter-clockwise order. \n",
    "    # The last point should match the first point to close the polygon.\n",
    "    # poly = orient(gdf.simplify(0.05, preserve_topology=False).loc[0],sign=1.0)\n",
    "    # else:\n",
    "    poly = orient(gdf.loc[0].geometry,sign=1.0)\n",
    "\n",
    "    geojson_data = gpd.GeoSeries(poly).to_json() # Convert to geojson\n",
    "    geojson_data = geojson_data.replace(' ', '') #remove spaces for API call\n",
    "    \n",
    "    #Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "    polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "    \n",
    "    print('\\nInput geojson:', geojson)\n",
    "    print('Simplified polygon coordinates based on geojson input:', polygon)\n",
    "    \n",
    "    # Create session to store cookie and pass credentials to capabilities url\n",
    "    session = requests.session()\n",
    "    s = session.get(capability_url)\n",
    "    response = session.get(s.url,auth=(uid,pwd))\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(response.content)\n",
    "    except:\n",
    "        try:\n",
    "            cont = str(request._content)\n",
    "            print('request status code:', request.status_code)\n",
    "            the_code = cont[cont.find('<Code>')+6:cont.find('</Code>')]\n",
    "            if len(the_code) < 1000:\n",
    "                print(the_code)\n",
    "            the_message = cont[cont.find('<Message>')+9:cont.find('</Message>')]\n",
    "            if len(the_message) < 5000:\n",
    "                print(the_message)\n",
    "            print('')\n",
    "            return 'none', response.status_code\n",
    "        except:\n",
    "            return 'none', response.status_code\n",
    "\n",
    "    #collect lists with each service option\n",
    "    subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "    \n",
    "    # this is for getting possible variable values from the granule search\n",
    "    if len(subagent) > 0 :\n",
    "        # variable subsetting\n",
    "        variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "        variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "        variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "        variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "    \n",
    "    # make sure to only request the variables that are available\n",
    "    def intersection(lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3\n",
    "    if vars_sub == 'all':\n",
    "        var_list_subsetting = ''\n",
    "    else:\n",
    "        var_list_subsetting = intersection(variable_vals,var_list)\n",
    "    \n",
    "    if len(subagent) < 1 :\n",
    "        print('No services exist for', short_name, 'version', latest_version)\n",
    "        agent = 'NO'\n",
    "        coverage,Boundingshape,polygon = '','',''\n",
    "    else:\n",
    "        agent = ''\n",
    "        subdict = subagent[0]\n",
    "        if (subdict['spatialSubsettingShapefile'] == 'true') and spatial_sub:\n",
    "            ######################################## Boundingshape = geojson_data\n",
    "            Boundingshape = polygon\n",
    "        else:\n",
    "            Boundingshape, polygon = '',''\n",
    "        coverage = ','.join(var_list_subsetting)\n",
    "    if (vars_sub=='all') & (not spatial_sub):\n",
    "        agent = 'NO'\n",
    "        \n",
    "    page_size = 100\n",
    "    request_mode = 'stream'\n",
    "    page_num = int(np.ceil(len(granules)/page_size))\n",
    "\n",
    "    param_dict = {'short_name': short_name, \n",
    "                  'producer_granule_id': granule_id,\n",
    "                  'version': version,  \n",
    "                  'polygon': polygon,\n",
    "                  'Boundingshape': Boundingshape,  \n",
    "                  'Coverage': coverage, \n",
    "                  'page_size': page_size, \n",
    "                  'request_mode': request_mode, \n",
    "                  'agent': agent, \n",
    "                  'email': 'yes'}\n",
    "\n",
    "    #Remove blank key-value-pairs\n",
    "    param_dict = {k: v for k, v in param_dict.items() if v != ''}\n",
    "\n",
    "    #Convert to string\n",
    "    param_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in param_dict.items())\n",
    "    param_string = param_string.replace(\"'\",\"\")\n",
    "\n",
    "    #Print API base URL + request parameters\n",
    "    endpoint_list = [] \n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        API_request = api_request = f'{base_url}?{param_string}&page_num={page_val}'\n",
    "        endpoint_list.append(API_request)\n",
    "\n",
    "    print('\\nAPI request URL:')\n",
    "    print(*endpoint_list, sep = \"\\n\") \n",
    "    \n",
    "    # Create an output folder if the folder does not already exist.\n",
    "    path = str(os.getcwd() + '/' + granule_output_path)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    # Different access methods depending on request mode:\n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        print('\\nOrder: ', page_val)\n",
    "        print('Requesting...')\n",
    "        request = session.get(base_url, params=param_dict)\n",
    "        print('HTTP response from order response URL: ', request.status_code)\n",
    "        # try: \n",
    "        #     cont = str(request._content)\n",
    "        #     print(cont[cont.find('<Code>')+6:cont.find('</Code>')],\n",
    "        #       '(', cont[cont.find('<Message>')+9:cont.find('</Message>')], ')\\n')\n",
    "        # except:\n",
    "        #     pass\n",
    "        request.raise_for_status()\n",
    "        d = request.headers['content-disposition']\n",
    "        fname = re.findall('filename=(.+)', d)\n",
    "        dirname = os.path.join(path,fname[0].strip('\\\"'))\n",
    "        print('Downloading...')\n",
    "        open(dirname, 'wb').write(request.content)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "\n",
    "    # Unzip outputs\n",
    "    for z in os.listdir(path): \n",
    "        if z.endswith('.zip'): \n",
    "            zip_name = path + \"/\" + z \n",
    "            zip_ref = zipfile.ZipFile(zip_name) \n",
    "            zip_ref.extractall(path) \n",
    "            zip_ref.close() \n",
    "            os.remove(zip_name) \n",
    "\n",
    "    # Clean up Outputs folder by removing individual granule folders \n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for file in files:\n",
    "            try:\n",
    "                shutil.move(os.path.join(root, file), path)\n",
    "            except OSError:\n",
    "                pass\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "            \n",
    "    print('\\nUnzipped files and cleaned up directory.')\n",
    "    print('Output data saved in:', granule_output_path)\n",
    "    \n",
    "    filelist = [granule_output_path+'/'+f for f in os.listdir(granule_output_path) \\\n",
    "                if os.path.isfile(os.path.join(granule_output_path, f)) & (granule_id in f)]\n",
    "    \n",
    "    if len(filelist) == 0: \n",
    "        return 'none'\n",
    "    else:\n",
    "        filename = filelist[0]\n",
    "    print('File to process: %s (%s)' % (filename, get_size(filename)))\n",
    "    \n",
    "    print(filename, 'status:', request.status_code)\n",
    "    return filename, request.status_code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icelakes-env",
   "language": "python",
   "name": "icelakes-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

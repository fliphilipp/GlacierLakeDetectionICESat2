{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20e0249-f57e-4bbe-8b32-5bea3c458e46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954886c1-ff36-4207-a925-a498d7553114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import icelakes\n",
    "from subprocess import call\n",
    "from icelakes.utilities import encedc, decedc\n",
    "from icelakes.nsidc import download_granule, edc\n",
    "from icelakes.detection import read_atl03, detect_lakes, melt_lake\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Test script to print some stats for a given ICESat-2 ATL03 granule.')\n",
    "parser.add_argument('--granule', type=str, default='ATL03_20210715182907_03381203_005_01.h5',\n",
    "                    help='The producer_id of the input ATL03 granule')\n",
    "parser.add_argument('--polygon', type=str, default='geojsons/jakobshavn_small.geojson',\n",
    "                    help='The file path of a geojson file for spatial subsetting')\n",
    "parser.add_argument('--is2_data_dir', type=str, default='IS2data',\n",
    "                    help='The directory into which to download ICESat-2 granules')\n",
    "parser.add_argument('--download_gtxs', type=str, default='all',\n",
    "                    help='String value or list of gtx names to download, also accepts \"all\"')\n",
    "parser.add_argument('--out_data_dir', type=str, default='detection_out_data',\n",
    "                    help='The directory to which to write the output data')\n",
    "parser.add_argument('--out_plot_dir', type=str, default='detection_out_plot',\n",
    "                    help='The directory to which to write the output plots')\n",
    "parser.add_argument('--out_stat_dir', type=str, default='detection_out_stat',\n",
    "                    help='The directory to which to write the granule stats')\n",
    "\n",
    "# set arguments as class for now, to run in jupyter\n",
    "if parser.prog == 'ipykernel_launcher.py':\n",
    "    class Args:\n",
    "        granule = 'ATL03_20210715182907_03381203_005_01.h5'\n",
    "        polygon = 'geojsons/jakobshavn_small.geojson'\n",
    "        is2_data_dir =  'IS2data'\n",
    "        download_gtxs = 'all'\n",
    "        out_data_dir = 'detection_out_data'\n",
    "        out_plot_dir = 'detection_out_plot'\n",
    "        out_stat_dir = 'detection_out_stat'\n",
    "    args=Args()\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "# try to figure out where the script is being executed (just to show those maps at conferences, etc...)\n",
    "try:\n",
    "    with open('location-wrapper.sh', 'rb') as file:\n",
    "        script = file.read()\n",
    "    rc = call(script, shell=True)\n",
    "except:\n",
    "    print('\\nUnable to determine compute location for this script.\\n')\n",
    "\n",
    "# shuffling files around for HTCondor\n",
    "for thispath in (args.is2_data_dir, args.out_data_dir, args.out_plot_dir):\n",
    "    if not os.path.exists(thispath): os.makedirs(thispath)\n",
    "\n",
    "# download the specified ICESat-2 data from NSIDC\n",
    "input_filename = args.is2_data_dir + '/processed_' + args.granule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddfef4-9914-457e-8b66-27d5923ec4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = download_granule(args.granule, args.download_gtxs, args.polygon, args.is2_data_dir, decedc(edc().u), decedc(edc().p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ca998b-0124-4551-892d-314050d26b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading in IS2data/processed_ATL03_20210715182907_03381203_005_01.h5\n",
      "  reading in beam:  --> done.\n",
      "  reading in IS2data/processed_ATL03_20210715182907_03381203_005_01.h5\n",
      "  reading in beam: gt1l  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt1l (weak)\n",
      "---> finding flat surfaces in photon data (15 / 666 were flat)\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(5 / 666 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:    5\n",
      "   --> iteration   1, number of lakes:    3\n",
      "   --> iteration   2, number of lakes:    2\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:<> 1:<> \n",
      "---> calculating remaining photon densities\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(5 / 666 pass lake quality test)\n",
      "RESULTS FOR : GT1L\n",
      "  lake    0 ( 68.46018°N, 49.45770°W) length:  0.5 km, surface elevation:  969.34 m, quality: 0.10175)\n",
      "  lake    1 ( 68.74898°N, 49.55362°W) length:  0.6 km, surface elevation:  932.93 m, quality: 0.03836)\n",
      "  reading in IS2data/processed_ATL03_20210715182907_03381203_005_01.h5\n",
      "  reading in beam: gt1r  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt1r (strong)\n",
      "---> finding flat surfaces in photon data (24 / 687 were flat)\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(8 / 687 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:    8\n",
      "   --> iteration   1, number of lakes:    4\n",
      "   --> iteration   2, number of lakes:    3\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:<> 1:<> 2:<> \n",
      "---> calculating remaining photon densities\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(8 / 687 pass lake quality test)\n",
      "RESULTS FOR : GT1R\n",
      "  lake    0 ( 68.45995°N, 49.45539°W) length:  0.4 km, surface elevation:  969.28 m, quality: 0.06776)\n",
      "  lake    1 ( 68.72285°N, 49.54260°W) length:  0.5 km, surface elevation:  945.05 m, quality: 0.04639)\n",
      "  lake    2 ( 68.74939°N, 49.55148°W) length:  0.7 km, surface elevation:  932.88 m, quality: 0.48958)\n",
      "  reading in IS2data/processed_ATL03_20210715182907_03381203_005_01.h5\n",
      "  reading in beam: gt2l  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt2l (weak)\n",
      "---> finding flat surfaces in photon data (41 / 712 were flat)\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(3 / 712 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:    3\n",
      "   --> iteration   1, number of lakes:    2\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:<<>> 1:<<<<<<<> \n",
      "---> calculating remaining photon densities\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(3 / 712 pass lake quality test)\n",
      "RESULTS FOR : GT2L\n",
      "  lake    0 ( 68.72867°N, 49.46627°W) length:  1.0 km, surface elevation:  945.11 m, quality: 0.22744)\n",
      "  reading in IS2data/processed_ATL03_20210715182907_03381203_005_01.h5\n",
      "  reading in beam: gt2r  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt2r (strong)\n",
      "---> finding flat surfaces in photon data (67 / 734 were flat)\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(8 / 734 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:    8\n",
      "   --> iteration   1, number of lakes:    4\n",
      "   --> iteration   2, number of lakes:    3\n",
      "   --> iteration   3, number of lakes:    2\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:<> 1:<> \n",
      "---> calculating remaining photon densities\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(8 / 734 pass lake quality test)\n",
      "RESULTS FOR : GT2R\n",
      "  lake    0 ( 68.35872°N, 49.34301°W) length:  0.5 km, surface elevation: 1029.13 m, quality: 0.00348)\n",
      "  lake    1 ( 68.72908°N, 49.46410°W) length:  1.0 km, surface elevation:  945.12 m, quality: 0.44104)\n",
      "  reading in IS2data/processed_ATL03_20210715182907_03381203_005_01.h5\n",
      "  reading in beam: gt3l  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt3l (weak)\n",
      "---> finding flat surfaces in photon data (13 / 722 were flat)\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(1 / 722 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:    1\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:> \n",
      "---> calculating remaining photon densities\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(1 / 722 pass lake quality test)\n",
      "RESULTS FOR : GT3L\n",
      "  lake    0 ( 68.59593°N, 49.34229°W) length:  0.3 km, surface elevation: 1023.15 m, quality: 0.00000)\n",
      "  reading in IS2data/processed_ATL03_20210715182907_03381203_005_01.h5\n",
      "  reading in beam: gt3r  --> done.\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "PROCESSING GROUND TRACK: gt3r (strong)\n",
      "---> finding flat surfaces in photon data (18 / 735 were flat)\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(1 / 735 pass lake quality test)\n",
      "---> merging major frame segments that possibly represent lakes iteratively\n",
      "   --> iteration   0, number of lakes:    1\n",
      "---> checking lake edges and extending them if the surface elevation matches\n",
      "extending lake  0:<< \n",
      "---> calculating remaining photon densities\n",
      "---> calculating photon densities & looking for second density peaks below the surface\n",
      "(1 / 735 pass lake quality test)\n",
      "RESULTS FOR : GT3R\n",
      "  lake    0 ( 68.63366°N, 49.35245°W) length:  0.6 km, surface elevation: 1025.20 m, quality: 0.33392)\n",
      "\n",
      "GRANULE STATS (length total, length lakes, photons total, photons lakes):612851.901,5748.000,19176006,160759\n"
     ]
    }
   ],
   "source": [
    "gtx_list, ancillary = read_atl03(input_filename, gtxs_to_read='none')\n",
    "\n",
    "# detect melt lakes\n",
    "lake_list = []\n",
    "granule_stats = {'length_total': 0.0, 'length_lakes': 0.0, 'n_photons_total': 0, 'n_photons_lakes': 0}\n",
    "for gtx in gtx_list:\n",
    "    lakes_found, gtx_stats = detect_lakes(input_filename, gtx, args.polygon, verbose=False)\n",
    "    for k in granule_stats.keys(): granule_stats[k] += gtx_stats[k]\n",
    "    lake_list += lakes_found\n",
    "\n",
    "# print stats for granule\n",
    "print('\\nGRANULE STATS (length total, length lakes, photons total, photons lakes):%.3f,%.3f,%i,%i' % tuple(granule_stats.values()))\n",
    "\n",
    "# save plots and lake data dictionaries\n",
    "for lake in lake_list:\n",
    "    filename_base = 'lake_%05i_%s_%s_%s_%s_%s' % ((1.0-lake.detection_quality)*10000, lake.ice_sheet, lake.melt_season, \n",
    "                                                  lake.polygon_name, lake.granule_id[:-4], lake.gtx)\n",
    "    # plot each lake and save to image\n",
    "    fig = lake.plot_detected(min_width=0.0, min_depth=0.0);\n",
    "    figname = args.out_plot_dir + '/%s.jpg' % filename_base\n",
    "    if fig is not None: fig.savefig(figname, dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # export each lake to pickle (TODO: add .h5 option soon)\n",
    "    pklname = args.out_data_dir + '/%s.pkl' % filename_base\n",
    "    with open(pklname, 'wb') as f: pickle.dump(vars(lake), f)\n",
    "\n",
    "lk = lake_list[0]\n",
    "statsfname = args.out_stat_dir + '/stats_%s_%s_%s_%s.csv' % (lk.ice_sheet, lk.melt_season, lk.polygon_name, lk.granule_id[:-4])\n",
    "with open('filename.txt', 'w') as f: print('%.3f,%.3f,%i,%i' % tuple(granule_stats.values()), file=f)\n",
    "\n",
    "print('\\n----------->   SUCCESS!   <-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdcf6c3-345c-4256-9dfa-8a2c0dd49a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d020a79-33d3-48bd-8eaf-5ea88ad685b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4f8fa-7f1c-4e93-830e-d67d74fab561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59a9b8-255e-44ec-85dd-972abbe6bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import icelakes\n",
    "from icelakes.utilities import encedc, decedc\n",
    "from icelakes.nsidc import download_granule, edc\n",
    "from icelakes.detection import read_atl03, detect_lakes, melt_lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78393f8a-aec1-4f8f-b2cd-28f139db889d",
   "metadata": {},
   "source": [
    "## parsing arguments from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99a20d-58ca-4171-abea-67a248e7f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Test script to print some stats for a given ICESat-2 ATL03 granule.')\n",
    "parser.add_argument('--granule', type=str, default='ATL03_20210715182907_03381203_005_01.h5',\n",
    "                    help='The producer_id of the input ATL03 granule')\n",
    "parser.add_argument('--polygon', type=str, default='geojsons/jakobshavn_small.geojson',\n",
    "                    help='The file path of a geojson file for spatial subsetting')\n",
    "parser.add_argument('--IS2datadir', type=str, default='IS2data',\n",
    "                    help='The directory into which to download ICESat-2 granules')\n",
    "parser.add_argument('--download_gtxs', type=str, default='all',\n",
    "                    help='String value or list of gtx names to download, also accepts \"all\"')\n",
    "\n",
    "# set arguments as class for now, to run in jupyter\n",
    "if parser.prog == 'ipykernel_launcher.py':\n",
    "    class Args:\n",
    "        granule = 'ATL03_20210715182907_03381203_005_01.h5'\n",
    "        polygon = 'geojsons/jakobshavn_test.geojson'\n",
    "        IS2datadir =  'IS2data'\n",
    "        download_gtxs = 'all'\n",
    "    args=Args()\n",
    "else:\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42800087-be5e-4f2f-9b63-a881fc00bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.granule = 'ATL03_20200116042313_03120610_005_01.h5'\n",
    "# args.granule = 'ATL03_20200116171056_03200612_005_01.h5'\n",
    "# args.polygon = 'geojsons/nivlisen_test.geojson'\n",
    "\n",
    "# args.granule = 'ATL03_20200204085006_06050610_005_01.h5'\n",
    "# args.granule = 'ATL03_20191229231828_00490612_005_01.h5'\t\n",
    "# args.granule = 'ATL03_20191229103045_00410610_005_01.h5'\n",
    "# args.granule = 'ATL03_20200131214610_05520612_005_01.h5'\n",
    "# args.granule = 'ATL03_20200131085827_05440610_005_01.h5'\n",
    "# args.granule = 'ATL03_20200127090648_04830610_005_01.h5'\n",
    "# args.granule = 'ATL03_20200127215431_04910612_005_01.h5'\n",
    "# args.granule = 'ATL03_20200102102224_01020610_005_01.h5'\n",
    "# args.granule = 'ATL03_20200102231007_01100612_005_01.h5'\n",
    "# args.granule = 'ATL03_20220224205325_09861410_005_01.h5'\n",
    "# args.granule = 'ATL03_20200204213749_06130612_005_01.h5'\n",
    "# args.polygon = 'geojsons/georgeVI_test.geojson'\n",
    "\n",
    "args.granule = 'ATL03_20190102184312_00810210_005_01.h5'\n",
    "# # args.granule = 'ATL03_20190103073055_00890212_005_01.h5'\n",
    "args.polygon = 'geojsons/amery_test.geojson'\n",
    "# args.polygon = 'geojsons/amery_1lake_test.geojson'\n",
    "\n",
    "# args.granule = 'ATL03_20210715182907_03381203_005_01.h5'\n",
    "# args.granule = 'ATL03_20210809171331_07191203_005_01.h5'\n",
    "# args.granule = 'ATL03_20210802061504_06051205_005_01.h5'\n",
    "# args.polygon = 'geojsons/jakobshavn_test.geojson'\n",
    "\n",
    "args.granule = 'ATL03_20190822144852_08480405_005_01.h5'\n",
    "# args.granule = 'ATL03_20210831140621_10531203_005_01.h5'\n",
    "args.polygon = 'geojsons/79N_test.geojson'\n",
    "\n",
    "# args.granule = 'ATL03_20200224204246_09180610_005_01.h5'\n",
    "# args.polygon = 'geojsons/shackleton_test.geojson'\n",
    "\n",
    "############################################################\n",
    "# args.granule = 'ATL03_20210822035912_09091205_005_01.h5'\n",
    "# args.polygon = 'geojsons/GrIS-NE-1lake-test.geojson'\n",
    "# args.polygon = 'geojsons/GrIS-NE-test.geojson'\n",
    "\n",
    "# args.granule = 'ATL03_20200116042313_03120610_005_01.h5'\n",
    "# args.granule = 'ATL03_20200116171056_03200612_005_01.h5'\n",
    "# args.polygon = 'geojsons/nivlisen_test.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e9ea4-af10-4b37-ad34-c0a8340a4d71",
   "metadata": {},
   "source": [
    "## download data from NSIDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2dffc8-8480-48a6-bd15-0a5e458e1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# input_filename = download_granule(args.granule, args.download_gtxs, args.polygon, args.IS2datadir, decedc(edc().u), decedc(edc().p))\n",
    "# just to not have to re-download nsidc data while testing\n",
    "input_filename = args.IS2datadir + '/' + 'processed_' + args.granule\n",
    "# print(input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7ddf4-70a5-460b-85dd-4f776ec2053c",
   "metadata": {},
   "source": [
    "## read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c97d73-f6fa-4f2c-a8b3-2aaad23a4dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "photon_data, bckgrd_data, ancillary = read_atl03(input_filename, geoid_h=True)\n",
    "gtx_list = list(photon_data.keys())\n",
    "lake_list = []\n",
    "for gtx in gtx_list:\n",
    "    lake_list += detect_lakes(photon_data, gtx, ancillary, args.polygon, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40974c89-5767-43f7-9aaf-913e3f9511f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars(lake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414cb0e-1938-4f1d-b118-1eff327204f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('ATL03_20190822144852_08480405_005_01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdad5be-a712-40b7-b464-c1d75ebc5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "'mframe_start': 219511169,\n",
    " 'mframe_end': 219511176,\n",
    " 'main_peak': 298.1453618258017,\n",
    " 'n_subsegs_per_mframe': 10,\n",
    " 'rgt': 848,\n",
    " 'gtx': 'gt2r',\n",
    " 'polygon_filename': 'geojsons/79N_test.geojson',\n",
    " 'polygon_name': '79N_test',\n",
    " 'beam_number': 4,\n",
    " 'beam_strength': 'weak',\n",
    " 'cycle_number': 4,\n",
    " 'sc_orient': 'backward',\n",
    " 'dead_time': 3.0525e-09,\n",
    " 'dead_time_meters': 0.4575582390225,\n",
    " 'photon_data':              lat        lon           h            dt     mframe  ph_id_pulse  \\\n",
    " 0      79.099935 -21.884912   -6.646666  5.172055e+07  219511169            1   \n",
    " 1      79.099932 -21.884912  123.536894  5.172055e+07  219511169            1   \n",
    " 2      79.099931 -21.884911  152.235123  5.172055e+07  219511169            1   \n",
    " 3      79.099931 -21.884911  169.133791  5.172055e+07  219511169            1   \n",
    " 4      79.099930 -21.884911  207.248434  5.172055e+07  219511169            1   \n",
    " ...          ...        ...         ...           ...        ...          ...   \n",
    " 29342  79.089955 -21.895447  304.832101  5.172055e+07  219511176          200   \n",
    " 29343  79.089955 -21.895447  304.845772  5.172055e+07  219511176          200   \n",
    " 29344  79.089951 -21.895446  471.812860  5.172055e+07  219511176          200   \n",
    " 29345  79.089951 -21.895446  500.124325  5.172055e+07  219511176          200   \n",
    " 29346  79.089947 -21.895445  666.257886  5.172055e+07  219511176          200   \n",
    " \n",
    "        qual         xatc      geoid  specular       snr  \n",
    " 0         0     0.000000  28.641219     False  0.055568  \n",
    " 1         0     0.329233  28.641207     False  0.082893  \n",
    " 2         0     0.401796  28.641204     False  0.005511  \n",
    " 3         0     0.444506  28.641202     False  0.000000  \n",
    " 4         0     0.540888  28.641199     False  0.013230  \n",
    " ...     ...          ...        ...       ...       ...  \n",
    " 29342     0  1136.297044  28.599387     False  0.987444  \n",
    " 29343     0  1136.297080  28.599387     False  0.987876  \n",
    " 29344     0  1136.719164  28.599372     False  0.000000  \n",
    " 29345     0  1136.790709  28.599369     False  0.013610  \n",
    " 29346     0  1137.210580  28.599353     False  0.000000  \n",
    " \n",
    " [29347 rows x 11 columns],\n",
    " 'mframe_data':                  lat        lon         xatc            dt  \\\n",
    " mframe                                                       \n",
    " 219511169  79.099306 -21.885564    71.819298  5.172055e+07   \n",
    " 219511170  79.098061 -21.886878   214.011309  5.172055e+07   \n",
    " 219511171  79.096804 -21.888218   355.859183  5.172055e+07   \n",
    " 219511172  79.095561 -21.889525   497.735448  5.172055e+07   \n",
    " 219511173  79.094311 -21.890828   639.768573  5.172055e+07   \n",
    " 219511174  79.093068 -21.892149   781.730268  5.172055e+07   \n",
    " 219511175  79.091827 -21.893465   923.874683  5.172055e+07   \n",
    " 219511176  79.090562 -21.894806  1065.879663  5.172055e+07   \n",
    " \n",
    "                            time    xatc_min     xatc_max  n_phot  \\\n",
    " mframe                                                             \n",
    " 219511169  2019-08-22, 15:49:07    0.000000   143.638596    4000   \n",
    " 219511170  2019-08-22, 15:49:07  142.544434   285.478184    3892   \n",
    " 219511171  2019-08-22, 15:49:07  284.469646   427.248721    3854   \n",
    " 219511172  2019-08-22, 15:49:07  426.129967   569.340928    3663   \n",
    " 219511173  2019-08-22, 15:49:07  568.290242   711.246904    3310   \n",
    " 219511174  2019-08-22, 15:49:07  710.292189   853.168347    3531   \n",
    " 219511175  2019-08-22, 15:49:07  852.352002   995.397365    3365   \n",
    " 219511176  2019-08-22, 15:49:07  994.548746  1137.210580    3732   \n",
    " \n",
    "            lake_qual_pass  ratio_2nd_returns  alignment_penalty  \\\n",
    " mframe                                                            \n",
    " 219511169           False                0.0           0.000000   \n",
    " 219511170           False                0.0           0.000000   \n",
    " 219511171           False                0.0           0.000000   \n",
    " 219511172            True                0.6           0.754098   \n",
    " 219511173            True                0.9           0.563380   \n",
    " 219511174           False                0.2           0.000000   \n",
    " 219511175           False                0.0           0.000000   \n",
    " 219511176           False                0.0           0.000000   \n",
    " \n",
    "            range_penalty  length_penalty  quality_secondreturns  \\\n",
    " mframe                                                            \n",
    " 219511169            0.0        0.000000               0.000000   \n",
    " 219511170            0.0        0.000000               0.000000   \n",
    " 219511171            0.0        0.000000               0.000000   \n",
    " 219511172            1.0        0.464758               0.373987   \n",
    " 219511173            1.0        0.853815               0.941481   \n",
    " 219511174            0.0        0.000000               0.000000   \n",
    " 219511175            0.0        0.000000               0.000000   \n",
    " 219511176            0.0        0.000000               0.000000   \n",
    " \n",
    "            quality_summary        peak  is_flat    snr_surf   snr_upper  \\\n",
    " mframe                                                                    \n",
    " 219511169         0.000000  300.475473    False   76.312112    1.157635   \n",
    " 219511170         0.000000  299.072565    False   87.897046    1.260032   \n",
    " 219511171         0.000000  298.188815    False  183.825378    1.536946   \n",
    " 219511172         0.131073  298.142788     True  576.220353  113.142857   \n",
    " 219511173         0.452874  298.147936     True  649.493877  560.714286   \n",
    " 219511174         0.000000  298.155873     True  199.574107    6.285714   \n",
    " 219511175         0.000000  301.533396    False   44.393204    4.166667   \n",
    " 219511176         0.000000  304.815025    False   95.356767    2.707641   \n",
    " \n",
    "             snr_lower  \n",
    " mframe                 \n",
    " 219511169    1.598639  \n",
    " 219511170    1.808756  \n",
    " 219511171    8.914286  \n",
    " 219511172  188.571429  \n",
    " 219511173  280.357143  \n",
    " 219511174   22.000000  \n",
    " 219511175    2.000000  \n",
    " 219511176    1.279435  ,\n",
    " 'date_time': '2019-08-22, 15:49:07',\n",
    " 'detection_2nd_returns': {'h': [295.4927876734144,\n",
    "   295.8927876734144,\n",
    "   296.7927876734143,\n",
    "   296.19278767341433,\n",
    "   295.8927876734144,\n",
    "   294.4927876734144,\n",
    "   295.59793597818134,\n",
    "   295.89793597818135,\n",
    "   294.6979359781814,\n",
    "   294.3979359781814,\n",
    "   294.9979359781814,\n",
    "   295.89793597818135,\n",
    "   296.1979359781813,\n",
    "   296.3979359781813,\n",
    "   295.89793597818135,\n",
    "   296.00587253459076,\n",
    "   296.7058725345907],\n",
    "  'xatc': [447.6116114947945,\n",
    "   461.93270757608116,\n",
    "   504.89599582739174,\n",
    "   519.2170919124037,\n",
    "   533.5381879936904,\n",
    "   562.1803801637143,\n",
    "   575.4380754306912,\n",
    "   589.7337416484952,\n",
    "   604.0294078662992,\n",
    "   618.3250740841031,\n",
    "   632.6207403019071,\n",
    "   646.9164065178484,\n",
    "   661.2120727356523,\n",
    "   675.5077389534563,\n",
    "   689.8034051712602,\n",
    "   717.4359971471131,\n",
    "   731.7236129492521],\n",
    "  'prom': [0.1284928810151718,\n",
    "   0.36515421012446836,\n",
    "   0.2049309245563225,\n",
    "   0.5289029759808644,\n",
    "   0.15810893699617254,\n",
    "   0.21721294981425124,\n",
    "   0.24791512631471685,\n",
    "   0.6715179576502254,\n",
    "   0.21817892516864765,\n",
    "   0.23845592164082105,\n",
    "   0.15905871131101604,\n",
    "   0.30926429678690637,\n",
    "   0.36201984225626305,\n",
    "   0.5859747608510968,\n",
    "   0.46658839261263607,\n",
    "   0.34591209836855835,\n",
    "   0.16736605761431464]},\n",
    " 'len_subsegs': 14.197058896673843,\n",
    " 'lat_min': 79.08994692762658,\n",
    " 'lat_max': 79.099935169235,\n",
    " 'lat': 79.09493431819325,\n",
    " 'lat_str': '79.09493°N',\n",
    " 'lon_min': -21.895449181790084,\n",
    " 'lon_max': -21.88490884781792,\n",
    " 'lon': -21.89018513076273,\n",
    " 'lon_str': '21.89019°W',\n",
    " 'ice_sheet': 'GrIS',\n",
    " 'melt_season': '2019',\n",
    " 'oaurl': 'https://openaltimetry.org/data/icesat2/elevation?product=ATL03&zoom_level=7&tab=photon&date=2019-08-22&minx=-21.895449181790084&miny=79.08994692762658&maxx=-21.88490884781792&maxy=79.099935169235&tracks=848&mapType=arctic&beams=4',\n",
    " 'surface_elevation': 298.14786182580076,\n",
    " 'surface_extent_detection': [[390.5, 768.5]],\n",
    " 'length_extent': 378.0,\n",
    " 'lat_surface_extent_detection': [[79.09318389316891, 79.09650013915729]],\n",
    " 'full_lat_extent_detection': [79.09318389316891, 79.09650013915729],\n",
    " 'detection_quality_info': {'strength_2nd_returns': 0.44930861634434227,\n",
    "  'h_range_2nd_returns': 0.8062257748298197,\n",
    "  'lake_length': 0.6533353413618184,\n",
    "  'lake_depth': 0.6581824951987176,\n",
    "  'qual_alignment': 0.13918205896853608},\n",
    " 'detection_quality': 0.021680393554695927}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10569c-7984-45c3-b031-c96f0a2a2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def write_to_hdf5(self, filename):\n",
    "    \n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        \n",
    "        comp=\"gzip\"\n",
    "        \n",
    "        props = f.create_group('properties')\n",
    "        props.create_dataset('granule_id', data=self.granule_id, dtype='S39')\n",
    "        \n",
    "        phdat = f.create_group('photon_data')\n",
    "        phdat.create_dataset('lon', data=self.photon_data.lon, dtype='f8', compression=comp)\n",
    "        phdat.create_dataset('lat', data=self.photon_data.lat, dtype='f8', compression=comp)\n",
    "        phdat.create_dataset('h', data=self.photon_data.h, dtype='f8', compression=comp)\n",
    "        phdat.create_dataset('xatc', data=self.photon_data.xatc, dtype='f8', compression=comp)\n",
    "        phdat.create_dataset('mframe', data=self.photon_data.xatc, dtype='uint32', compression=comp)\n",
    "        phdat.create_dataset('ph_id_pulse', data=self.photon_data.xatc, dtype='uint8', compression=comp)\n",
    "        \n",
    "    return\n",
    "\n",
    "from icelakes.utilities import get_size\n",
    "mylake = lake_list_ordered[0]\n",
    "\n",
    "filename = 'data_lakes_detected/lake_example.h5'\n",
    "write_to_hdf5(mylake, filename)\n",
    "print(get_size(filename))\n",
    "\n",
    "filename = 'data_lakes_detected/lake_example.pkl'\n",
    "with open(filename, 'wb') as f: pickle.dump(vars(lake), f)\n",
    "print(get_size(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f085cd-fae6-44e3-b4e9-55ff27da1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists('test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13f9cf-62fb-4660-95d5-894497f1ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def descend_obj(obj,sep='\\t'):\n",
    "    \"\"\"\n",
    "    Iterate through groups in a HDF5 file and prints the groups and datasets names and datasets attributes\n",
    "    \"\"\"\n",
    "    if type(obj) in [h5py._hl.group.Group,h5py._hl.files.File]:\n",
    "        for key in obj.keys():\n",
    "            print(sep,'-',key,':',obj[key])\n",
    "            descend_obj(obj[key],sep=sep+'\\t')\n",
    "    elif type(obj)==h5py._hl.dataset.Dataset:\n",
    "        for key in obj.attrs.keys():\n",
    "            print(sep+'\\t','-',key,':',obj.attrs[key])\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    descend_obj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39826c66-d277-48a3-8250-187236ab8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "quality_list = []\n",
    "for i,lake in enumerate(lake_list):\n",
    "    quality_list.append(lake.detection_quality)\n",
    "\n",
    "sort_idxs = np.argsort(1-np.array(quality_list))\n",
    "lake_list_ordered = [lake_list[i] for i in sort_idxs]\n",
    "\n",
    "for lake in lake_list_ordered:\n",
    "    \n",
    "    fig = lake.plot_detected(min_width=0.0, min_depth=0.0)\n",
    "    filename_base = 'lake_%05i_%s_%s_%s_%s_%s.jpg' % ((1-lake.detection_quality)*10000, lake.ice_sheet, lake.melt_season, \n",
    "                                                          lake.polygon_name, lake.granule_id[:-4], lake.gtx)\n",
    "    figname = 'figs_quality/%s.jpg' % filename_base\n",
    "    if fig is not None: fig.savefig(figname, dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    display(fig)\n",
    "    \n",
    "    print('lake number: %i' % i)\n",
    "    print(lake.oaurl)\n",
    "    \n",
    "    pklname = 'pickles/%s.pkl' % filename_base\n",
    "    with open(pklname, 'wb') as f: pickle.dump(vars(lake), f)\n",
    "    print('wrote to file: pklname')\n",
    "    print('_______________________________________________________________________________', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a073b55-baa9-448c-a1ec-c3da60efed52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1508af7-5d63-4faf-ade2-b82fc79014c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049962b-e0ef-4169-9180-6460a80b89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "for dirtozip in ['figs_quality', 'pickles']:\n",
    "    filelist = [dirtozip+'/'+f for f in os.listdir(dirtozip) if os.path.isfile(os.path.join(dirtozip+'/', f))]\n",
    "    ZipFile = zipfile.ZipFile(\"zip_test_%s.zip\"%dirtozip, \"w\" )\n",
    "    for file in filelist:\n",
    "        ZipFile.write(file, compress_type=zipfile.ZIP_DEFLATED)\n",
    "    ZipFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ff97d-7fdf-4d9e-80bc-ce0914dc0b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbbf27-101a-4633-9021-74929eeced0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6bad0-54c4-4dc5-9a96-37f5a9825e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepyx-env",
   "language": "python",
   "name": "icepyx-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

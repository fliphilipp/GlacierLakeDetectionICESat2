{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c97d73-f6fa-4f2c-a8b3-2aaad23a4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "import rsa\n",
    "\n",
    "granule = 'ATL03_20210715182907_03381203_005_01.h5'\n",
    "shapefile = '/shapefiles/jakobshavn_small.shp'\n",
    "gtxs = 'all'\n",
    "datadir = '/IS2data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99f2e5a4-9cf6-4024-8fe8-18dc8e1f042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_granule_nsidc(granule_id, gtxs, shapefile, granule_output_path, uid, pwd): \n",
    "    \"\"\"\n",
    "    Download a single ICESat-2 ATL03 granule based on its producer ID,\n",
    "    subsets it to a given shapefile, and puts it into the specified\n",
    "    output directory as a .h5 file. A NASA earthdata user id (uid), and\n",
    "    the associated password are required. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    granule_id : string\n",
    "        the producer_granule_id for CMR search\n",
    "    gtxs : string or list\n",
    "        the ground tracks to request\n",
    "        possible values:\n",
    "            'gt1l' or 'gt1r' or 'gt2l', ... (single gtx)\n",
    "            ['gt1l', 'gt3r', ...] (list of gtxs)\n",
    "    shapefile : string\n",
    "        filepath to the shapefile used for spatial subsetting\n",
    "    granule_output_path : string\n",
    "        folder in which to save the subsetted granule\n",
    "    uid : string\n",
    "        earthdata user id\n",
    "    pwd : string\n",
    "        the associated password\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nothing\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> download_granule_nsidc(granule_id='ATL03_20210715182907_03381203_005_01.h5', \n",
    "                               shapefile='/shapefiles/jakobshavn.shp', \n",
    "                               gtxs='gt1l'\n",
    "                               granule_output_path='/IS2data', \n",
    "                               uid='myuserid', \n",
    "                               pwd='mypasword')\n",
    "    \"\"\"\n",
    "    \n",
    "    import requests\n",
    "    import json\n",
    "    import zipfile\n",
    "    import os\n",
    "    import shutil\n",
    "    import re\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Polygon, mapping\n",
    "    from shapely.geometry.polygon import orient\n",
    "    from xml.etree import ElementTree as ET\n",
    "    import numpy as np\n",
    "    \n",
    "    short_name = 'ATL03'\n",
    "    version = granule_id[30:33]\n",
    "    granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "    capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{version}.xml'\n",
    "    base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "    \n",
    "    shapefile_filepath = str(os.getcwd() + shapefile)\n",
    "    \n",
    "    # set the variables for subsetting\n",
    "    vars_sub = ['/ancillary_data/atlas_sdp_gps_epoch',\n",
    "                '/orbit_info/rgt',\n",
    "                '/orbit_info/cycle_number',\n",
    "                '/orbit_info/sc_orient',\n",
    "                '/gtx/geolocation/ph_index_beg',\n",
    "                '/gtx/geolocation/segment_dist_x',\n",
    "                '/gtx/geolocation/segment_length',\n",
    "                '/gtx/geophys_corr/dem_h',\n",
    "                '/gtx/geophys_corr/geoid',\n",
    "                '/gtx/bckgrd_atlas/pce_mframe_cnt',\n",
    "                '/gtx/bckgrd_atlas/bckgrd_counts',\n",
    "                '/gtx/bckgrd_atlas/bckgrd_int_height',\n",
    "                '/gtx/bckgrd_atlas/delta_time',\n",
    "                '/gtx/heights/lat_ph',\n",
    "                '/gtx/heights/lon_ph',\n",
    "                '/gtx/heights/h_ph',\n",
    "                '/gtx/heights/dist_ph_along',\n",
    "                '/gtx/heights/delta_time',\n",
    "                '/gtx/heights/pce_mframe_cnt',\n",
    "                '/gtx/heights/quality_ph']\n",
    "    beam_list = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "    \n",
    "    if gtxs == 'all':\n",
    "        var_list = sum([[v.replace('/gtx','/'+bm) for bm in beam_list] if '/gtx' in v else [v] for v in vars_sub],[])\n",
    "    elif type(gtxs) == str:\n",
    "        var_list = [v.replace('/gtx','/'+gtxs.lower()) if '/gtx' in v else v for v in vars_sub]\n",
    "    elif type(gtxs) == list:\n",
    "        var_list = sum([[v.replace('/gtx','/'+bm.lower()) for bm in gtxs] if '/gtx' in v else [v] for v in vars_sub],[])\n",
    "    else: # default to requesting all beams\n",
    "        var_list = sum([[v.replace('/gtx','/'+bm) for bm in beam_list] if '/gtx' in v else [v] for v in vars_sub],[])\n",
    "    \n",
    "    # search for the given granule\n",
    "    search_params = {\n",
    "        'short_name': short_name,\n",
    "        'page_size': 100,\n",
    "        'page_num': 1,\n",
    "        'producer_granule_id': granule_id}\n",
    "\n",
    "    granules = []\n",
    "    headers={'Accept': 'application/json'}\n",
    "    while True:\n",
    "        response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "        results = json.loads(response.content)\n",
    "\n",
    "        if len(results['feed']['entry']) == 0:\n",
    "            # Out of results, so break out of loop\n",
    "            break\n",
    "\n",
    "        # Collect results and increment page_num\n",
    "        granules.extend(results['feed']['entry'])\n",
    "        search_params['page_num'] += 1\n",
    "\n",
    "    print('\\nDownloading ICESat-2 data. Found granules:')\n",
    "    for result in granules:\n",
    "        print('  '+result['producer_granule_id'], f', {float(result[\"granule_size\"]):.2f} MB',sep='')\n",
    "        \n",
    "    # Use geopandas to read in polygon file as GeoDataFrame object \n",
    "    # Note: a KML or geojson, or almost any other vector-based spatial data format could be substituted here.\n",
    "    gdf = gpd.read_file(shapefile_filepath)\n",
    "    \n",
    "    # Simplify polygon for complex shapes in order to pass a reasonable request length to CMR. \n",
    "    # The larger the tolerance value, the more simplified the polygon.\n",
    "    # Orient counter-clockwise: CMR polygon points need to be provided in counter-clockwise order. \n",
    "    # The last point should match the first point to close the polygon.\n",
    "    poly = orient(gdf.simplify(0.05, preserve_topology=False).loc[0],sign=1.0)\n",
    "\n",
    "    geojson = gpd.GeoSeries(poly).to_json() # Convert to geojson\n",
    "    geojson = geojson.replace(' ', '') #remove spaces for API call\n",
    "    \n",
    "    #Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "    polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "    \n",
    "    print('\\nInput shapefile:', shapefile)\n",
    "    print('Simplified polygon coordinates based on shapefile input:', polygon)\n",
    "    \n",
    "    # Create session to store cookie and pass credentials to capabilities url\n",
    "    session = requests.session()\n",
    "    s = session.get(capability_url)\n",
    "    response = session.get(s.url,auth=(uid,pwd))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('\\n\\n', uid)\n",
    "    print(pwd)\n",
    "    print(capability_url)\n",
    "    print(s.url)\n",
    "    print(response)\n",
    "    print(response.content, '\\n\\n')\n",
    "    '''\n",
    "    root = ET.fromstring(response.content)\n",
    "\n",
    "    #collect lists with each service option\n",
    "    subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "    \n",
    "    # this is for getting possible variable values from the granule search\n",
    "    if len(subagent) > 0 :\n",
    "        # variable subsetting\n",
    "        variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "        variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "        variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "        variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "    \n",
    "    # make sure to only request the variables that are available\n",
    "    def intersection(lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3\n",
    "    var_list_subsetting = intersection(variable_vals,var_list)\n",
    "    \n",
    "    if len(subagent) < 1 :\n",
    "        print('No services exist for', short_name, 'version', latest_version)\n",
    "        agent = 'NO'\n",
    "        coverage,Boundingshape = '',''\n",
    "    else:\n",
    "        agent = ''\n",
    "        subdict = subagent[0]\n",
    "        if subdict['spatialSubsettingShapefile'] == 'true':\n",
    "            Boundingshape = geojson\n",
    "        else:\n",
    "            Boundingshape = ''\n",
    "        coverage = ','.join(var_list_subsetting)\n",
    "    '''\n",
    "    agent = ''\n",
    "    Boundingshape = geojson\n",
    "    Boundingshape = ''\n",
    "    coverage = ','.join(vars_sub)\n",
    "    coverage = ''\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    page_size = 100\n",
    "    request_mode = 'stream'\n",
    "    page_num = int(np.ceil(len(granules)/page_size))\n",
    "\n",
    "    param_dict = {'short_name': short_name, \n",
    "                  'producer_granule_id': granule_id,\n",
    "                  'version': version,  \n",
    "                  'polygon': polygon,\n",
    "                  'Boundingshape': Boundingshape,  \n",
    "                  'Coverage': coverage, \n",
    "                  'page_size': page_size, \n",
    "                  'request_mode': request_mode, \n",
    "                  'agent': agent, \n",
    "                  'email': 'yes'}\n",
    "\n",
    "    #Remove blank key-value-pairs\n",
    "    param_dict = {k: v for k, v in param_dict.items() if v != ''}\n",
    "\n",
    "    #Convert to string\n",
    "    param_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in param_dict.items())\n",
    "    param_string = param_string.replace(\"'\",\"\")\n",
    "\n",
    "    #Print API base URL + request parameters\n",
    "    endpoint_list = [] \n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        API_request = api_request = f'{base_url}?{param_string}&page_num={page_val}'\n",
    "        endpoint_list.append(API_request)\n",
    "\n",
    "    print('\\nAPI request URL:')\n",
    "    print(*endpoint_list, sep = \"\\n\") \n",
    "    \n",
    "    # Create an output folder if the folder does not already exist.\n",
    "    path = str(os.getcwd() + granule_output_path)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    # Different access methods depending on request mode:\n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        print('\\nOrder: ', page_val)\n",
    "        print('Requesting...')\n",
    "        request = session.get(base_url, params=param_dict)\n",
    "        print('HTTP response from order response URL: ', request.status_code)\n",
    "        request.raise_for_status()\n",
    "        d = request.headers['content-disposition']\n",
    "        fname = re.findall('filename=(.+)', d)\n",
    "        dirname = os.path.join(path,fname[0].strip('\\\"'))\n",
    "        print('Downloading...')\n",
    "        open(dirname, 'wb').write(request.content)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "\n",
    "    # Unzip outputs\n",
    "    for z in os.listdir(path): \n",
    "        if z.endswith('.zip'): \n",
    "            zip_name = path + \"/\" + z \n",
    "            zip_ref = zipfile.ZipFile(zip_name) \n",
    "            zip_ref.extractall(path) \n",
    "            zip_ref.close() \n",
    "            os.remove(zip_name) \n",
    "\n",
    "    # Clean up Outputs folder by removing individual granule folders \n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for file in files:\n",
    "            try:\n",
    "                shutil.move(os.path.join(root, file), path)\n",
    "            except OSError:\n",
    "                pass\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "            \n",
    "    print('\\nUnzipped files and cleaned up directory.')\n",
    "    print('Output data saved in:', granule_output_path)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4109e218-83b3-4b5d-abad-f15f05d3db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading ICESat-2 data. Found granules:\n",
      "  ATL03_20210715182907_03381203_005_01.h5, 5065.14 MB\n",
      "\n",
      "Input shapefile: /shapefiles/jakobshavn_small.shp\n",
      "Simplified polygon coordinates based on shapefile input: -49.59098555896507,69.10415896488335,-50.33805587146507,68.42374368072217,-50.01945235584007,68.20452489563448,-48.47038009021507,68.14325568125464,-48.16276290271507,68.66085965388693,-48.12980391834007,69.0963205694427,-49.59098555896507,69.10415896488335\n",
      "\n",
      "\n",
      " arndtp\n",
      "My5ecretPw0rd!\n",
      "https://n5eil02u.ecs.nsidc.org/egi/capabilities/ATL03.005.xml\n",
      "https://uat.urs.earthdata.nasa.gov/oauth/authorize?app_type=401&client_id=PGVMJ5nUzSnQkI5o23gMxA&response_type=code&redirect_uri=https%3A%2F%2Fn5eil02u.ecs.nsidc.org%2FOPS%2Fredirect&state=aHR0cHM6Ly9uNWVpbDAydS5lY3MubnNpZGMub3JnL2VnaS9jYXBhYmlsaXRpZXMvQVRMMDMuMDA1LnhtbA\n",
      "<Response [400]>\n",
      "b'{\"error\":\"invalid_request\"}' \n",
      "\n",
      "\n",
      "\n",
      "API request URL:\n",
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=ATL03&producer_granule_id=ATL03_20210715182907_03381203_005_01.h5&version=005&polygon=-49.59098555896507,69.10415896488335,-50.33805587146507,68.42374368072217,-50.01945235584007,68.20452489563448,-48.47038009021507,68.14325568125464,-48.16276290271507,68.66085965388693,-48.12980391834007,69.0963205694427,-49.59098555896507,69.10415896488335&page_size=100&request_mode=stream&email=yes&page_num=1\n",
      "\n",
      "Order:  1\n",
      "Requesting...\n",
      "HTTP response from order response URL:  400\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://uat.urs.earthdata.nasa.gov/oauth/authorize?app_type=401&client_id=PGVMJ5nUzSnQkI5o23gMxA&response_type=code&redirect_uri=https%3A%2F%2Fn5eil02u.ecs.nsidc.org%2FOPS%2Fredirect&state=aHR0cHM6Ly9uNWVpbDAydS5lY3MubnNpZGMub3JnL2VnaS9yZXF1ZXN0P3Nob3J0X25hbWU9QVRMMDMmcHJvZHVjZXJfZ3JhbnVsZV9pZD1BVEwwM18yMDIxMDcxNTE4MjkwN18wMzM4MTIwM18wMDVfMDEuaDUmdmVyc2lvbj0wMDUmcG9seWdvbj0tNDkuNTkwOTg1NTU4OTY1MDclMkM2OS4xMDQxNTg5NjQ4ODMzNSUyQy01MC4zMzgwNTU4NzE0NjUwNyUyQzY4LjQyMzc0MzY4MDcyMjE3JTJDLTUwLjAxOTQ1MjM1NTg0MDA3JTJDNjguMjA0NTI0ODk1NjM0NDglMkMtNDguNDcwMzgwMDkwMjE1MDclMkM2OC4xNDMyNTU2ODEyNTQ2NCUyQy00OC4xNjI3NjI5MDI3MTUwNyUyQzY4LjY2MDg1OTY1Mzg4NjkzJTJDLTQ4LjEyOTgwMzkxODM0MDA3JTJDNjkuMDk2MzIwNTY5NDQyNyUyQy00OS41OTA5ODU1NTg5NjUwNyUyQzY5LjEwNDE1ODk2NDg4MzM1JnBhZ2Vfc2l6ZT0xMDAmcmVxdWVzdF9tb2RlPXN0cmVhbSZlbWFpbD15ZXM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d8bbd04f6d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownload_granule_nsidc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgranule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgtxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapefile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecedc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecedc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-615735a19faf>\u001b[0m in \u001b[0;36mdownload_granule_nsidc\u001b[0;34m(granule_id, gtxs, shapefile, granule_output_path, uid, pwd)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HTTP response from order response URL: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content-disposition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filename=(.+)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/icepyx-env/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://uat.urs.earthdata.nasa.gov/oauth/authorize?app_type=401&client_id=PGVMJ5nUzSnQkI5o23gMxA&response_type=code&redirect_uri=https%3A%2F%2Fn5eil02u.ecs.nsidc.org%2FOPS%2Fredirect&state=aHR0cHM6Ly9uNWVpbDAydS5lY3MubnNpZGMub3JnL2VnaS9yZXF1ZXN0P3Nob3J0X25hbWU9QVRMMDMmcHJvZHVjZXJfZ3JhbnVsZV9pZD1BVEwwM18yMDIxMDcxNTE4MjkwN18wMzM4MTIwM18wMDVfMDEuaDUmdmVyc2lvbj0wMDUmcG9seWdvbj0tNDkuNTkwOTg1NTU4OTY1MDclMkM2OS4xMDQxNTg5NjQ4ODMzNSUyQy01MC4zMzgwNTU4NzE0NjUwNyUyQzY4LjQyMzc0MzY4MDcyMjE3JTJDLTUwLjAxOTQ1MjM1NTg0MDA3JTJDNjguMjA0NTI0ODk1NjM0NDglMkMtNDguNDcwMzgwMDkwMjE1MDclMkM2OC4xNDMyNTU2ODEyNTQ2NCUyQy00OC4xNjI3NjI5MDI3MTUwNyUyQzY4LjY2MDg1OTY1Mzg4NjkzJTJDLTQ4LjEyOTgwMzkxODM0MDA3JTJDNjkuMDk2MzIwNTY5NDQyNyUyQy00OS41OTA5ODU1NTg5NjUwNyUyQzY5LjEwNDE1ODk2NDg4MzM1JnBhZ2Vfc2l6ZT0xMDAmcmVxdWVzdF9tb2RlPXN0cmVhbSZlbWFpbD15ZXM"
     ]
    }
   ],
   "source": [
    "download_granule_nsidc(granule, gtxs, shapefile, datadir, decedc(edc().u), decedc(edc().p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6af77-f4af-41b6-b51d-a5659919ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [datadir[1:]+'/'+f for f in listdir(datadir[1:]) if isfile(join(datadir[1:], f)) & ('.h5' in f)]\n",
    "print('\\nNumber of processed ATL03 granules to read in: ' + str(len(filelist)))\n",
    "    \n",
    "photon_data, bckgrd_data, ancillary = read_atl03(filelist[0], geoid_h=True)\n",
    "print_granule_stats(photon_data, bckgrd_data, ancillary, outfile='stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8794d1-c824-4a9e-a4b4-a697e8f75299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = 'Outputs_ATL03_test/'\n",
    "datapath = 'IS2data/'\n",
    "filelist = [datapath+f for f in listdir(datapath) if isfile(join(datapath, f)) & ('.h5' in f)]\n",
    "print('number of processed ATL03 granules: ' + str(len(filelist)))\n",
    "for f in filelist: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40602965-7bfe-4da7-9717-39512ec1f215",
   "metadata": {},
   "source": [
    "# a function for reading in the relevant variables from ATL03 granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dff47a-5c58-4234-9949-155f70fd7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_atl03(filename, geoid_h=True):\n",
    "    # reads in the necessary variables at photon rate from an .h5 file in a dataframe:\n",
    "    # - lat: latitude\n",
    "    # - xatc: along-track distance from the equator crossing\n",
    "    # - h: elevation above the geoid (or above reference ellipsoid if geoid_h is set to False)\n",
    "    \n",
    "    # open file\n",
    "    f = h5py.File(filename, 'r')\n",
    "    \n",
    "    # make dictionary for beam data to be stored in\n",
    "    dfs = {}\n",
    "    dfs_bckgrd = {}\n",
    "    beamlist = [x for x in list(f.keys()) if 'gt' in x]\n",
    "    \n",
    "    conf_landice = 3 # index for the land ice confidence\n",
    "    \n",
    "    orient = f['orbit_info']['sc_orient'][0]\n",
    "    def orient_string(sc_orient):\n",
    "        if sc_orient == 0:\n",
    "            return 'backward'\n",
    "        elif sc_orient == 1:\n",
    "            return 'forward'\n",
    "        elif sc_orient == 2:\n",
    "            return 'transition'\n",
    "        else:\n",
    "            return 'error'\n",
    "        \n",
    "    orient_str = orient_string(orient)\n",
    "    gtl = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "    beam_strength_dict = {k:['weak','strong'][k%2] for k in np.arange(1,7,1)}\n",
    "    if orient_str == 'forward':\n",
    "        bl = np.arange(6,0,-1)\n",
    "        gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "        gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "    elif orient_str == 'backward':\n",
    "        bl = np.arange(1,7,1)\n",
    "        gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "        gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "    else:\n",
    "        gtx_beam_dict = {k:'undefined' for k in gtl}\n",
    "        gtx_strength_dict = {k:'undefined' for k in gtl}\n",
    "        \n",
    "\n",
    "    ancillary = {'atlas_sdp_gps_epoch': f['ancillary_data']['atlas_sdp_gps_epoch'][0],\n",
    "                 'rgt': f['orbit_info']['rgt'][0],\n",
    "                 'cycle_number': f['orbit_info']['cycle_number'][0],\n",
    "                 'sc_orient': orient_str,\n",
    "                 'gtx_beam_dict': gtx_beam_dict,\n",
    "                 'gtx_strength_dict': gtx_strength_dict}\n",
    "    \n",
    "    # loop through all beams\n",
    "    for beam in beamlist:\n",
    "        try:\n",
    "            #### get photon-level data\n",
    "            df = pd.DataFrame({'lat': np.array(f[beam]['heights']['lat_ph']),\n",
    "                               'lon': np.array(f[beam]['heights']['lon_ph']),\n",
    "                               'h': np.array(f[beam]['heights']['h_ph']),\n",
    "                               'dt': np.array(f[beam]['heights']['delta_time']),\n",
    "                               # 'conf': np.array(f[beam]['heights']['signal_conf_ph'][:,conf_landice]),\n",
    "                               # not using ATL03 confidences here\n",
    "                               'mframe': np.array(f[beam]['heights']['pce_mframe_cnt']),\n",
    "                               'qual': np.array(f[beam]['heights']['quality_ph'])}) \n",
    "                               # 0=nominal,1=afterpulse,2=impulse_response_effect,3=tep\n",
    "\n",
    "            df_bckgrd = pd.DataFrame({'pce_mframe_cnt': np.array(f[beam]['bckgrd_atlas']['pce_mframe_cnt']),\n",
    "                                      'bckgrd_counts': np.array(f[beam]['bckgrd_atlas']['bckgrd_counts']),\n",
    "                                      'bckgrd_int_height': np.array(f[beam]['bckgrd_atlas']['bckgrd_int_height']),\n",
    "                                      'delta_time': np.array(f[beam]['bckgrd_atlas']['delta_time'])})\n",
    "\n",
    "            #### calculate along-track distances [meters from the equator crossing] from segment-level data\n",
    "            df['xatc'] = np.full_like(df.lat, fill_value=np.nan)\n",
    "            ph_index_beg = np.int32(f[beam]['geolocation']['ph_index_beg']) - 1\n",
    "            segment_dist_x = np.array(f[beam]['geolocation']['segment_dist_x'])\n",
    "            segment_length = np.array(f[beam]['geolocation']['segment_length'])\n",
    "            valid = ph_index_beg>=0 # need to delete values where there's no photons in the segment (-1 value)\n",
    "\n",
    "            df.loc[ph_index_beg[valid], 'xatc'] = segment_dist_x[valid]\n",
    "            df.xatc.fillna(method='ffill',inplace=True)\n",
    "            df.xatc += np.array(f[beam]['heights']['dist_ph_along'])\n",
    "\n",
    "            #### now we can filter out TEP (we don't do IRF / afterpulses because it seems to not be very good...)\n",
    "            df.query('qual < 3',inplace=True) \n",
    "            # df.drop(columns=['qual'], inplace=True)\n",
    "\n",
    "            #### sort by along-track distance (for interpolation to work smoothly)\n",
    "            df.sort_values(by='xatc',inplace=True)\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            if geoid_h:\n",
    "                #### interpolate geoid to photon level using along-track distance, and add to elevation\n",
    "                geophys_geoid = np.array(f[beam]['geophys_corr']['geoid'])\n",
    "                print(np.sum(np.isnan(df.xatc)))\n",
    "                print(np.sum(np.isnan(geophys_geoid)))\n",
    "                print(np.sum(np.isnan(segment_dist_x+0.5*segment_length)))\n",
    "                geoid = np.interp(np.array(df.xatc), segment_dist_x+0.5*segment_length, geophys_geoid)\n",
    "                df['h'] = df.h - geoid\n",
    "                df['geoid'] = geoid\n",
    "\n",
    "            #### save to list of dataframes\n",
    "            dfs[beam] = df\n",
    "            dfs_bckgrd[beam] = df_bckgrd\n",
    "            print(beam, end=' ')\n",
    "        except Exception as e:\n",
    "            print('Error for {f:s} on {b:s} ... skipping:'.format(f=filename, b=beam), e)\n",
    "    return dfs, dfs_bckgrd, ancillary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef5c43-dcfa-4bda-9385-a7ba2cc77803",
   "metadata": {},
   "source": [
    "# read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2448cac-ca65-44e5-860f-7a8ece85e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ifile, file in enumerate(filelist):\n",
    "for ifile, file in enumerate([filelist[0]]):\n",
    "    print('Reading file {i:4d} / {n:4d}: {fn:s} >>> '.format(i=ifile+1, n=len(filelist), fn=file[file.find('ATL03_'):]), end = ' ')\n",
    "    photon_data, bckgrd_data, ancillary = read_atl03(file)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c005f-8b7e-428e-ac1c-3e1348767dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf6878-30a8-4eb4-8f90-9516d897905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillary['cycle_number'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09dc85d-2ac9-4221-b483-37943d9c0654",
   "metadata": {},
   "source": [
    "# a list of known lakes for development / checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17765897-87cb-4082-8863-a98bda9deb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnownLake:\n",
    "    def __init__(self, fn, gtx, latlims):\n",
    "        self.fn = fn\n",
    "        self.gtx = gtx\n",
    "        self.latlims = latlims\n",
    "    \n",
    "    def getData(self, dfs):\n",
    "        df = dfs[self.gtx]\n",
    "        return df[(df.lat > self.latlims[0]) & (df.lat < self.latlims[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ca16a-f8bc-40d5-8360-c7927b543e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = file # to keep this out of the loop for now\n",
    "lakes = {1: KnownLake(fn, 'gt1l', [67.433, 67.463]),\n",
    "         2: KnownLake(fn, 'gt1l', [68.6, 68.65]),\n",
    "         3: KnownLake(fn, 'gt1l', [68.535, 68.595]),\n",
    "         4: KnownLake(fn, 'gt1l', [68.353, 68.37]),\n",
    "         5: KnownLake(fn, 'gt1l', [68.02, 68.09]),\n",
    "         6: KnownLake(fn, 'gt1l', [67.95, 67.98]),\n",
    "         7: KnownLake(fn, 'gt1l', [67.675, 67.6975]),\n",
    "         8: KnownLake(fn, 'gt1l', [67.37, 67.4]),\n",
    "         9: KnownLake(fn, 'gt1l', [67.02, 67.055]),\n",
    "         10: KnownLake(fn, 'gt2l', [68.716, 68.726]),\n",
    "         11: KnownLake(fn, 'gt2l', [68.5, 68.547]),\n",
    "         12: KnownLake(fn, 'gt2l', [68.33, 68.375]),\n",
    "         13: KnownLake(fn, 'gt2l', [68.2425, 68.27]),\n",
    "         14: KnownLake(fn, 'gt2l', [67.9023, 67.938]),\n",
    "         15: KnownLake(fn, 'gt2l', [67.617, 67.671]),\n",
    "         16: KnownLake(fn, 'gt2l', [67.3827, 67.4151]),\n",
    "         17: KnownLake(fn, 'gt2l', [67.0057, 67.0408]),\n",
    "         18: KnownLake(fn, 'gt2r', [67.3826, 67.4171]),\n",
    "         19: KnownLake(fn, 'gt3l', [68.5505, 68.5826]),\n",
    "         20: KnownLake(fn, 'gt3l', [68.4309, 68.4508]),\n",
    "         21: KnownLake(fn, 'gt3l', [67.07, 67.1]),\n",
    "         22: KnownLake(fn, 'gt3l', [66.9306, 67.0137]),\n",
    "         23: KnownLake(fn, 'gt2l', [-71.6687, -71.6166]),\n",
    "         24: KnownLake(fn, 'gt2l', [-71.8903, -71.8587]),\n",
    "         25: KnownLake(fn, 'gt2l', [-72.9505, -72.8595]),\n",
    "         26: KnownLake(fn, 'gt1l', [-72.6468, -72.5127]),\n",
    "         27: KnownLake(fn, 'gt3l', [-73.2389, -73.2073]),\n",
    "         28: KnownLake(fn, 'gt1l', [-70.85, -70.745]),\n",
    "         29: KnownLake(fn, 'gt1l', [-71.513, -71.445]),\n",
    "         30: KnownLake(fn, 'gt1l', [-72.94, -72.85]),\n",
    "         31: KnownLake(fn, 'gt3l', [-72.8602, -72.6714]),\n",
    "         32: KnownLake(fn, 'gt3l', [-72.08, -71.99]),\n",
    "         33: KnownLake(fn, 'gt1l', [66, 69]),\n",
    "         34: KnownLake(fn, 'gt2l', [66, 69]),\n",
    "         35: KnownLake(fn, 'gt3l', [66, 69]),\n",
    "         36: KnownLake(fn, 'gt1r', [66, 69]),\n",
    "         37: KnownLake(fn, 'gt2r', [66, 69]),\n",
    "         38: KnownLake(fn, 'gt3r', [66, 69]),\n",
    "         39: KnownLake(fn, 'gt1l', [-74, -70.5]),\n",
    "         40: KnownLake(fn, 'gt2l', [-74, -70.5]),\n",
    "         41: KnownLake(fn, 'gt3l', [-74, -70.5]),\n",
    "         42: KnownLake(fn, 'gt1r', [-74, -70.5]),\n",
    "         43: KnownLake(fn, 'gt2r', [-74, -70.5]),\n",
    "         44: KnownLake(fn, 'gt3r', [-74, -70.5]),\n",
    "         45: KnownLake(fn, 'gt1l', [63, 72]),\n",
    "         46: KnownLake(fn, 'gt1l', [-90, 90]),\n",
    "         47: KnownLake(fn, 'gt2l', [-90, 90]),\n",
    "         48: KnownLake(fn, 'gt3l', [-90, 90]),\n",
    "         49: KnownLake(fn, 'gt1r', [-90, 90]),\n",
    "         50: KnownLake(fn, 'gt2r', [-90, 90]),\n",
    "         51: KnownLake(fn, 'gt3r', [-90, 90]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e68c9-84a8-4e47-9fe2-0ab4a93763e5",
   "metadata": {},
   "source": [
    "# make a DataFrame for a given granule/gtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d268395-1ee3-488a-8433-7c5a0f56db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "thisLake = 46\n",
    "df = lakes[thisLake].getData(photon_data)\n",
    "df['snr'] = 0.0\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992a9e8-53b9-48cf-92ec-7410aaec55cc",
   "metadata": {},
   "source": [
    "# create a dataframe that aggregates info at the major frame level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337028e7-863b-4447-9b76-9e1c128a448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mframe_group = df.groupby('mframe')\n",
    "df_mframe = mframe_group[['lat','lon', 'xatc', 'dt']].mean()\n",
    "df_mframe.drop(df_mframe.head(1).index,inplace=True)\n",
    "df_mframe.drop(df_mframe.tail(1).index,inplace=True)\n",
    "def convert_time_to_string(dt):\n",
    "    epoch = dt + datetime.datetime.timestamp(datetime.datetime(2018,1,1))\n",
    "    return datetime.datetime.fromtimestamp(epoch).strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "df_mframe['time'] = df_mframe['dt'].map(convert_time_to_string)\n",
    "df_mframe['xatc_min'] = mframe_group['xatc'].min()\n",
    "df_mframe['xatc_max'] = mframe_group['xatc'].max()\n",
    "df_mframe['n_phot'] = mframe_group['h'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180654c-1076-4153-9e8a-2aaff776517a",
   "metadata": {},
   "source": [
    "# check for flat surfaces in each major frame of the granule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a71f11-a836-4d3f-9dcf-dce70eed6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def find_flat_lake_surfaces(mframe, df=df, bin_height_coarse=1.0, bin_height_fine=0.02, smoothing_histogram=0.2, buffer=4.0,\n",
    "                            width_surf=0.1, width_buff=0.3, rel_dens_upper_thresh=0.3, rel_dens_lower_thresh=0.3):\n",
    "    selector_segment = (df.mframe == mframe)\n",
    "    dfseg = df[selector_segment]\n",
    "    \n",
    "    # find main broad peak\n",
    "    bins_coarse1 = np.arange(start=dfseg.h.min(), stop=dfseg.h.max(), step=bin_height_coarse)\n",
    "    hist_mid1 = bins_coarse1[:-1] + 0.5 * bin_height_coarse\n",
    "    peak_loc1 = hist_mid1[np.argmax(np.histogram(dfseg.h, bins=bins_coarse1)[0])]\n",
    "    \n",
    "    # decrease bin width and find finer peak\n",
    "    bins_coarse2 = np.arange(start=peak_loc1-buffer, stop=peak_loc1+buffer, step=bin_height_fine)\n",
    "    hist_mid2 = bins_coarse2[:-1] + 0.5 * bin_height_fine\n",
    "    hist = np.histogram(dfseg.h, bins=bins_coarse2)\n",
    "    window_size = int(smoothing_histogram/bin_height_fine)\n",
    "    hist_vals = hist[0] / np.max(hist[0])\n",
    "    hist_vals_smoothed = np.array(pd.Series(hist_vals).rolling(window_size,center=True,min_periods=1).mean())\n",
    "    peak_loc2 = hist_mid2[np.argmax(hist_vals_smoothed)]\n",
    "    \n",
    "    # calculate relative photon densities\n",
    "    peak_upper = peak_loc2 + width_surf\n",
    "    peak_lower = peak_loc2 - width_surf\n",
    "    above_upper = peak_upper + width_buff\n",
    "    below_lower = peak_lower - width_buff\n",
    "    sum_peak = np.sum((dfseg.h > peak_lower) & (dfseg.h < peak_upper))\n",
    "    sum_above = np.sum((dfseg.h > peak_upper) & (dfseg.h < above_upper))\n",
    "    sum_below = np.sum((dfseg.h > below_lower) & (dfseg.h < peak_lower))\n",
    "    rel_dens_upper = (sum_above / width_buff) / (sum_peak / (width_surf*2))\n",
    "    rel_dens_lower = (sum_below / width_buff) / (sum_peak / (width_surf*2))\n",
    "    \n",
    "    # check for flat surface, if found calculate SNR and look for bottom return\n",
    "    is_flat_like_lake = (rel_dens_upper < rel_dens_upper_thresh) & (rel_dens_lower < rel_dens_lower_thresh)\n",
    "    \n",
    "    return peak_loc2, is_flat_like_lake\n",
    "\n",
    "# get all the flat segments and select\n",
    "df_mframe['peak'], df_mframe['is_flat'] = list(zip(*df_mframe.index.map(find_flat_lake_surfaces)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19ebf7-677b-4a01-90b4-d503984beabf",
   "metadata": {},
   "source": [
    "# a function for calculating SNR and checking for second returns\n",
    "(apply this only to the major frames with flat surfaces for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9997c-7087-4cfc-81ac-649f621839e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize columns to be added\n",
    "df_mframe['lake_qual_pass'] = False\n",
    "df_mframe['has_densities'] = False\n",
    "df_mframe['ratio_2nd_returns'] = 0.0\n",
    "df_mframe['quality_summary'] = 0.0\n",
    "df_mframe['h_2nd_returns'] = [[]] * len(df_mframe)\n",
    "df_mframe['xatc_2nd_returns'] = [[]] * len(df_mframe)\n",
    "df_mframe['proms_2nd_returns'] = [[]] * len(df_mframe)\n",
    "df_mframe_selected = df_mframe[df_mframe['is_flat']]\n",
    "\n",
    "def get_densities_and_2nd_peaks(df, df_mframe, df_mframe_selected, aspect=30, K_phot=10, dh_signal=0.2, n_subsegs=7,\n",
    "                                bin_height_snr=0.1, smoothing_length=1.0, buffer=4.0, print_results=False):\n",
    "    \n",
    "    for mframe in df_mframe_selected.index:\n",
    "        \n",
    "        selector_segment = (df.mframe == mframe)\n",
    "        dfseg = df[selector_segment]\n",
    "\n",
    "        xmin = df_mframe_selected['xatc_min'].loc[mframe]\n",
    "        xmax = df_mframe_selected['xatc_max'].loc[mframe]\n",
    "        nphot = df_mframe_selected['n_phot'].loc[mframe]\n",
    "        peak_loc2 = df_mframe_selected['peak'].loc[mframe]\n",
    "\n",
    "        # the radius in which to look for neighbors\n",
    "        dfseg_nosurface = dfseg[(dfseg.h > (peak_loc2+dh_signal)) | (dfseg.h < (peak_loc2-dh_signal))]\n",
    "        nphot_bckgrd = len(dfseg_nosurface.h)\n",
    "        # radius of a circle in which we expect to find one non-lake-surface signal photon\n",
    "        wid = np.sqrt(((dfseg_nosurface.h.max()-dfseg_nosurface.h.min()-2*dh_signal)*((xmax-xmin)/aspect)/nphot_bckgrd)/np.pi)\n",
    "\n",
    "        # buffer segment for density calculation\n",
    "        selector_buffer = (df.xatc >= (dfseg.xatc.min()-aspect*wid)) & (df.xatc <= (dfseg.xatc.max()+aspect*wid))\n",
    "        dfseg_buffer = df[selector_buffer]\n",
    "        dfseg_buffer.xatc += np.random.uniform(low=-0.35, high=0.35, size=len(dfseg_buffer.xatc))\n",
    "\n",
    "        # normalize xatc to be regularly spaced and scaled by the aspect parameter\n",
    "        xmin_buff = dfseg_buffer.xatc.min()\n",
    "        xmax_buff = dfseg_buffer.xatc.max()\n",
    "        nphot_buff = len(dfseg_buffer.xatc)\n",
    "        xnorm = np.linspace(xmin_buff, xmax_buff, nphot_buff) / aspect\n",
    "\n",
    "        # KD tree query distances\n",
    "        Xn = np.array(np.transpose(np.vstack((xnorm, dfseg_buffer['h']))))\n",
    "        kdt = KDTree(Xn, leaf_size=40)\n",
    "        idx, dist = kdt.query_radius(Xn, r=wid, count_only=False, return_distance=True,sort_results=True)\n",
    "        density = (np.array([np.sum(1-np.abs(x/wid)) if (len(x)<(K_phot+1)) \n",
    "                   else np.sum(1-np.abs(x[:K_phot+1]/wid))\n",
    "                   for x in dist]) - 1) / K_phot\n",
    "\n",
    "        #print(' density calculated')\n",
    "        densities = np.array(density[dfseg_buffer.mframe == mframe])\n",
    "        densities /= np.max(densities)\n",
    "\n",
    "        # add SNR to dataframes\n",
    "        dfseg['snr'] = densities\n",
    "        df.loc[selector_segment, 'snr'] = dfseg['snr']\n",
    "        df_mframe['has_densities'].loc[mframe] = True\n",
    "\n",
    "        # subdivide into segments again to check for second return\n",
    "        subsegs = np.linspace(xmin, xmax, n_subsegs+1) \n",
    "        subsegwidth = subsegs[1] - subsegs[0]\n",
    "\n",
    "        n_2nd_returns = 0\n",
    "        prominences = []\n",
    "        elev_2ndpeaks = []\n",
    "        subpeaks_xatc = []\n",
    "        for subsegstart in subsegs[:-1]:\n",
    "\n",
    "            subsegend = subsegstart + subsegwidth\n",
    "            selector_subseg = ((dfseg.xatc > subsegstart) & (dfseg.xatc < subsegend))\n",
    "            dfsubseg = dfseg[selector_subseg]\n",
    "            \n",
    "            # avoid looking for peaks when there's no / very little data\n",
    "            if len(dfsubseg > 10):\n",
    "\n",
    "                # get the median of the snr values in each bin\n",
    "                bins_subseg_snr = np.arange(start=np.max((dfsubseg.h.min(),peak_loc2-70)), stop=peak_loc2+2*buffer, step=bin_height_snr)\n",
    "                mid_subseg_snr = bins_subseg_snr[:-1] + 0.5 * bin_height_snr\n",
    "                snrstats = binned_statistic(dfsubseg.h, dfsubseg.snr, statistic='median', bins=bins_subseg_snr)\n",
    "                snr_median = snrstats[0]\n",
    "                snr_median[np.isnan(snr_median)] = 0\n",
    "                window_size_sub = int(smoothing_length/bin_height_snr)\n",
    "                snr_vals_smoothed = np.array(pd.Series(snr_median).rolling(window_size_sub,center=True,min_periods=1).mean())\n",
    "                snr_vals_smoothed /= np.max(snr_vals_smoothed)\n",
    "\n",
    "                # take histogram binning values into account, but clip surface peak to second highest peak height\n",
    "                subhist, subhist_edges = np.histogram(dfsubseg.h, bins=bins_subseg_snr)\n",
    "                subhist_nosurface = subhist.copy()\n",
    "                subhist_nosurface[(mid_subseg_snr < (peak_loc2+2*dh_signal)) & (mid_subseg_snr > (peak_loc2-2*dh_signal))] = 0\n",
    "                subhist_nosurface_smoothed = np.array(pd.Series(subhist_nosurface).rolling(window_size_sub,center=True,min_periods=1).mean())\n",
    "                subhist_max = subhist_nosurface_smoothed.max()\n",
    "                subhist_smoothed = np.array(pd.Series(subhist).rolling(window_size_sub,center=True,min_periods=1).mean())\n",
    "                subhist_smoothed = np.clip(subhist_smoothed, 0, subhist_max)\n",
    "                subhist_smoothed /= np.max(subhist_smoothed)\n",
    "\n",
    "                # combine histogram and snr values to find peaks\n",
    "                snr_hist_smoothed = subhist_smoothed * snr_vals_smoothed\n",
    "                peaks, peak_props = find_peaks(snr_hist_smoothed, height=0.1, distance=int(0.5/bin_height_snr), prominence=0.1)\n",
    "\n",
    "                if len(peaks) >= 2: \n",
    "                    idx_surfpeak = np.argmin(peak_loc2 - mid_subseg_snr[peaks])\n",
    "                    peak_props['prominences'][idx_surfpeak] = 0\n",
    "\n",
    "                    # classify as second peak only if prominence is larger 0.2\n",
    "                    prominence_secondpeak = np.max(peak_props['prominences'])\n",
    "                    prominence_threshold = 0.3\n",
    "                    if prominence_secondpeak > prominence_threshold:\n",
    "\n",
    "                        idx_2ndreturn = np.argmax(peak_props['prominences'])\n",
    "                        secondpeak_h = mid_subseg_snr[peaks[idx_2ndreturn]]\n",
    "\n",
    "                        # classify as second peak only if elevation is 0.5m lower than main peak (surface) and higher than 50m below surface\n",
    "                        if (secondpeak_h < (peak_loc2-0.5)) & (secondpeak_h > (peak_loc2-50)):\n",
    "                            secondpeak_xtac = subsegstart + subsegwidth/2\n",
    "                            n_2nd_returns += 1\n",
    "                            prominences.append(prominence_secondpeak)\n",
    "                            elev_2ndpeaks.append(secondpeak_h)\n",
    "                            subpeaks_xatc.append(secondpeak_xtac)\n",
    "\n",
    "        ratio_2nd_returns = n_2nd_returns/n_subsegs\n",
    "        df_mframe['ratio_2nd_returns'].loc[mframe] = ratio_2nd_returns\n",
    "        quality_secondreturns = np.sum(prominences) / n_subsegs\n",
    "\n",
    "        min_quality = (0.2 + (ratio_2nd_returns-0.2)*(0.1/0.8))\n",
    "        quality_summary = 0.0\n",
    "        quality_pass = 'No'\n",
    "        if (ratio_2nd_returns >= 0.2) & (quality_secondreturns > min_quality):\n",
    "            quality_pass = 'Yes'\n",
    "            quality_summary = ratio_2nd_returns*(quality_secondreturns-min_quality)/(ratio_2nd_returns-min_quality)\n",
    "            df_mframe['lake_qual_pass'].loc[mframe] = True\n",
    "            df_mframe['quality_summary'].loc[mframe] = quality_summary\n",
    "            df_mframe['h_2nd_returns'].loc[mframe] = elev_2ndpeaks\n",
    "            df_mframe['xatc_2nd_returns'].loc[mframe] = subpeaks_xatc\n",
    "            df_mframe['proms_2nd_returns'].loc[mframe] = prominences\n",
    "        # if (percent_2d_returns >= 30) & (quality_secondreturns > 0.4):\n",
    "        flatstring = 'Yes' if df_mframe['is_flat'].loc[mframe] else 'No'\n",
    "        \n",
    "        if print_results:\n",
    "            print('mframe %d: Elevation: %.2fm. Flat: %s. 2nd returns %3d%%. Quality summary: %.2f. Passes check: %s.' % \\\n",
    "                (mframe, peak_loc2, flatstring, np.round(ratio_2nd_returns*100), quality_summary, quality_pass))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e09814-48f1-4e10-a36d-f629cb41bf4d",
   "metadata": {},
   "source": [
    "# calculate the densities and possible second returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ee303-8034-47f6-85fc-7ce04d8a3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_densities_and_2nd_peaks(df, df_mframe, df_mframe_selected, print_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86aee6-c41f-4958-a844-f131c31cbdc6",
   "metadata": {},
   "source": [
    "# check for densities and possible second returns two major frames around where high-quality lakes were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393dc1d-05c0-4267-9dcd-3a59c761ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# check for densities and possible second returns two mframes around where passing quality lakes were detected\n",
    "num_additional = 1\n",
    "count = 0\n",
    "while (num_additional > 0) & (count<20):\n",
    "    count+=1\n",
    "    selected = df_mframe[df_mframe['lake_qual_pass']].index \n",
    "    lst1 = np.unique(list(selected-2) + list(selected-1) + list(selected+1) + list(selected+2))\n",
    "    lst2 = df_mframe.index\n",
    "    inter = list(set(lst1) & set(lst2))\n",
    "    df_include_surrounding = df_mframe.loc[inter]\n",
    "    # df_include_surrounding = df_mframe.loc[np.unique(list(selected-2) + list(selected-1) + list(selected+1) + list(selected+2))]\n",
    "    df_additional = df_include_surrounding[~df_include_surrounding['has_densities']]\n",
    "    num_additional = len(df_additional)\n",
    "    if num_additional > 0:\n",
    "        get_densities_and_2nd_peaks(df, df_mframe, df_additional, print_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2005c3-4413-49bf-b7f7-bc7f184642d2",
   "metadata": {},
   "source": [
    "# merge detected lakes iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860985b-e037-4688-88a9-7c21ac8bf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_dist_mframes = 7\n",
    "max_dist_elev = 0.1\n",
    "df_mframe.sort_index(inplace=True)\n",
    "start_mframe = np.array(df_mframe.index[df_mframe['lake_qual_pass']],dtype=int)\n",
    "stop_mframe = np.array(df_mframe.index[df_mframe['lake_qual_pass']],dtype=int)\n",
    "surf_elevs = np.array(df_mframe['peak'][df_mframe['lake_qual_pass']])\n",
    "n_lakes = len(surf_elevs)\n",
    "n_lakes_old = n_lakes + 1\n",
    "iteration = 0\n",
    "\n",
    "debug = False\n",
    "\n",
    "while n_lakes < n_lakes_old:\n",
    "    \n",
    "    start_mframe_old = start_mframe\n",
    "    stop_mframe_old = stop_mframe\n",
    "    surf_elevs_old = surf_elevs\n",
    "    n_lakes_old = n_lakes\n",
    "    start_mframe = []\n",
    "    stop_mframe = []\n",
    "    surf_elevs = []\n",
    "    \n",
    "    last_merged = True\n",
    "    for i in range(n_lakes - 1):\n",
    "        is_closeby = ((start_mframe_old[i + 1] - stop_mframe_old[i]) <= max_dist_mframes)\n",
    "        is_at_same_elevation = (np.abs(surf_elevs_old[i + 1] - surf_elevs_old[i]) < max_dist_elev)\n",
    "        if debug: \n",
    "            print('%10d %10d diff: %4d, close: %5s, %7.2f %7.2f diff: %7.2f, same: %5s' % \\\n",
    "                 (stop_mframe_old[i], start_mframe_old[i + 1], start_mframe_old[i + 1] - stop_mframe_old[i],\n",
    "                  is_closeby, surf_elevs_old[i], surf_elevs_old[i + 1], np.abs(surf_elevs_old[i + 1] - surf_elevs_old[i]),\n",
    "                  is_at_same_elevation), end=' ')\n",
    "        \n",
    "        # if merging\n",
    "        if (is_closeby & is_at_same_elevation):\n",
    "            \n",
    "            if last_merged == False:\n",
    "                start_mframe[-1] = start_mframe_old[i]\n",
    "                stop_mframe[-1] = stop_mframe_old[i + 1]\n",
    "                surf_elevs[-1] = (surf_elevs_old[i] + surf_elevs_old[i+1]) / 2\n",
    "            else: \n",
    "                start_mframe.append(start_mframe_old[i])\n",
    "                stop_mframe.append(stop_mframe_old[i + 1])\n",
    "                surf_elevs.append((surf_elevs_old[i] + surf_elevs_old[i+1]) / 2)\n",
    "            if debug:\n",
    "                print('--> merge')\n",
    "            last_merged = True\n",
    "        \n",
    "        # if keeping \n",
    "        elif (iteration > 0):\n",
    "            \n",
    "            if i==0:\n",
    "                start_mframe.append(start_mframe_old[i])\n",
    "                stop_mframe.append(stop_mframe_old[i])\n",
    "                surf_elevs.append(surf_elevs_old[i])\n",
    "            \n",
    "            start_mframe.append(start_mframe_old[i+1])\n",
    "            stop_mframe.append(stop_mframe_old[i+1])\n",
    "            surf_elevs.append(surf_elevs_old[i+1])\n",
    "            if debug:\n",
    "                print('--> keep')\n",
    "            last_merged = False\n",
    "    \n",
    "    iteration += 1\n",
    "    n_lakes = len(surf_elevs)\n",
    "    print('iteration %3d, number of lakes: %4d' % (iteration, n_lakes))\n",
    "    \n",
    "df_extracted_lakes = pd.DataFrame({'mframe_start': start_mframe, 'mframe_end': stop_mframe, 'surf_elev': surf_elevs})\n",
    "df_extracted_lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b1d7c-b44d-40e0-a304-d67314af8024",
   "metadata": {},
   "source": [
    "# check for adjacient peaks at surface elevation to extend lake further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca77d6-4e0b-49e2-a988-6e39efa54764",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_check = 3\n",
    "elev_tol = 0.2\n",
    "for i in range(len(df_extracted_lakes)):\n",
    "    \n",
    "    thislake = df_extracted_lakes.iloc[i]\n",
    "    thiselev = thislake['surf_elev']\n",
    "    \n",
    "    # check for extending before\n",
    "    extent_before = thislake['mframe_start']\n",
    "    check_before = extent_before - 1\n",
    "    left_to_check = n_check\n",
    "    while left_to_check > 0:\n",
    "        # print('check!')\n",
    "        if np.abs(df_mframe['peak'].loc[check_before] - thiselev) < elev_tol:\n",
    "            extent_before = check_before\n",
    "            left_to_check = n_check\n",
    "            print('extend before')\n",
    "        else:\n",
    "            left_to_check -= 1\n",
    "            \n",
    "        check_before -= 1\n",
    "        \n",
    "    df_extracted_lakes['mframe_start'].iloc[i] = extent_before\n",
    "    \n",
    "    # check for extending after\n",
    "    extent_after = thislake['mframe_end']\n",
    "    check_after = extent_after + 1\n",
    "    left_to_check = n_check\n",
    "    while left_to_check > 0:\n",
    "        # print('check!')\n",
    "        if np.abs(df_mframe['peak'].loc[check_after] - thiselev) < elev_tol:\n",
    "            extent_after = check_after\n",
    "            left_to_check = n_check\n",
    "            print('extend after')\n",
    "        else:\n",
    "            left_to_check -= 1\n",
    "            \n",
    "        check_after += 1\n",
    "        \n",
    "    df_extracted_lakes['mframe_end'].iloc[i] = extent_after\n",
    "\n",
    "# expand each lake by two major frames\n",
    "df_extracted_lakes['mframe_start'] -= 2\n",
    "df_extracted_lakes['mframe_end'] += 2\n",
    "df_extracted_lakes\n",
    "    # for mf in check_before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1174ac-27be-40e9-94a5-00fabb3639a1",
   "metadata": {},
   "source": [
    "# run the SNR algorithm and second peak finding for the remaining major frames within lake boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c15c14-2b26-4578-b346-2b9d00d9319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_to_calculate_densities = []\n",
    "\n",
    "for i in range(len(df_extracted_lakes)):\n",
    "    \n",
    "    thislake = df_extracted_lakes.iloc[i]\n",
    "    extent_start = thislake['mframe_start']\n",
    "    extent_end = thislake['mframe_end']\n",
    "    \n",
    "    dfs_to_calculate_densities.append(df_mframe[(df_mframe.index >= extent_start) & (df_mframe.index <= extent_end)])\n",
    "    \n",
    "df_to_calculate_densities = pd.concat(dfs_to_calculate_densities)\n",
    "df_to_calculate_densities = df_to_calculate_densities[~df_to_calculate_densities['has_densities']]\n",
    "\n",
    "get_densities_and_2nd_peaks(df, df_mframe, df_to_calculate_densities, print_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793027c-8cc4-4bba-9b23-c0996cab72df",
   "metadata": {},
   "source": [
    "# plot for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cce58-faeb-4341-919c-bb6195da49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "for i in range(len(df_extracted_lakes)):\n",
    "# for i in [2]:\n",
    "    \n",
    "    thislake = df_extracted_lakes.iloc[i]\n",
    "    thiselev = thislake['surf_elev']\n",
    "    extent_start = thislake['mframe_start']\n",
    "    extent_end = thislake['mframe_end']\n",
    "    \n",
    "    # subset the dataframes to the current lake extent\n",
    "    df_lake = df[(df['mframe'] >= extent_start) & (df['mframe'] <= extent_end)]\n",
    "    df_mframe_lake = df_mframe[(df_mframe.index >= extent_start) & (df_mframe.index <= extent_end)]\n",
    "    h_2nds = [v for l in list(df_mframe_lake['h_2nd_returns']) for v in l]\n",
    "    xatc_2nds = [v for l in list(df_mframe_lake['xatc_2nd_returns']) for v in l]\n",
    "    prom_2nds = [v for l in list(df_mframe_lake['proms_2nd_returns']) for v in l]\n",
    "    \n",
    "    # get statistics\n",
    "    # average mframe quality summary excluding the two mframes for buffer on each side\n",
    "    lake_quality = np.sum(df_mframe_lake['quality_summary']) / (len(df_mframe_lake) - 4)\n",
    "    lake_time = convert_time_to_string(df_mframe_lake['dt'].mean())\n",
    "    lake_lat = df_mframe_lake['lat'].mean()\n",
    "    lake_lat_min = df_mframe_lake['lat'].min()\n",
    "    lake_lat_max = df_mframe_lake['lat'].max()\n",
    "    lake_lat_str = '%.5fN'%(lake_lat) if lake_lat>=0 else '%.5fS'%(-lake_lat)\n",
    "    lake_lon = df_mframe_lake['lon'].mean()\n",
    "    lake_lon_min = df_mframe_lake['lon'].min()\n",
    "    lake_lon_max = df_mframe_lake['lon'].max()\n",
    "    lake_lon_str = '%.5fE'%(lake_lon) if lake_lon>=0 else '%.5fW'%(-lake_lon)\n",
    "    lake_gtx = lakes[thisLake].gtx\n",
    "    lake_track = ancillary['rgt']\n",
    "    lake_cycle = ancillary['cycle_number']\n",
    "    lake_beam_strength = ancillary['gtx_strength_dict'][lake_gtx]\n",
    "    lake_minh = np.min((df_mframe_lake['peak'].min(), np.min(h_2nds)))\n",
    "    h_range = thiselev - lake_minh\n",
    "    lake_maxh = np.min((df_mframe_lake['peak'].max(), thiselev+5*h_range))\n",
    "    buffer_top = np.max((0.2*h_range, 1.0))\n",
    "    buffer_bottom = np.max((0.3*h_range, 2.0))\n",
    "    lake_minh_plot = lake_minh - buffer_bottom\n",
    "    lake_maxh_plot = lake_maxh + buffer_top\n",
    "    \n",
    "    fig = plt.figure(figsize=[9, 5], dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "       \n",
    "    ax.scatter(df_lake.xatc-df_lake.xatc.min(), df_lake.h, s=6, c='k', alpha=0.05, edgecolors='none')\n",
    "    scatt = ax.scatter(df_lake.xatc-df_lake.xatc.min(), df_lake.h, s=3, c=df_lake.snr, alpha=1, edgecolors='none',\n",
    "                       cmap=cmc.lajolla,vmin=0,vmax=1)\n",
    "                        \n",
    "    # plot surface elevation\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ax.plot([xmin, xmax], [thiselev, thiselev], 'g-', lw=0.5)\n",
    "    \n",
    "    # plot mframe bounds\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    mframe_bounds_xatc = list(df_mframe_lake['xatc_min']) + [df_mframe_lake['xatc_max'].iloc[-1]]\n",
    "    for xmframe in mframe_bounds_xatc:\n",
    "        ax.plot([xmframe-df_lake.xatc.min(), xmframe-df_lake.xatc.min()], [ymin, ymax], 'k-', lw=0.5)\n",
    "\n",
    "    dfpass = df_mframe_lake[df_mframe_lake['lake_qual_pass']]\n",
    "    dfnopass = df_mframe_lake[~df_mframe_lake['lake_qual_pass']]\n",
    "    ax.plot(dfpass.xatc-df_lake.xatc.min(), dfpass.peak, marker='o', mfc='g', mec='g', linestyle = 'None', ms=5)\n",
    "    ax.plot(dfnopass.xatc-df_lake.xatc.min(), dfnopass.peak, marker='o', mfc='none', mec='r', linestyle = 'None', ms=3)\n",
    "\n",
    "    for i,prom in enumerate(prom_2nds):\n",
    "        ax.plot(xatc_2nds[i]-df_lake.xatc.min(), h_2nds[i], marker='o', mfc='none', mec='b', linestyle = 'None', ms=prom*4)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='4%', pad=0.05)\n",
    "    fig.colorbar(scatt, cax=cax, orientation='vertical')\n",
    "    \n",
    "    ax.set_ylim((lake_minh_plot, lake_maxh_plot))\n",
    "    ax.set_xlim((0.0, df_mframe_lake['xatc_max'].iloc[-1] - df_lake.xatc.min()))\n",
    "    \n",
    "    ax.set_title('Lake at (%s, %s) on %s\\nICESat-2 track %d %s (%s), cycle %d [lake quality: %.2f]' % \\\n",
    "                 (lake_lat_str, lake_lon_str, lake_time, lake_track, lake_gtx,lake_beam_strength, lake_cycle, lake_quality))\n",
    "    ax.set_ylabel('elevation above geoid [m]')\n",
    "    ax.set_xlabel('along-track distance [m]')\n",
    "    \n",
    "    # ax.text(0.99,0.01,'lat')\n",
    "    url = 'https://openaltimetry.org/data/api/icesat2/atlXX?'\n",
    "    url += 'date={date}&minx={minx}&miny={miny}&maxx={maxx}&maxy={maxy}&trackId={track}&beamName={beam}'.format(\n",
    "            date=lake_time[:10],minx=lake_lon_min,miny=lake_lat_min,maxx=lake_lon_max,maxy=lake_lat_max,track=lake_track,beam=lake_gtx)\n",
    "    url += '&outputFormat=json&client=jupyter'\n",
    "    print('oaurl = \\'%s\\'' % url)\n",
    "    \n",
    "    # save figure\n",
    "    fig_dir = 'figs/lakes_found_test/'\n",
    "    if not os.path.exists(fig_dir): os.makedirs(fig_dir)\n",
    "    epoch = df_mframe_lake['dt'].mean() + datetime.datetime.timestamp(datetime.datetime(2018,1,1))\n",
    "    dateid = datetime.datetime.fromtimestamp(epoch).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    latid = '%dN'%(int(np.round(lake_lat*1e5))) if lake_lat>=0 else '%dS'%(-int(np.round(lake_lat*1e5)))\n",
    "    lonid = '%dE'%(int(np.round(lake_lon*1e5))) if lake_lon>=0 else '%dS'%(-int(np.round(lake_lon*1e5)))\n",
    "    figname = fig_dir + 'lake_found_%s%s_%d_%s_%s-%s.jpg' % (lake_track, lake_gtx, lake_cycle, dateid, latid, lonid)\n",
    "    plt.savefig(figname, dpi=600, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830327de-34aa-4019-9dd9-152801af6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r figs/lakes_found_test.zip figs/lakes_found_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708d858-6bac-44a6-b999-2274c9a9130a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172d98e-5228-4128-94c6-5d8a01a58d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26289482-cafd-49c0-821a-aaddac935db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepyx-env",
   "language": "python",
   "name": "icepyx-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c97d73-f6fa-4f2c-a8b3-2aaad23a4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib widget\n",
    "import h5py\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import matplotlib.pylab as plt\n",
    "import cmocean\n",
    "import cmocean.cm as cmo\n",
    "from cmcrameri import cm as cmc\n",
    "import seaborn as sns\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from warnings import warn\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import binned_statistic\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import hdbscan\n",
    "from sklearn.neighbors import KDTree\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import datetime\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8794d1-c824-4a9e-a4b4-a697e8f75299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of processed ATL03 granules: 1\n",
      "IS2data/processed_ATL03_20210715182907_03381203_005_01.h5\n"
     ]
    }
   ],
   "source": [
    "# datapath = 'Outputs_ATL03_test/'\n",
    "datapath = 'IS2data/'\n",
    "filelist = [datapath+f for f in listdir(datapath) if isfile(join(datapath, f)) & ('.h5' in f)]\n",
    "print('number of processed ATL03 granules: ' + str(len(filelist)))\n",
    "for f in filelist: print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40602965-7bfe-4da7-9717-39512ec1f215",
   "metadata": {},
   "source": [
    "# a function for reading in the relevant variables from ATL03 granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95dff47a-5c58-4234-9949-155f70fd7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_atl03(filename, geoid_h=True):\n",
    "    # reads in the necessary variables at photon rate from an .h5 file in a dataframe:\n",
    "    # - lat: latitude\n",
    "    # - xatc: along-track distance from the equator crossing\n",
    "    # - h: elevation above the geoid (or above reference ellipsoid if geoid_h is set to False)\n",
    "    \n",
    "    # open file\n",
    "    f = h5py.File(filename, 'r')\n",
    "    \n",
    "    # make dictionary for beam data to be stored in\n",
    "    dfs = {}\n",
    "    dfs_bckgrd = {}\n",
    "    beamlist = [x for x in list(f.keys()) if 'gt' in x]\n",
    "    \n",
    "    conf_landice = 3 # index for the land ice confidence\n",
    "    \n",
    "    orient = f['orbit_info']['sc_orient'][0]\n",
    "    def orient_string(sc_orient):\n",
    "        if sc_orient == 0:\n",
    "            return 'backward'\n",
    "        elif sc_orient == 1:\n",
    "            return 'forward'\n",
    "        elif sc_orient == 2:\n",
    "            return 'transition'\n",
    "        else:\n",
    "            return 'error'\n",
    "        \n",
    "    orient_str = orient_string(orient)\n",
    "    gtl = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "    beam_strength_dict = {k:['weak','strong'][k%2] for k in np.arange(1,7,1)}\n",
    "    if orient_str == 'forward':\n",
    "        bl = np.arange(6,0,-1)\n",
    "        gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "        gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "    elif orient_str == 'backward':\n",
    "        bl = np.arange(1,7,1)\n",
    "        gtx_beam_dict = {k:v for (k,v) in zip(gtl,bl)}\n",
    "        gtx_strength_dict = {k:beam_strength_dict[gtx_beam_dict[k]] for k in gtl}\n",
    "    else:\n",
    "        gtx_beam_dict = {k:'undefined' for k in gtl}\n",
    "        gtx_strength_dict = {k:'undefined' for k in gtl}\n",
    "        \n",
    "\n",
    "    ancillary = {'atlas_sdp_gps_epoch': f['ancillary_data']['atlas_sdp_gps_epoch'][0],\n",
    "                 'rgt': f['orbit_info']['rgt'][0],\n",
    "                 'cycle_number': f['orbit_info']['cycle_number'][0],\n",
    "                 'sc_orient': orient_str,\n",
    "                 'gtx_beam_dict': gtx_beam_dict,\n",
    "                 'gtx_strength_dict': gtx_strength_dict}\n",
    "    \n",
    "    # loop through all beams\n",
    "    for beam in beamlist:\n",
    "        try:\n",
    "            #### get photon-level data\n",
    "            df = pd.DataFrame({'lat': np.array(f[beam]['heights']['lat_ph']),\n",
    "                               'lon': np.array(f[beam]['heights']['lon_ph']),\n",
    "                               'h': np.array(f[beam]['heights']['h_ph']),\n",
    "                               'dt': np.array(f[beam]['heights']['delta_time']),\n",
    "                               # 'conf': np.array(f[beam]['heights']['signal_conf_ph'][:,conf_landice]),\n",
    "                               # not using ATL03 confidences here\n",
    "                               'mframe': np.array(f[beam]['heights']['pce_mframe_cnt']),\n",
    "                               'qual': np.array(f[beam]['heights']['quality_ph'])}) \n",
    "                               # 0=nominal,1=afterpulse,2=impulse_response_effect,3=tep\n",
    "\n",
    "            df_bckgrd = pd.DataFrame({'pce_mframe_cnt': np.array(f[beam]['bckgrd_atlas']['pce_mframe_cnt']),\n",
    "                                      'bckgrd_counts': np.array(f[beam]['bckgrd_atlas']['bckgrd_counts']),\n",
    "                                      'bckgrd_int_height': np.array(f[beam]['bckgrd_atlas']['bckgrd_int_height']),\n",
    "                                      'delta_time': np.array(f[beam]['bckgrd_atlas']['delta_time'])})\n",
    "\n",
    "            #### calculate along-track distances [meters from the equator crossing] from segment-level data\n",
    "            df['xatc'] = np.full_like(df.lat, fill_value=np.nan)\n",
    "            ph_index_beg = np.int32(f[beam]['geolocation']['ph_index_beg']) - 1\n",
    "            segment_dist_x = np.array(f[beam]['geolocation']['segment_dist_x'])\n",
    "            segment_length = np.array(f[beam]['geolocation']['segment_length'])\n",
    "            valid = ph_index_beg>=0 # need to delete values where there's no photons in the segment (-1 value)\n",
    "\n",
    "            df.loc[ph_index_beg[valid], 'xatc'] = segment_dist_x[valid]\n",
    "            df.xatc.fillna(method='ffill',inplace=True)\n",
    "            df.xatc += np.array(f[beam]['heights']['dist_ph_along'])\n",
    "\n",
    "            #### now we can filter out TEP (we don't do IRF / afterpulses because it seems to not be very good...)\n",
    "            df.query('qual < 3',inplace=True) \n",
    "            # df.drop(columns=['qual'], inplace=True)\n",
    "\n",
    "            #### sort by along-track distance (for interpolation to work smoothly)\n",
    "            df.sort_values(by='xatc',inplace=True)\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            if geoid_h:\n",
    "                #### interpolate geoid to photon level using along-track distance, and add to elevation\n",
    "                geophys_geoid = np.array(f[beam]['geophys_corr']['geoid'])\n",
    "                print(np.sum(np.isnan(df.xatc)))\n",
    "                print(np.sum(np.isnan(geophys_geoid)))\n",
    "                print(np.sum(np.isnan(segment_dist_x+0.5*segment_length)))\n",
    "                geoid = np.interp(np.array(df.xatc), segment_dist_x+0.5*segment_length, geophys_geoid)\n",
    "                df['h'] = df.h - geoid\n",
    "                df['geoid'] = geoid\n",
    "\n",
    "            #### save to list of dataframes\n",
    "            dfs[beam] = df\n",
    "            dfs_bckgrd[beam] = df_bckgrd\n",
    "            print(beam, end=' ')\n",
    "        except Exception as e:\n",
    "            print('Error for {f:s} on {b:s} ... skipping:'.format(f=filename, b=beam), e)\n",
    "    return dfs, dfs_bckgrd, ancillary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef5c43-dcfa-4bda-9385-a7ba2cc77803",
   "metadata": {},
   "source": [
    "# read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2448cac-ca65-44e5-860f-7a8ece85e773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file    1 /    1: ATL03_20210715182907_03381203_005_01.h5 >>>  0\n",
      "0\n",
      "0\n",
      "gt1l 0\n",
      "0\n",
      "0\n",
      "gt1r 0\n",
      "0\n",
      "0\n",
      "gt2l 0\n",
      "0\n",
      "0\n",
      "gt2r 0\n",
      "0\n",
      "0\n",
      "gt3l 0\n",
      "0\n",
      "0\n",
      "gt3r \n"
     ]
    }
   ],
   "source": [
    "# for ifile, file in enumerate(filelist):\n",
    "for ifile, file in enumerate([filelist[0]]):\n",
    "    print('Reading file {i:4d} / {n:4d}: {fn:s} >>> '.format(i=ifile+1, n=len(filelist), fn=file[file.find('ATL03_'):]), end = ' ')\n",
    "    photon_data, bckgrd_data, ancillary = read_atl03(file)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "501c005f-8b7e-428e-ac1c-3e1348767dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atlas_sdp_gps_epoch': 1198800018.0,\n",
       " 'rgt': 338,\n",
       " 'cycle_number': 12,\n",
       " 'sc_orient': 'forward',\n",
       " 'gtx_beam_dict': {'gt1l': 6,\n",
       "  'gt1r': 5,\n",
       "  'gt2l': 4,\n",
       "  'gt2r': 3,\n",
       "  'gt3l': 2,\n",
       "  'gt3r': 1},\n",
       " 'gtx_strength_dict': {'gt1l': 'weak',\n",
       "  'gt1r': 'strong',\n",
       "  'gt2l': 'weak',\n",
       "  'gt2r': 'strong',\n",
       "  'gt3l': 'weak',\n",
       "  'gt3r': 'strong'}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancillary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eedf6878-30a8-4eb4-8f90-9516d897905f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int8')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancillary['cycle_number'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09dc85d-2ac9-4221-b483-37943d9c0654",
   "metadata": {},
   "source": [
    "# a list of known lakes for development / checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17765897-87cb-4082-8863-a98bda9deb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnownLake:\n",
    "    def __init__(self, fn, gtx, latlims):\n",
    "        self.fn = fn\n",
    "        self.gtx = gtx\n",
    "        self.latlims = latlims\n",
    "    \n",
    "    def getData(self, dfs):\n",
    "        df = dfs[self.gtx]\n",
    "        return df[(df.lat > self.latlims[0]) & (df.lat < self.latlims[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90ca16a-f8bc-40d5-8360-c7927b543e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = file # to keep this out of the loop for now\n",
    "lakes = {1: KnownLake(fn, 'gt1l', [67.433, 67.463]),\n",
    "         2: KnownLake(fn, 'gt1l', [68.6, 68.65]),\n",
    "         3: KnownLake(fn, 'gt1l', [68.535, 68.595]),\n",
    "         4: KnownLake(fn, 'gt1l', [68.353, 68.37]),\n",
    "         5: KnownLake(fn, 'gt1l', [68.02, 68.09]),\n",
    "         6: KnownLake(fn, 'gt1l', [67.95, 67.98]),\n",
    "         7: KnownLake(fn, 'gt1l', [67.675, 67.6975]),\n",
    "         8: KnownLake(fn, 'gt1l', [67.37, 67.4]),\n",
    "         9: KnownLake(fn, 'gt1l', [67.02, 67.055]),\n",
    "         10: KnownLake(fn, 'gt2l', [68.716, 68.726]),\n",
    "         11: KnownLake(fn, 'gt2l', [68.5, 68.547]),\n",
    "         12: KnownLake(fn, 'gt2l', [68.33, 68.375]),\n",
    "         13: KnownLake(fn, 'gt2l', [68.2425, 68.27]),\n",
    "         14: KnownLake(fn, 'gt2l', [67.9023, 67.938]),\n",
    "         15: KnownLake(fn, 'gt2l', [67.617, 67.671]),\n",
    "         16: KnownLake(fn, 'gt2l', [67.3827, 67.4151]),\n",
    "         17: KnownLake(fn, 'gt2l', [67.0057, 67.0408]),\n",
    "         18: KnownLake(fn, 'gt2r', [67.3826, 67.4171]),\n",
    "         19: KnownLake(fn, 'gt3l', [68.5505, 68.5826]),\n",
    "         20: KnownLake(fn, 'gt3l', [68.4309, 68.4508]),\n",
    "         21: KnownLake(fn, 'gt3l', [67.07, 67.1]),\n",
    "         22: KnownLake(fn, 'gt3l', [66.9306, 67.0137]),\n",
    "         23: KnownLake(fn, 'gt2l', [-71.6687, -71.6166]),\n",
    "         24: KnownLake(fn, 'gt2l', [-71.8903, -71.8587]),\n",
    "         25: KnownLake(fn, 'gt2l', [-72.9505, -72.8595]),\n",
    "         26: KnownLake(fn, 'gt1l', [-72.6468, -72.5127]),\n",
    "         27: KnownLake(fn, 'gt3l', [-73.2389, -73.2073]),\n",
    "         28: KnownLake(fn, 'gt1l', [-70.85, -70.745]),\n",
    "         29: KnownLake(fn, 'gt1l', [-71.513, -71.445]),\n",
    "         30: KnownLake(fn, 'gt1l', [-72.94, -72.85]),\n",
    "         31: KnownLake(fn, 'gt3l', [-72.8602, -72.6714]),\n",
    "         32: KnownLake(fn, 'gt3l', [-72.08, -71.99]),\n",
    "         33: KnownLake(fn, 'gt1l', [66, 69]),\n",
    "         34: KnownLake(fn, 'gt2l', [66, 69]),\n",
    "         35: KnownLake(fn, 'gt3l', [66, 69]),\n",
    "         36: KnownLake(fn, 'gt1r', [66, 69]),\n",
    "         37: KnownLake(fn, 'gt2r', [66, 69]),\n",
    "         38: KnownLake(fn, 'gt3r', [66, 69]),\n",
    "         39: KnownLake(fn, 'gt1l', [-74, -70.5]),\n",
    "         40: KnownLake(fn, 'gt2l', [-74, -70.5]),\n",
    "         41: KnownLake(fn, 'gt3l', [-74, -70.5]),\n",
    "         42: KnownLake(fn, 'gt1r', [-74, -70.5]),\n",
    "         43: KnownLake(fn, 'gt2r', [-74, -70.5]),\n",
    "         44: KnownLake(fn, 'gt3r', [-74, -70.5]),\n",
    "         45: KnownLake(fn, 'gt1l', [63, 72]),\n",
    "         46: KnownLake(fn, 'gt1l', [-90, 90]),\n",
    "         47: KnownLake(fn, 'gt2l', [-90, 90]),\n",
    "         48: KnownLake(fn, 'gt3l', [-90, 90]),\n",
    "         49: KnownLake(fn, 'gt1r', [-90, 90]),\n",
    "         50: KnownLake(fn, 'gt2r', [-90, 90]),\n",
    "         51: KnownLake(fn, 'gt3r', [-90, 90]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e68c9-84a8-4e47-9fe2-0ab4a93763e5",
   "metadata": {},
   "source": [
    "# make a DataFrame for a given granule/gtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d268395-1ee3-488a-8433-7c5a0f56db98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 871 ms, sys: 488 ms, total: 1.36 s\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thisLake = 46\n",
    "df = lakes[thisLake].getData(photon_data)\n",
    "df['snr'] = 0.0\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992a9e8-53b9-48cf-92ec-7410aaec55cc",
   "metadata": {},
   "source": [
    "# create a dataframe that aggregates info at the major frame level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "337028e7-863b-4447-9b76-9e1c128a448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 967 ms, sys: 452 ms, total: 1.42 s\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mframe_group = df.groupby('mframe')\n",
    "df_mframe = mframe_group[['lat','lon', 'xatc', 'dt']].mean()\n",
    "df_mframe.drop(df_mframe.head(1).index,inplace=True)\n",
    "df_mframe.drop(df_mframe.tail(1).index,inplace=True)\n",
    "def convert_time_to_string(dt):\n",
    "    epoch = dt + datetime.datetime.timestamp(datetime.datetime(2018,1,1))\n",
    "    return datetime.datetime.fromtimestamp(epoch).strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "df_mframe['time'] = df_mframe['dt'].map(convert_time_to_string)\n",
    "df_mframe['xatc_min'] = mframe_group['xatc'].min()\n",
    "df_mframe['xatc_max'] = mframe_group['xatc'].max()\n",
    "df_mframe['n_phot'] = mframe_group['h'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180654c-1076-4153-9e8a-2aaff776517a",
   "metadata": {},
   "source": [
    "# check for flat surfaces in each major frame of the granule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a71f11-a836-4d3f-9dcf-dce70eed6253",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Maximum allowed size exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/icepyx-env/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m   5096\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5098\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5100\u001b[0m         \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_attributes_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/icepyx-env/lib/python3.9/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mfind_flat_lake_surfaces\u001b[0;34m(mframe, df, bin_height_coarse, bin_height_fine, smoothing_histogram, buffer, width_surf, width_buff, rel_dens_upper_thresh, rel_dens_lower_thresh)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Maximum allowed size exceeded"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def find_flat_lake_surfaces(mframe, df=df, bin_height_coarse=1.0, bin_height_fine=0.02, smoothing_histogram=0.2, buffer=4.0,\n",
    "                            width_surf=0.1, width_buff=0.3, rel_dens_upper_thresh=0.3, rel_dens_lower_thresh=0.3):\n",
    "    selector_segment = (df.mframe == mframe)\n",
    "    dfseg = df[selector_segment]\n",
    "    \n",
    "    # find main broad peak\n",
    "    bins_coarse1 = np.arange(start=dfseg.h.min(), stop=dfseg.h.max(), step=bin_height_coarse)\n",
    "    hist_mid1 = bins_coarse1[:-1] + 0.5 * bin_height_coarse\n",
    "    peak_loc1 = hist_mid1[np.argmax(np.histogram(dfseg.h, bins=bins_coarse1)[0])]\n",
    "    \n",
    "    # decrease bin width and find finer peak\n",
    "    bins_coarse2 = np.arange(start=peak_loc1-buffer, stop=peak_loc1+buffer, step=bin_height_fine)\n",
    "    hist_mid2 = bins_coarse2[:-1] + 0.5 * bin_height_fine\n",
    "    hist = np.histogram(dfseg.h, bins=bins_coarse2)\n",
    "    window_size = int(smoothing_histogram/bin_height_fine)\n",
    "    hist_vals = hist[0] / np.max(hist[0])\n",
    "    hist_vals_smoothed = np.array(pd.Series(hist_vals).rolling(window_size,center=True,min_periods=1).mean())\n",
    "    peak_loc2 = hist_mid2[np.argmax(hist_vals_smoothed)]\n",
    "    \n",
    "    # calculate relative photon densities\n",
    "    peak_upper = peak_loc2 + width_surf\n",
    "    peak_lower = peak_loc2 - width_surf\n",
    "    above_upper = peak_upper + width_buff\n",
    "    below_lower = peak_lower - width_buff\n",
    "    sum_peak = np.sum((dfseg.h > peak_lower) & (dfseg.h < peak_upper))\n",
    "    sum_above = np.sum((dfseg.h > peak_upper) & (dfseg.h < above_upper))\n",
    "    sum_below = np.sum((dfseg.h > below_lower) & (dfseg.h < peak_lower))\n",
    "    rel_dens_upper = (sum_above / width_buff) / (sum_peak / (width_surf*2))\n",
    "    rel_dens_lower = (sum_below / width_buff) / (sum_peak / (width_surf*2))\n",
    "    \n",
    "    # check for flat surface, if found calculate SNR and look for bottom return\n",
    "    is_flat_like_lake = (rel_dens_upper < rel_dens_upper_thresh) & (rel_dens_lower < rel_dens_lower_thresh)\n",
    "    \n",
    "    return peak_loc2, is_flat_like_lake\n",
    "\n",
    "# get all the flat segments and select\n",
    "df_mframe['peak'], df_mframe['is_flat'] = list(zip(*df_mframe.index.map(find_flat_lake_surfaces)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19ebf7-677b-4a01-90b4-d503984beabf",
   "metadata": {},
   "source": [
    "# a function for calculating SNR and checking for second returns\n",
    "(apply this only to the major frames with flat surfaces for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9997c-7087-4cfc-81ac-649f621839e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize columns to be added\n",
    "df_mframe['lake_qual_pass'] = False\n",
    "df_mframe['has_densities'] = False\n",
    "df_mframe['ratio_2nd_returns'] = 0.0\n",
    "df_mframe['quality_summary'] = 0.0\n",
    "df_mframe['h_2nd_returns'] = [[]] * len(df_mframe)\n",
    "df_mframe['xatc_2nd_returns'] = [[]] * len(df_mframe)\n",
    "df_mframe['proms_2nd_returns'] = [[]] * len(df_mframe)\n",
    "df_mframe_selected = df_mframe[df_mframe['is_flat']]\n",
    "\n",
    "def get_densities_and_2nd_peaks(df, df_mframe, df_mframe_selected, aspect=30, K_phot=10, dh_signal=0.2, n_subsegs=7,\n",
    "                                bin_height_snr=0.1, smoothing_length=1.0, buffer=4.0, print_results=False):\n",
    "    \n",
    "    for mframe in df_mframe_selected.index:\n",
    "        \n",
    "        selector_segment = (df.mframe == mframe)\n",
    "        dfseg = df[selector_segment]\n",
    "\n",
    "        xmin = df_mframe_selected['xatc_min'].loc[mframe]\n",
    "        xmax = df_mframe_selected['xatc_max'].loc[mframe]\n",
    "        nphot = df_mframe_selected['n_phot'].loc[mframe]\n",
    "        peak_loc2 = df_mframe_selected['peak'].loc[mframe]\n",
    "\n",
    "        # the radius in which to look for neighbors\n",
    "        dfseg_nosurface = dfseg[(dfseg.h > (peak_loc2+dh_signal)) | (dfseg.h < (peak_loc2-dh_signal))]\n",
    "        nphot_bckgrd = len(dfseg_nosurface.h)\n",
    "        # radius of a circle in which we expect to find one non-lake-surface signal photon\n",
    "        wid = np.sqrt(((dfseg_nosurface.h.max()-dfseg_nosurface.h.min()-2*dh_signal)*((xmax-xmin)/aspect)/nphot_bckgrd)/np.pi)\n",
    "\n",
    "        # buffer segment for density calculation\n",
    "        selector_buffer = (df.xatc >= (dfseg.xatc.min()-aspect*wid)) & (df.xatc <= (dfseg.xatc.max()+aspect*wid))\n",
    "        dfseg_buffer = df[selector_buffer]\n",
    "        dfseg_buffer.xatc += np.random.uniform(low=-0.35, high=0.35, size=len(dfseg_buffer.xatc))\n",
    "\n",
    "        # normalize xatc to be regularly spaced and scaled by the aspect parameter\n",
    "        xmin_buff = dfseg_buffer.xatc.min()\n",
    "        xmax_buff = dfseg_buffer.xatc.max()\n",
    "        nphot_buff = len(dfseg_buffer.xatc)\n",
    "        xnorm = np.linspace(xmin_buff, xmax_buff, nphot_buff) / aspect\n",
    "\n",
    "        # KD tree query distances\n",
    "        Xn = np.array(np.transpose(np.vstack((xnorm, dfseg_buffer['h']))))\n",
    "        kdt = KDTree(Xn, leaf_size=40)\n",
    "        idx, dist = kdt.query_radius(Xn, r=wid, count_only=False, return_distance=True,sort_results=True)\n",
    "        density = (np.array([np.sum(1-np.abs(x/wid)) if (len(x)<(K_phot+1)) \n",
    "                   else np.sum(1-np.abs(x[:K_phot+1]/wid))\n",
    "                   for x in dist]) - 1) / K_phot\n",
    "\n",
    "        #print(' density calculated')\n",
    "        densities = np.array(density[dfseg_buffer.mframe == mframe])\n",
    "        densities /= np.max(densities)\n",
    "\n",
    "        # add SNR to dataframes\n",
    "        dfseg['snr'] = densities\n",
    "        df.loc[selector_segment, 'snr'] = dfseg['snr']\n",
    "        df_mframe['has_densities'].loc[mframe] = True\n",
    "\n",
    "        # subdivide into segments again to check for second return\n",
    "        subsegs = np.linspace(xmin, xmax, n_subsegs+1) \n",
    "        subsegwidth = subsegs[1] - subsegs[0]\n",
    "\n",
    "        n_2nd_returns = 0\n",
    "        prominences = []\n",
    "        elev_2ndpeaks = []\n",
    "        subpeaks_xatc = []\n",
    "        for subsegstart in subsegs[:-1]:\n",
    "\n",
    "            subsegend = subsegstart + subsegwidth\n",
    "            selector_subseg = ((dfseg.xatc > subsegstart) & (dfseg.xatc < subsegend))\n",
    "            dfsubseg = dfseg[selector_subseg]\n",
    "            \n",
    "            # avoid looking for peaks when there's no / very little data\n",
    "            if len(dfsubseg > 10):\n",
    "\n",
    "                # get the median of the snr values in each bin\n",
    "                bins_subseg_snr = np.arange(start=np.max((dfsubseg.h.min(),peak_loc2-70)), stop=peak_loc2+2*buffer, step=bin_height_snr)\n",
    "                mid_subseg_snr = bins_subseg_snr[:-1] + 0.5 * bin_height_snr\n",
    "                snrstats = binned_statistic(dfsubseg.h, dfsubseg.snr, statistic='median', bins=bins_subseg_snr)\n",
    "                snr_median = snrstats[0]\n",
    "                snr_median[np.isnan(snr_median)] = 0\n",
    "                window_size_sub = int(smoothing_length/bin_height_snr)\n",
    "                snr_vals_smoothed = np.array(pd.Series(snr_median).rolling(window_size_sub,center=True,min_periods=1).mean())\n",
    "                snr_vals_smoothed /= np.max(snr_vals_smoothed)\n",
    "\n",
    "                # take histogram binning values into account, but clip surface peak to second highest peak height\n",
    "                subhist, subhist_edges = np.histogram(dfsubseg.h, bins=bins_subseg_snr)\n",
    "                subhist_nosurface = subhist.copy()\n",
    "                subhist_nosurface[(mid_subseg_snr < (peak_loc2+2*dh_signal)) & (mid_subseg_snr > (peak_loc2-2*dh_signal))] = 0\n",
    "                subhist_nosurface_smoothed = np.array(pd.Series(subhist_nosurface).rolling(window_size_sub,center=True,min_periods=1).mean())\n",
    "                subhist_max = subhist_nosurface_smoothed.max()\n",
    "                subhist_smoothed = np.array(pd.Series(subhist).rolling(window_size_sub,center=True,min_periods=1).mean())\n",
    "                subhist_smoothed = np.clip(subhist_smoothed, 0, subhist_max)\n",
    "                subhist_smoothed /= np.max(subhist_smoothed)\n",
    "\n",
    "                # combine histogram and snr values to find peaks\n",
    "                snr_hist_smoothed = subhist_smoothed * snr_vals_smoothed\n",
    "                peaks, peak_props = find_peaks(snr_hist_smoothed, height=0.1, distance=int(0.5/bin_height_snr), prominence=0.1)\n",
    "\n",
    "                if len(peaks) >= 2: \n",
    "                    idx_surfpeak = np.argmin(peak_loc2 - mid_subseg_snr[peaks])\n",
    "                    peak_props['prominences'][idx_surfpeak] = 0\n",
    "\n",
    "                    # classify as second peak only if prominence is larger 0.2\n",
    "                    prominence_secondpeak = np.max(peak_props['prominences'])\n",
    "                    prominence_threshold = 0.3\n",
    "                    if prominence_secondpeak > prominence_threshold:\n",
    "\n",
    "                        idx_2ndreturn = np.argmax(peak_props['prominences'])\n",
    "                        secondpeak_h = mid_subseg_snr[peaks[idx_2ndreturn]]\n",
    "\n",
    "                        # classify as second peak only if elevation is 0.5m lower than main peak (surface) and higher than 50m below surface\n",
    "                        if (secondpeak_h < (peak_loc2-0.5)) & (secondpeak_h > (peak_loc2-50)):\n",
    "                            secondpeak_xtac = subsegstart + subsegwidth/2\n",
    "                            n_2nd_returns += 1\n",
    "                            prominences.append(prominence_secondpeak)\n",
    "                            elev_2ndpeaks.append(secondpeak_h)\n",
    "                            subpeaks_xatc.append(secondpeak_xtac)\n",
    "\n",
    "        ratio_2nd_returns = n_2nd_returns/n_subsegs\n",
    "        df_mframe['ratio_2nd_returns'].loc[mframe] = ratio_2nd_returns\n",
    "        quality_secondreturns = np.sum(prominences) / n_subsegs\n",
    "\n",
    "        min_quality = (0.2 + (ratio_2nd_returns-0.2)*(0.1/0.8))\n",
    "        quality_summary = 0.0\n",
    "        quality_pass = 'No'\n",
    "        if (ratio_2nd_returns >= 0.2) & (quality_secondreturns > min_quality):\n",
    "            quality_pass = 'Yes'\n",
    "            quality_summary = ratio_2nd_returns*(quality_secondreturns-min_quality)/(ratio_2nd_returns-min_quality)\n",
    "            df_mframe['lake_qual_pass'].loc[mframe] = True\n",
    "            df_mframe['quality_summary'].loc[mframe] = quality_summary\n",
    "            df_mframe['h_2nd_returns'].loc[mframe] = elev_2ndpeaks\n",
    "            df_mframe['xatc_2nd_returns'].loc[mframe] = subpeaks_xatc\n",
    "            df_mframe['proms_2nd_returns'].loc[mframe] = prominences\n",
    "        # if (percent_2d_returns >= 30) & (quality_secondreturns > 0.4):\n",
    "        flatstring = 'Yes' if df_mframe['is_flat'].loc[mframe] else 'No'\n",
    "        \n",
    "        if print_results:\n",
    "            print('mframe %d: Elevation: %.2fm. Flat: %s. 2nd returns %3d%%. Quality summary: %.2f. Passes check: %s.' % \\\n",
    "                (mframe, peak_loc2, flatstring, np.round(ratio_2nd_returns*100), quality_summary, quality_pass))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e09814-48f1-4e10-a36d-f629cb41bf4d",
   "metadata": {},
   "source": [
    "# calculate the densities and possible second returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ee303-8034-47f6-85fc-7ce04d8a3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_densities_and_2nd_peaks(df, df_mframe, df_mframe_selected, print_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86aee6-c41f-4958-a844-f131c31cbdc6",
   "metadata": {},
   "source": [
    "# check for densities and possible second returns two major frames around where high-quality lakes were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393dc1d-05c0-4267-9dcd-3a59c761ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# check for densities and possible second returns two mframes around where passing quality lakes were detected\n",
    "num_additional = 1\n",
    "count = 0\n",
    "while (num_additional > 0) & (count<20):\n",
    "    count+=1\n",
    "    selected = df_mframe[df_mframe['lake_qual_pass']].index \n",
    "    lst1 = np.unique(list(selected-2) + list(selected-1) + list(selected+1) + list(selected+2))\n",
    "    lst2 = df_mframe.index\n",
    "    inter = list(set(lst1) & set(lst2))\n",
    "    df_include_surrounding = df_mframe.loc[inter]\n",
    "    # df_include_surrounding = df_mframe.loc[np.unique(list(selected-2) + list(selected-1) + list(selected+1) + list(selected+2))]\n",
    "    df_additional = df_include_surrounding[~df_include_surrounding['has_densities']]\n",
    "    num_additional = len(df_additional)\n",
    "    if num_additional > 0:\n",
    "        get_densities_and_2nd_peaks(df, df_mframe, df_additional, print_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2005c3-4413-49bf-b7f7-bc7f184642d2",
   "metadata": {},
   "source": [
    "# merge detected lakes iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860985b-e037-4688-88a9-7c21ac8bf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_dist_mframes = 7\n",
    "max_dist_elev = 0.1\n",
    "df_mframe.sort_index(inplace=True)\n",
    "start_mframe = np.array(df_mframe.index[df_mframe['lake_qual_pass']],dtype=int)\n",
    "stop_mframe = np.array(df_mframe.index[df_mframe['lake_qual_pass']],dtype=int)\n",
    "surf_elevs = np.array(df_mframe['peak'][df_mframe['lake_qual_pass']])\n",
    "n_lakes = len(surf_elevs)\n",
    "n_lakes_old = n_lakes + 1\n",
    "iteration = 0\n",
    "\n",
    "debug = False\n",
    "\n",
    "while n_lakes < n_lakes_old:\n",
    "    \n",
    "    start_mframe_old = start_mframe\n",
    "    stop_mframe_old = stop_mframe\n",
    "    surf_elevs_old = surf_elevs\n",
    "    n_lakes_old = n_lakes\n",
    "    start_mframe = []\n",
    "    stop_mframe = []\n",
    "    surf_elevs = []\n",
    "    \n",
    "    last_merged = True\n",
    "    for i in range(n_lakes - 1):\n",
    "        is_closeby = ((start_mframe_old[i + 1] - stop_mframe_old[i]) <= max_dist_mframes)\n",
    "        is_at_same_elevation = (np.abs(surf_elevs_old[i + 1] - surf_elevs_old[i]) < max_dist_elev)\n",
    "        if debug: \n",
    "            print('%10d %10d diff: %4d, close: %5s, %7.2f %7.2f diff: %7.2f, same: %5s' % \\\n",
    "                 (stop_mframe_old[i], start_mframe_old[i + 1], start_mframe_old[i + 1] - stop_mframe_old[i],\n",
    "                  is_closeby, surf_elevs_old[i], surf_elevs_old[i + 1], np.abs(surf_elevs_old[i + 1] - surf_elevs_old[i]),\n",
    "                  is_at_same_elevation), end=' ')\n",
    "        \n",
    "        # if merging\n",
    "        if (is_closeby & is_at_same_elevation):\n",
    "            \n",
    "            if last_merged == False:\n",
    "                start_mframe[-1] = start_mframe_old[i]\n",
    "                stop_mframe[-1] = stop_mframe_old[i + 1]\n",
    "                surf_elevs[-1] = (surf_elevs_old[i] + surf_elevs_old[i+1]) / 2\n",
    "            else: \n",
    "                start_mframe.append(start_mframe_old[i])\n",
    "                stop_mframe.append(stop_mframe_old[i + 1])\n",
    "                surf_elevs.append((surf_elevs_old[i] + surf_elevs_old[i+1]) / 2)\n",
    "            if debug:\n",
    "                print('--> merge')\n",
    "            last_merged = True\n",
    "        \n",
    "        # if keeping \n",
    "        elif (iteration > 0):\n",
    "            \n",
    "            if i==0:\n",
    "                start_mframe.append(start_mframe_old[i])\n",
    "                stop_mframe.append(stop_mframe_old[i])\n",
    "                surf_elevs.append(surf_elevs_old[i])\n",
    "            \n",
    "            start_mframe.append(start_mframe_old[i+1])\n",
    "            stop_mframe.append(stop_mframe_old[i+1])\n",
    "            surf_elevs.append(surf_elevs_old[i+1])\n",
    "            if debug:\n",
    "                print('--> keep')\n",
    "            last_merged = False\n",
    "    \n",
    "    iteration += 1\n",
    "    n_lakes = len(surf_elevs)\n",
    "    print('iteration %3d, number of lakes: %4d' % (iteration, n_lakes))\n",
    "    \n",
    "df_extracted_lakes = pd.DataFrame({'mframe_start': start_mframe, 'mframe_end': stop_mframe, 'surf_elev': surf_elevs})\n",
    "df_extracted_lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b1d7c-b44d-40e0-a304-d67314af8024",
   "metadata": {},
   "source": [
    "# check for adjacient peaks at surface elevation to extend lake further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca77d6-4e0b-49e2-a988-6e39efa54764",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_check = 3\n",
    "elev_tol = 0.2\n",
    "for i in range(len(df_extracted_lakes)):\n",
    "    \n",
    "    thislake = df_extracted_lakes.iloc[i]\n",
    "    thiselev = thislake['surf_elev']\n",
    "    \n",
    "    # check for extending before\n",
    "    extent_before = thislake['mframe_start']\n",
    "    check_before = extent_before - 1\n",
    "    left_to_check = n_check\n",
    "    while left_to_check > 0:\n",
    "        # print('check!')\n",
    "        if np.abs(df_mframe['peak'].loc[check_before] - thiselev) < elev_tol:\n",
    "            extent_before = check_before\n",
    "            left_to_check = n_check\n",
    "            print('extend before')\n",
    "        else:\n",
    "            left_to_check -= 1\n",
    "            \n",
    "        check_before -= 1\n",
    "        \n",
    "    df_extracted_lakes['mframe_start'].iloc[i] = extent_before\n",
    "    \n",
    "    # check for extending after\n",
    "    extent_after = thislake['mframe_end']\n",
    "    check_after = extent_after + 1\n",
    "    left_to_check = n_check\n",
    "    while left_to_check > 0:\n",
    "        # print('check!')\n",
    "        if np.abs(df_mframe['peak'].loc[check_after] - thiselev) < elev_tol:\n",
    "            extent_after = check_after\n",
    "            left_to_check = n_check\n",
    "            print('extend after')\n",
    "        else:\n",
    "            left_to_check -= 1\n",
    "            \n",
    "        check_after += 1\n",
    "        \n",
    "    df_extracted_lakes['mframe_end'].iloc[i] = extent_after\n",
    "\n",
    "# expand each lake by two major frames\n",
    "df_extracted_lakes['mframe_start'] -= 2\n",
    "df_extracted_lakes['mframe_end'] += 2\n",
    "df_extracted_lakes\n",
    "    # for mf in check_before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1174ac-27be-40e9-94a5-00fabb3639a1",
   "metadata": {},
   "source": [
    "# run the SNR algorithm and second peak finding for the remaining major frames within lake boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c15c14-2b26-4578-b346-2b9d00d9319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_to_calculate_densities = []\n",
    "\n",
    "for i in range(len(df_extracted_lakes)):\n",
    "    \n",
    "    thislake = df_extracted_lakes.iloc[i]\n",
    "    extent_start = thislake['mframe_start']\n",
    "    extent_end = thislake['mframe_end']\n",
    "    \n",
    "    dfs_to_calculate_densities.append(df_mframe[(df_mframe.index >= extent_start) & (df_mframe.index <= extent_end)])\n",
    "    \n",
    "df_to_calculate_densities = pd.concat(dfs_to_calculate_densities)\n",
    "df_to_calculate_densities = df_to_calculate_densities[~df_to_calculate_densities['has_densities']]\n",
    "\n",
    "get_densities_and_2nd_peaks(df, df_mframe, df_to_calculate_densities, print_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793027c-8cc4-4bba-9b23-c0996cab72df",
   "metadata": {},
   "source": [
    "# plot for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cce58-faeb-4341-919c-bb6195da49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "for i in range(len(df_extracted_lakes)):\n",
    "# for i in [2]:\n",
    "    \n",
    "    thislake = df_extracted_lakes.iloc[i]\n",
    "    thiselev = thislake['surf_elev']\n",
    "    extent_start = thislake['mframe_start']\n",
    "    extent_end = thislake['mframe_end']\n",
    "    \n",
    "    # subset the dataframes to the current lake extent\n",
    "    df_lake = df[(df['mframe'] >= extent_start) & (df['mframe'] <= extent_end)]\n",
    "    df_mframe_lake = df_mframe[(df_mframe.index >= extent_start) & (df_mframe.index <= extent_end)]\n",
    "    h_2nds = [v for l in list(df_mframe_lake['h_2nd_returns']) for v in l]\n",
    "    xatc_2nds = [v for l in list(df_mframe_lake['xatc_2nd_returns']) for v in l]\n",
    "    prom_2nds = [v for l in list(df_mframe_lake['proms_2nd_returns']) for v in l]\n",
    "    \n",
    "    # get statistics\n",
    "    # average mframe quality summary excluding the two mframes for buffer on each side\n",
    "    lake_quality = np.sum(df_mframe_lake['quality_summary']) / (len(df_mframe_lake) - 4)\n",
    "    lake_time = convert_time_to_string(df_mframe_lake['dt'].mean())\n",
    "    lake_lat = df_mframe_lake['lat'].mean()\n",
    "    lake_lat_min = df_mframe_lake['lat'].min()\n",
    "    lake_lat_max = df_mframe_lake['lat'].max()\n",
    "    lake_lat_str = '%.5f°N'%(lake_lat) if lake_lat>=0 else '%.5f°S'%(-lake_lat)\n",
    "    lake_lon = df_mframe_lake['lon'].mean()\n",
    "    lake_lon_min = df_mframe_lake['lon'].min()\n",
    "    lake_lon_max = df_mframe_lake['lon'].max()\n",
    "    lake_lon_str = '%.5f°E'%(lake_lon) if lake_lon>=0 else '%.5f°W'%(-lake_lon)\n",
    "    lake_gtx = lakes[thisLake].gtx\n",
    "    lake_track = ancillary['rgt']\n",
    "    lake_cycle = ancillary['cycle_number']\n",
    "    lake_beam_strength = ancillary['gtx_strength_dict'][lake_gtx]\n",
    "    lake_minh = np.min((df_mframe_lake['peak'].min(), np.min(h_2nds)))\n",
    "    h_range = thiselev - lake_minh\n",
    "    lake_maxh = np.min((df_mframe_lake['peak'].max(), thiselev+5*h_range))\n",
    "    buffer_top = np.max((0.2*h_range, 1.0))\n",
    "    buffer_bottom = np.max((0.3*h_range, 2.0))\n",
    "    lake_minh_plot = lake_minh - buffer_bottom\n",
    "    lake_maxh_plot = lake_maxh + buffer_top\n",
    "    \n",
    "    fig = plt.figure(figsize=[9, 5], dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "       \n",
    "    ax.scatter(df_lake.xatc-df_lake.xatc.min(), df_lake.h, s=6, c='k', alpha=0.05, edgecolors='none')\n",
    "    scatt = ax.scatter(df_lake.xatc-df_lake.xatc.min(), df_lake.h, s=3, c=df_lake.snr, alpha=1, edgecolors='none',\n",
    "                       cmap=cmc.lajolla,vmin=0,vmax=1)\n",
    "                        \n",
    "    # plot surface elevation\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ax.plot([xmin, xmax], [thiselev, thiselev], 'g-', lw=0.5)\n",
    "    \n",
    "    # plot mframe bounds\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    mframe_bounds_xatc = list(df_mframe_lake['xatc_min']) + [df_mframe_lake['xatc_max'].iloc[-1]]\n",
    "    for xmframe in mframe_bounds_xatc:\n",
    "        ax.plot([xmframe-df_lake.xatc.min(), xmframe-df_lake.xatc.min()], [ymin, ymax], 'k-', lw=0.5)\n",
    "\n",
    "    dfpass = df_mframe_lake[df_mframe_lake['lake_qual_pass']]\n",
    "    dfnopass = df_mframe_lake[~df_mframe_lake['lake_qual_pass']]\n",
    "    ax.plot(dfpass.xatc-df_lake.xatc.min(), dfpass.peak, marker='o', mfc='g', mec='g', linestyle = 'None', ms=5)\n",
    "    ax.plot(dfnopass.xatc-df_lake.xatc.min(), dfnopass.peak, marker='o', mfc='none', mec='r', linestyle = 'None', ms=3)\n",
    "\n",
    "    for i,prom in enumerate(prom_2nds):\n",
    "        ax.plot(xatc_2nds[i]-df_lake.xatc.min(), h_2nds[i], marker='o', mfc='none', mec='b', linestyle = 'None', ms=prom*4)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='4%', pad=0.05)\n",
    "    fig.colorbar(scatt, cax=cax, orientation='vertical')\n",
    "    \n",
    "    ax.set_ylim((lake_minh_plot, lake_maxh_plot))\n",
    "    ax.set_xlim((0.0, df_mframe_lake['xatc_max'].iloc[-1] - df_lake.xatc.min()))\n",
    "    \n",
    "    ax.set_title('Lake at (%s, %s) on %s\\nICESat-2 track %d %s (%s), cycle %d [lake quality: %.2f]' % \\\n",
    "                 (lake_lat_str, lake_lon_str, lake_time, lake_track, lake_gtx,lake_beam_strength, lake_cycle, lake_quality))\n",
    "    ax.set_ylabel('elevation above geoid [m]')\n",
    "    ax.set_xlabel('along-track distance [m]')\n",
    "    \n",
    "    # ax.text(0.99,0.01,'lat')\n",
    "    url = 'https://openaltimetry.org/data/api/icesat2/atlXX?'\n",
    "    url += 'date={date}&minx={minx}&miny={miny}&maxx={maxx}&maxy={maxy}&trackId={track}&beamName={beam}'.format(\n",
    "            date=lake_time[:10],minx=lake_lon_min,miny=lake_lat_min,maxx=lake_lon_max,maxy=lake_lat_max,track=lake_track,beam=lake_gtx)\n",
    "    url += '&outputFormat=json&client=jupyter'\n",
    "    print('oaurl = \\'%s\\'' % url)\n",
    "    \n",
    "    # save figure\n",
    "    fig_dir = 'figs/lakes_found_test/'\n",
    "    if not os.path.exists(fig_dir): os.makedirs(fig_dir)\n",
    "    epoch = df_mframe_lake['dt'].mean() + datetime.datetime.timestamp(datetime.datetime(2018,1,1))\n",
    "    dateid = datetime.datetime.fromtimestamp(epoch).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    latid = '%dN'%(int(np.round(lake_lat*1e5))) if lake_lat>=0 else '%dS'%(-int(np.round(lake_lat*1e5)))\n",
    "    lonid = '%dE'%(int(np.round(lake_lon*1e5))) if lake_lon>=0 else '%dS'%(-int(np.round(lake_lon*1e5)))\n",
    "    figname = fig_dir + 'lake_found_%s%s_%d_%s_%s-%s.jpg' % (lake_track, lake_gtx, lake_cycle, dateid, latid, lonid)\n",
    "    plt.savefig(figname, dpi=600, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830327de-34aa-4019-9dd9-152801af6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r figs/lakes_found_test.zip figs/lakes_found_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708d858-6bac-44a6-b999-2274c9a9130a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172d98e-5228-4128-94c6-5d8a01a58d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26289482-cafd-49c0-821a-aaddac935db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepyx-env",
   "language": "python",
   "name": "icepyx-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "039ba52a-d58a-4a09-a3bd-924892489471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IS2ML_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "af9f97e9-b748-43fd-a541-b1041c66f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stats for all lakes\n",
    "fn_lakestats = '../GLD3_GrIS_lakestats.csv'\n",
    "df = pd.read_csv(fn_lakestats)\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(df, crs='EPSG:3413')\n",
    "gdfg = gdf[gdf.label=='good_lake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "625d0069-264c-45b2-97dc-3f8a5339d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where the data lives (this is from the FLUID-SuRRF output file structure)\n",
    "# need to adjust the base_dir\n",
    "kwargs = {\n",
    "    'base_dir': '/Users/parndt/jupyterprojects/GLD3_complete/GrIS/',  # base directory where the fluid surrf data lives\n",
    "    'data_dir': 'detection_out_data/',                                # the directory with the data files (structure from output)\n",
    "    'ground_track_buffer': 7.5,          # radius in meters, for a 11-m diameter footprint and some allowance for geolocation uncertainty\n",
    "    'max_cloudiness': 20,                # 20 seems to be good, sometimes over lakes the cloudScore thinks there are clouds\n",
    "    'days_buffer': 7,                    # limit to +/- one week from ICESat-2 acquisition, can later filter for NDWI overlap / correlation with band reflectances\n",
    "    'min_sun_elevation': 20,             # 20 is commonly used, beelow that data often has issues\n",
    "    'limit_n_imgs': 20,                  # limit the total number of images queried, ground track points with missing data after this are discarded\n",
    "    'ndwi_thresh': 0.125,                 # this threshold is used to calculate NDWI overlap between ICESat-2 and Sentinel-2 water extent\n",
    "    're_calculate_if_existing': False, # re-do calculations even if file already exists\n",
    "    'FLUID_SuRRF_info': gdfg\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061476a0-3f6b-4382-830b-c11fef09e942",
   "metadata": {},
   "source": [
    "Only uncomment and run the code below when doing a full new run! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ffc64-025e-412c-b9a6-dd65a30122f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "#### for a clean, new run: clear the output data (except for metadata files from Sentinel-2)\n",
    "############################################################################################################\n",
    "# folders_to_reset = ['training_data_CSVs', 'imagery_gt', 'atl03_segments']\n",
    "# for folder in folders_to_reset:\n",
    "#     ! rm -rf $folder\n",
    "#     os.makedirs(folder)\n",
    "#     open('%s/.gitkeep' % folder,'a').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354ce84-daf3-410c-97ce-830b750c597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_outputs = True\n",
    "# iterate = gdfg.lake_id\n",
    "# for ilk, lake_id in enumerate(iterate):\n",
    "#     if (ilk < (len(iterate)-1)) and clear_outputs and (ilk % 5 == 0):\n",
    "#         clear_output(wait=True)\n",
    "#     print('\\n_________________________________________________ lake %5i / %5i _________________________________________________\\n' % (ilk+1, len(gdfg)))\n",
    "#     try:\n",
    "#         fig_path, lk, gdf_final, lk_info = get_data_and_plot(lake_id=lake_id, **kwargs)\n",
    "#     except:\n",
    "#         warnings.warn('Getting data failed for lake %i! (%s)' % (ilk+1, lake_id))\n",
    "#         traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b8a81-efc6-4cd5-9226-51df006f52d8",
   "metadata": {},
   "source": [
    "# Clean up data and compile into parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "789136f9-c5cc-4788-bf8a-357fd23241e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number *_lakeinfo.csv: 6952\n",
      "number *.csv: 6952\n"
     ]
    }
   ],
   "source": [
    "# compile all data together\n",
    "base_dir = 'training_data_CSVs'\n",
    "\n",
    "if base_dir[-1] != '/':\n",
    "    base_dir += '/'\n",
    "    \n",
    "searchdir = base_dir\n",
    "searchfor = '_lakeinfo.csv'\n",
    "filelist_info = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f)]\n",
    "filelist_info.sort()\n",
    "listlength_info = len(filelist_info)\n",
    "print('number *_lakeinfo.csv:', listlength_info)\n",
    "\n",
    "searchfor = '.csv'\n",
    "exclude = '_lakeinfo.csv'\n",
    "filelist = [searchdir+f for f in os.listdir(searchdir) \\\n",
    "            if os.path.isfile(os.path.join(searchdir, f)) & (searchfor in f) & (exclude not in f)]\n",
    "filelist.sort()\n",
    "listlength = len(filelist)\n",
    "print('number *.csv:', listlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "81880460-d7bf-4f6d-88df-6d5a00332bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling info\n",
      " 6952 /  6952\n",
      "\n",
      "compiling data\n",
      " 6952 /  6952\n",
      "\n",
      " --> done.\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "print('compiling info')\n",
    "for ifn, fninfo in enumerate(filelist_info):\n",
    "    print('%5i / %5i' % (ifn+1, listlength_info), end='\\r')\n",
    "    dfs.append(pd.read_csv(fninfo))\n",
    "dfinfo = pd.concat(dfs)\n",
    "\n",
    "dfs = []\n",
    "print('\\n\\ncompiling data')\n",
    "for ifn, fn in enumerate(filelist):\n",
    "    print('%5i / %5i' % (ifn+1, listlength_info), end='\\r')\n",
    "    dfs.append(pd.read_csv(fn))\n",
    "df_data = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "print('\\n\\n --> done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dcfa8cfe-aa68-4d2d-87dc-438006868feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore specific warnings\n",
    "import scipy\n",
    "warnings.filterwarnings(\"ignore\", category=scipy.stats.ConstantInputWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"divide by zero encountered in log\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
    "\n",
    "df_save = df_data.copy()\n",
    "df_saveinfo = dfinfo.copy()\n",
    "\n",
    "# give matching id columns the same name\n",
    "df_saveinfo = df_saveinfo.rename(columns={'lake_id': 'IS2_id'})\n",
    "\n",
    "# drop unnecessary elevation info from IS2\n",
    "df_save = df_save.drop(columns=['h_fit_surf', 'h_fit_bed'])\n",
    "\n",
    "# remove nan data \n",
    "df_save = df_save[~df_save.isna().any(axis=1)].reset_index(drop=True)\n",
    "\n",
    "# cast surface classification to uint8\n",
    "df_save.SCL = df_save.SCL.astype(np.uint8)\n",
    "\n",
    "# filter out confidence values lower than 0.05 so nobody grossly abuses data that can't be trusted\n",
    "conf_thresh = 0.05\n",
    "df_save = df_save[df_save.conf > conf_thresh]\n",
    "\n",
    "def norma(x):\n",
    "    xn = x - np.nanmin(x)\n",
    "    return xn / np.nanmax(xn)\n",
    "\n",
    "def check_finite(arr1, arr2):\n",
    "    return ~(np.isinf(arr1) | np.isnan(arr1) | np.isinf(arr2) | np.isnan(arr2))\n",
    "\n",
    "def get_matching_stats(col, ndwi_thresh=0.125, min_valid_values=10, conf_thresh=conf_thresh):\n",
    "\n",
    "    min_fin = min_valid_values\n",
    "    \n",
    "    ndwi_match = (col.depth > 0) == (col.ndwi > ndwi_thresh)\n",
    "    fin = check_finite(col.depth, col.ndwi)\n",
    "    mat = ndwi_match & fin\n",
    "    ndwi_match_perc = 0.0 if (np.sum(mat) < min_fin) else ndwi_match.mean() * 100\n",
    "\n",
    "    ndwi_match_depth = (col.depth > 0) & (col.ndwi > ndwi_thresh)\n",
    "    is_depth = col.depth > 0\n",
    "    fin = check_finite(col.depth, col.ndwi)\n",
    "    mat = ndwi_match_depth & fin\n",
    "    ndwi_match_perc_depth = 0.0 if (np.sum(mat) < min_fin) else ndwi_match_depth.sum() / is_depth.sum() * 100\n",
    "    \n",
    "    ndwi_match_norm = (col.depth > 0) == (norma(col.ndwi) > ndwi_thresh)\n",
    "    fin = check_finite(col.depth, norma(col.ndwi))\n",
    "    mat = ndwi_match & fin\n",
    "    ndwi_match_perc_norm = 0.0 if (np.sum(mat) < min_fin) else ndwi_match_norm.mean() * 100\n",
    "\n",
    "    ndwi_match_perc_max = np.nanmax((ndwi_match_perc, ndwi_match_perc_norm, 0.0))\n",
    "\n",
    "    ndwi_match = ndwi_match_depth & (col.conf >= conf_thresh) # use only values where ICESat-2 sees a water depth\n",
    "    bg_avg = norma(np.mean(np.vstack((norma(np.log(col.B2)), norma(np.log(col.B3)))), axis=0))\n",
    "    fin = check_finite(col.depth, bg_avg)\n",
    "    mat = ndwi_match & fin\n",
    "    Rsquared_log_B2B3 = -1.0 if (np.sum(fin) < min_fin) else pearsonr(-col.depth[fin], bg_avg[fin]).statistic\n",
    "    Rsquared_log_B2B3_match = -1.0 if (np.sum(mat) < min_fin) else pearsonr(-col.depth[mat], bg_avg[mat]).statistic\n",
    "    fin = check_finite(col.depth, np.log(col.B2))\n",
    "    mat = ndwi_match & fin\n",
    "    Rsquared_log_B2 = -1.0 if (np.sum(fin) < min_fin) else pearsonr(-col.depth[fin], np.log(col.B2)[fin]).statistic\n",
    "    Rsquared_log_B2_match = -1.0 if (np.sum(mat) < min_fin) else pearsonr(-col.depth[mat], np.log(col.B2)[mat]).statistic\n",
    "    fin = check_finite(col.depth, np.log(col.B3))\n",
    "    mat = ndwi_match & fin\n",
    "    Rsquared_log_B3 = -1.0 if (np.sum(fin) < min_fin) else pearsonr(-col.depth[fin], np.log(col.B3)[fin]).statistic\n",
    "    Rsquared_log_B3_match = -1.0 if (np.sum(mat) < min_fin) else pearsonr(-col.depth[mat], np.log(col.B3)[mat]).statistic\n",
    "    fin = check_finite(col.depth, np.log(col.B4))\n",
    "    mat = ndwi_match & fin\n",
    "    Rsquared_log_B4 = -1.0 if (np.sum(fin) < min_fin) else pearsonr(-col.depth[fin], np.log(col.B4)[fin]).statistic\n",
    "    Rsquared_log_B4_match = -1.0 if (np.sum(mat) < min_fin) else pearsonr(-col.depth[mat], np.log(col.B4)[mat]).statistic\n",
    "    Rsquared_max_full = np.nanmax((Rsquared_log_B2, Rsquared_log_B3, Rsquared_log_B4, Rsquared_log_B2B3, -1.0))\n",
    "    Rsquared_max_match = np.nanmax((Rsquared_log_B2_match, Rsquared_log_B3_match, Rsquared_log_B4_match, Rsquared_log_B2B3_match, -1.0))\n",
    "    Rsquared_max = np.nanmax((Rsquared_max_full, Rsquared_max_match, -1.0))\n",
    "\n",
    "    return pd.Series({\n",
    "        'ndwi_thresh': ndwi_thresh,\n",
    "        'ndwi_match_perc': ndwi_match_perc,\n",
    "        'ndwi_match_perc_norm': ndwi_match_perc_norm,\n",
    "        'ndwi_match_perc_depth': ndwi_match_perc_depth,\n",
    "        'ndwi_match_perc_max': ndwi_match_perc_max,\n",
    "        'Rsquared_log_B2B3': Rsquared_log_B2B3,\n",
    "        'Rsquared_log_B2B3_match': Rsquared_log_B2B3_match,\n",
    "        'Rsquared_log_B2': Rsquared_log_B2,\n",
    "        'Rsquared_log_B2_match': Rsquared_log_B2_match,\n",
    "        'Rsquared_log_B3': Rsquared_log_B3,\n",
    "        'Rsquared_log_B3_match': Rsquared_log_B3_match,\n",
    "        'Rsquared_log_B4': Rsquared_log_B4,\n",
    "        'Rsquared_log_B4_match': Rsquared_log_B4_match,\n",
    "        'Rsquared_max_full': Rsquared_max_full,\n",
    "        'Rsquared_max_match': Rsquared_max_match,\n",
    "        'Rsquared_max': Rsquared_max,\n",
    "    })\n",
    "\n",
    "is2groups = df_save.groupby('IS2_id')\n",
    "df_info_new = is2groups.apply(get_matching_stats)\n",
    "df_saveinfo = df_saveinfo.drop(columns=[\n",
    "    'ndwi_match_perc',\n",
    "    'ndwi_match_perc_norm',\n",
    "    'ndwi_match_perc_max',\n",
    "    'ndwi_thresh',\n",
    "    'Rsquared_log_B2B3',\n",
    "    'Rsquared_log_B2B3_match',\n",
    "    'Rsquared_log_B2',\n",
    "    'Rsquared_log_B2_match',\n",
    "    'Rsquared_log_B3',\n",
    "    'Rsquared_log_B3_match',\n",
    "    'Rsquared_log_B4',\n",
    "    'Rsquared_log_B4_match',\n",
    "    'Rsquared_max_full',\n",
    "    'Rsquared_max_match',\n",
    "    'Rsquared_max']).set_index('IS2_id').join(df_info_new)\n",
    "\n",
    "is_zero_depth = is2groups['depth'].max() == 0.0\n",
    "not_zero_depth_indices = is_zero_depth[~is_zero_depth].index\n",
    "not_zero_depth = df_save.apply(lambda x: x.IS2_id in not_zero_depth_indices, axis=1)\n",
    "df_save = df_save[not_zero_depth].reset_index(drop=True)\n",
    "\n",
    "# remove any info for lake segments that have been fully removed (just in case)\n",
    "is2ids = np.unique(df_save.IS2_id)\n",
    "is_in_data = df_saveinfo.apply(lambda x: x.name in is2ids, axis=1)\n",
    "df_saveinfo = df_saveinfo[is_in_data]\n",
    "\n",
    "# sort by estimated ICESat-2 quality\n",
    "df_save['depth_quality_sort'] = df_save.apply(lambda x: df_saveinfo.loc[x.IS2_id,'depth_quality_sort'], axis=1)\n",
    "df_save = df_save.sort_values(by=['depth_quality_sort', 'IS2_id', 'xatc'], ascending=[False, True, True])\n",
    "df_saveinfo = df_saveinfo.sort_values(by='depth_quality_sort', ascending=False)\n",
    "\n",
    "df_saveinfo = df_saveinfo[[\n",
    "    'ice_sheet',\n",
    "    'melt_season',\n",
    "    'date_time',\n",
    "    'lon',\n",
    "    'lat',\n",
    "    'surface_elevation',\n",
    "    'depth_quality_sort',\n",
    "    'max_depth',\n",
    "    'cycle_number',\n",
    "    'rgt',\n",
    "    'gtx',\n",
    "    'beam_strength',\n",
    "    'beam_number',\n",
    "    'sc_orient',\n",
    "    'granule_id',\n",
    "    'polygon_name',\n",
    "    'lat_max',\n",
    "    'lat_min',\n",
    "    'lon_max',\n",
    "    'lon_min',\n",
    "    'conf_mean',\n",
    "    'conf_median',\n",
    "    'conf_q90',\n",
    "    'length_water',\n",
    "    'total_length',\n",
    "    'depth_mean',\n",
    "    'depth_median',\n",
    "    'ratio_water',\n",
    "    'ndwi_thresh',\n",
    "    'ndwi_match_perc',\n",
    "    'ndwi_match_perc_norm',\n",
    "    'ndwi_match_perc_depth',\n",
    "    'ndwi_match_perc_max',\n",
    "    'Rsquared_log_B2B3',\n",
    "    'Rsquared_log_B2B3_match',\n",
    "    'Rsquared_log_B2',\n",
    "    'Rsquared_log_B2_match',\n",
    "    'Rsquared_log_B3',\n",
    "    'Rsquared_log_B3_match',\n",
    "    'Rsquared_log_B4',\n",
    "    'Rsquared_log_B4_match',\n",
    "    'Rsquared_max_full',\n",
    "    'Rsquared_max_match',\n",
    "    'Rsquared_max',\n",
    "    'datetime_S2',\n",
    "    'tdiff_str',\n",
    "    'tdiff_sec_mean',\n",
    "    'tdiff_sec_min',\n",
    "    'tdiff_sec_max',\n",
    "    'n_scenes',\n",
    "    'basin',\n",
    "    'sub_basin',\n",
    "    'glacier_type',\n",
    "    'S2_id_first'\n",
    "]]\n",
    "\n",
    "df_save = df_save.drop(columns=['depth_quality_sort']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44920f70-cb16-420c-aff0-2d187c2a13de",
   "metadata": {},
   "source": [
    "## save to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e4d7595e-73c1-4725-b1ef-205f99f5da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> saved file as training_data_gris/lake_segments_depth_reflectance_S2.parquet\n",
      "\n",
      "--> saved file as training_data_gris/lake_segment_properties.parquet\n",
      "\n",
      "--> saved file as training_data_gris/lake_segment_properties.csv\n",
      "\n",
      "--> saved file as training_data_gris/lake_segments_depth_reflectance_S2.csv\n"
     ]
    }
   ],
   "source": [
    "fn_out_info = 'training_data_gris/lake_segment_properties.csv'\n",
    "fn_out_data = 'training_data_gris/lake_segments_depth_reflectance_S2.csv'\n",
    "\n",
    "import pyarrow as pa\n",
    "fn_out_data_pq = fn_out_data.replace('.csv', '.parquet')\n",
    "SCHEMA = pa.schema([\n",
    "    ('lat', pa.float32()),\n",
    "    ('lon', pa.float32()),\n",
    "    ('xatc', pa.uint16()),\n",
    "    ('depth', pa.float32()),\n",
    "    ('conf', pa.float32()),\n",
    "    ('B1', pa.float32()),\n",
    "    ('B2', pa.float32()),\n",
    "    ('B3', pa.float32()),\n",
    "    ('B4', pa.float32()),\n",
    "    ('B5', pa.float32()),\n",
    "    ('B6', pa.float32()),\n",
    "    ('B7', pa.float32()),\n",
    "    ('B8', pa.float32()),\n",
    "    ('B8A', pa.float32()),\n",
    "    ('B9', pa.float32()),\n",
    "    ('B11', pa.float32()),\n",
    "    ('B12', pa.float32()),\n",
    "    ('AOT', pa.float32()),\n",
    "    ('WVP', pa.float32()),\n",
    "    ('SCL', pa.uint8()),\n",
    "    ('MSK_CLDPRB', pa.float32()),\n",
    "    ('MSK_SNWPRB', pa.float32()),\n",
    "    ('cloudScore', pa.float32()),\n",
    "    ('ndwi', pa.float32()),\n",
    "    ('SZA', pa.float32()),\n",
    "    ('SAA', pa.float32()),\n",
    "    ('VZA_B1', pa.float32()),\n",
    "    ('VZA_B2', pa.float32()),\n",
    "    ('VZA_B3', pa.float32()),\n",
    "    ('VZA_B4', pa.float32()),\n",
    "    ('VZA_B5', pa.float32()),\n",
    "    ('VZA_B6', pa.float32()),\n",
    "    ('VZA_B7', pa.float32()),\n",
    "    ('VZA_B8', pa.float32()),\n",
    "    ('VZA_B8A', pa.float32()),\n",
    "    ('VZA_B9', pa.float32()),\n",
    "    ('VZA_B11', pa.float32()),\n",
    "    ('VZA_B12', pa.float32()),\n",
    "    ('VAA_B1', pa.float32()),\n",
    "    ('VAA_B2', pa.float32()),\n",
    "    ('VAA_B3', pa.float32()),\n",
    "    ('VAA_B4', pa.float32()),\n",
    "    ('VAA_B5', pa.float32()),\n",
    "    ('VAA_B6', pa.float32()),\n",
    "    ('VAA_B7', pa.float32()),\n",
    "    ('VAA_B8', pa.float32()),\n",
    "    ('VAA_B8A', pa.float32()),\n",
    "    ('VAA_B9', pa.float32()),\n",
    "    ('VAA_B11', pa.float32()),\n",
    "    ('VAA_B12', pa.float32()),\n",
    "    ('tdiff_sec', pa.uint32()),\n",
    "    ('plot_id', pa.uint16()),\n",
    "    ('S2_id', pa.string()),\n",
    "    ('IS2_id', pa.string()),\n",
    "])\n",
    "df_save.to_parquet(fn_out_data_pq, schema=SCHEMA)\n",
    "print('\\n--> saved file as %s' % fn_out_data_pq)\n",
    "\n",
    "fn_out_info_pq = fn_out_info.replace('.csv', '.parquet')\n",
    "df_saveinfo.to_parquet(fn_out_info_pq)\n",
    "print('\\n--> saved file as %s' % fn_out_info_pq)\n",
    "\n",
    "df_saveinfo.to_csv(fn_out_info)\n",
    "print('\\n--> saved file as %s' % fn_out_info)\n",
    "df_save.to_csv(fn_out_data, index=False)\n",
    "print('\\n--> saved file as %s' % fn_out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080acfff-3b45-47a9-ab5d-91773619085c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeicelakes-env",
   "language": "python",
   "name": "eeicelakes-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

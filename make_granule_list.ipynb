{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20db82b2-14e2-41bf-9120-76a4784859f3",
   "metadata": {},
   "source": [
    "# Make Granule List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d93a001c-dbf8-4a15-bade-8a059b1e1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility function for making granule list\n",
    "from icelakes.nsidc import make_granule_list\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef7f3d5e-d7b6-4584-8e36-d047038c7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to convert shapefile to geojson - if needed\n",
    "# shp2geojson_nsidc('shapefiles/jakobshavn_small.shp')\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry.polygon import orient\n",
    "\n",
    "\n",
    "lons = [-154, -153]\n",
    "lats = [-85.46, -85.41]\n",
    "\n",
    "coords = [(lons[x[0]], lats[x[1]]) for x in [(0,0), (1,0), (1,1), (0,1), (0,0)]]\n",
    "poly = Polygon(coords)\n",
    "gdf = gpd.GeoDataFrame(geometry=[poly], crs='EPSG:4326') \n",
    "fn_gjsn = 'geojsons/test_ross.geojson'\n",
    "gdf.to_file(fn_gjsn, driver='GeoJSON')\n",
    "outname_list = fn_gjsn.split('/')[-1].replace('.geojson','.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92f59a5a-4a52-460e-8ee5-098b74f5159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-154, -85.46),\n",
       " (-153, -85.46),\n",
       " (-153, -85.41),\n",
       " (-154, -85.41),\n",
       " (-154, -85.46)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "429121f9-9bbb-4db7-97f1-106d13f05a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 ATL03 version 006 granules over test_ross.geojson between 2022-01-04 and 2022-01-04.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>granule</th>\n",
       "      <th>geojson</th>\n",
       "      <th>description</th>\n",
       "      <th>geojson_clip</th>\n",
       "      <th>size_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL03_20220104191501_02061411_006_01.h5</td>\n",
       "      <td>geojsons/test_ross.geojson</td>\n",
       "      <td>AIS_2021-22_test_ross</td>\n",
       "      <td>geojsons/test_ross.geojson</td>\n",
       "      <td>3532.847979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   granule                     geojson  \\\n",
       "0  ATL03_20220104191501_02061411_006_01.h5  geojsons/test_ross.geojson   \n",
       "\n",
       "             description                geojson_clip      size_mb  \n",
       "0  AIS_2021-22_test_ross  geojsons/test_ross.geojson  3532.847979  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = make_granule_list(fn_gjsn.split('/')[-1], start_date='2022-01-04', end_date='2022-01-04', \n",
    "                       icesheet='AIS', meltseason='2021-22', list_out_name=outname_list,\n",
    "                       version=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10414a0f-a67e-4f19-bc23-9335fc32d1c5",
   "metadata": {},
   "source": [
    "# Greenland 2019 - June 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0fa54-ab3d-4b62-ba0d-fcf9e4d87e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist = []\n",
    "filename_out = 'granule_lists/GRE_2000_May2019_Jun2023.csv'\n",
    "\n",
    "startyear = 2019\n",
    "endyear = 2023\n",
    "startday = '05-15'\n",
    "endday = '09-15'\n",
    "icesheet = 'GrIS'\n",
    "\n",
    "searchfor = 'simplified_GRE_2000'\n",
    "gjsn_dir = 'geojsons'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "print('____________________________________________________________________________')\n",
    "print('GREENLAND')\n",
    "print('____________________________________________________________________________')\n",
    "\n",
    "for i, gjsn in enumerate(gjsn_list):\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(geojson, '(', i+1, '/', len(gjsn_list), ')')\n",
    "    for yr in np.arange(startyear, endyear+1):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        df = make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(df)\n",
    "\n",
    "df_all = pd.concat(dflist)\n",
    "df_all.loc[:, 'description'] = df_all.apply(lambda x: x.loc['description'].replace('simplified_','').replace('GRE_','').replace('ANT_',''), axis=1)\n",
    "df_all.reset_index(inplace=True, drop=True)\n",
    "df_all.to_csv(filename_out.replace('.csv', '_size.csv'), header=False, index=False)\n",
    "\n",
    "print('Number of ganules over Greenland:', np.sum(df_all.apply(lambda x: 'GrIS' in x.loc['description'], axis=1)))\n",
    "print('Number of ganules over Antarctica:', np.sum(df_all.apply(lambda x: 'AIS' in x.loc['description'], axis=1)))\n",
    "print('Total number of granules:', len(df_all))\n",
    "maxrow = df_all.loc[np.argmax(df_all.size_mb),:]\n",
    "print('Largest granule: %.1f GB, %s, %s' % (maxrow.size_mb/1000, maxrow.granule, maxrow.geojson))\n",
    "print('Total size: %.2f TB' % (np.sum(df_all.size_mb)/1e6))\n",
    "\n",
    "df_all_nosize = df_all.drop(columns='size_mb').copy()\n",
    "df_all_nosize.to_csv(filename_out, header=False, index=False)\n",
    "df_all_nosize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c43d20-ed7d-447a-80be-3bd4cffb418d",
   "metadata": {},
   "source": [
    "# Antarctica 2018/19 - 2020/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592ac93-2a18-4e98-9b2c-a24debb53089",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist = []\n",
    "filename_out = 'granule_lists/ANT_1000_Dec2018_Mar2021.csv'\n",
    "\n",
    "startyear = 2018\n",
    "endyear = 2021\n",
    "startday = '12-01'\n",
    "endday = '03-01'\n",
    "icesheet = 'AIS'\n",
    "\n",
    "searchfor = 'simplified_ANT_1000'\n",
    "gjsn_dir = 'geojsons'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "\n",
    "print('____________________________________________________________________________')\n",
    "print('ANTARCTICA 2018/19 - 2020/21')\n",
    "print('____________________________________________________________________________')\n",
    "for i, gjsn in enumerate(gjsn_list):\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(geojson, '(', i+1, '/', len(gjsn_list), ')')\n",
    "    for yr in np.arange(startyear, endyear):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr+1, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        df = make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(df)\n",
    "\n",
    "df_all = pd.concat(dflist)\n",
    "df_all.loc[:, 'description'] = df_all.apply(lambda x: x.loc['description'].replace('simplified_','').replace('GRE_','').replace('ANT_',''), axis=1)\n",
    "df_all.reset_index(inplace=True, drop=True)\n",
    "df_all.to_csv(filename_out.replace('.csv', '_size.csv'), header=False, index=False)\n",
    "\n",
    "print('Number of ganules over Greenland:', np.sum(df_all.apply(lambda x: 'GrIS' in x.loc['description'], axis=1)))\n",
    "print('Number of ganules over Antarctica:', np.sum(df_all.apply(lambda x: 'AIS' in x.loc['description'], axis=1)))\n",
    "print('Total number of granules:', len(df_all))\n",
    "maxrow = df_all.loc[np.argmax(df_all.size_mb),:]\n",
    "print('Largest granule: %.1f GB, %s, %s' % (maxrow.size_mb/1000, maxrow.granule, maxrow.geojson))\n",
    "print('Total size: %.2f TB' % (np.sum(df_all.size_mb)/1e6))\n",
    "\n",
    "df_all_nosize = df_all.drop(columns='size_mb').copy()\n",
    "df_all_nosize.to_csv(filename_out, header=False, index=False)\n",
    "df_all_nosize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b96c84-9b5e-45a4-9119-82790753fa8f",
   "metadata": {},
   "source": [
    "# Antarctica 2021/22 - 2022/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdbe018-f535-4564-8f45-252812ddfabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist = []\n",
    "filename_out = 'granule_lists/ANT_1000_Dec2021_Mar2023.csv'\n",
    "\n",
    "startyear = 2021\n",
    "endyear = 2023\n",
    "startday = '12-01'\n",
    "endday = '03-01'\n",
    "icesheet = 'AIS'\n",
    "\n",
    "searchfor = 'simplified_ANT_1000'\n",
    "gjsn_dir = 'geojsons'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "\n",
    "print('____________________________________________________________________________')\n",
    "print('ANTARCTICA 2021/22 - 2022/23')\n",
    "print('____________________________________________________________________________')\n",
    "for i, gjsn in enumerate(gjsn_list):\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(geojson, '(', i+1, '/', len(gjsn_list), ')')\n",
    "    for yr in np.arange(startyear, endyear):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr+1, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        df = make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(df)\n",
    "\n",
    "df_all = pd.concat(dflist)\n",
    "df_all.loc[:, 'description'] = df_all.apply(lambda x: x.loc['description'].replace('simplified_','').replace('GRE_','').replace('ANT_',''), axis=1)\n",
    "df_all.reset_index(inplace=True, drop=True)\n",
    "df_all.to_csv(filename_out.replace('.csv', '_size.csv'), header=False, index=False)\n",
    "\n",
    "print('Number of ganules over Greenland:', np.sum(df_all.apply(lambda x: 'GrIS' in x.loc['description'], axis=1)))\n",
    "print('Number of ganules over Antarctica:', np.sum(df_all.apply(lambda x: 'AIS' in x.loc['description'], axis=1)))\n",
    "print('Total number of granules:', len(df_all))\n",
    "maxrow = df_all.loc[np.argmax(df_all.size_mb),:]\n",
    "print('Largest granule: %.1f GB, %s, %s' % (maxrow.size_mb/1000, maxrow.granule, maxrow.geojson))\n",
    "print('Total size: %.2f TB' % (np.sum(df_all.size_mb)/1e6))\n",
    "\n",
    "df_all_nosize = df_all.drop(columns='size_mb').copy()\n",
    "df_all_nosize.to_csv(filename_out, header=False, index=False)\n",
    "df_all_nosize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c5ae7-1b09-472d-9739-c452fe17f95d",
   "metadata": {},
   "source": [
    "# greenland extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3583c-c5f8-492e-be2a-37dc7dbd8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_out = 'granule_lists/GRE_2000_extraMaySep.csv'\n",
    "dflist = []\n",
    "\n",
    "startyear = 2019\n",
    "endyear = 2023\n",
    "icesheet = 'GrIS'\n",
    "searchfor = 'simplified_GRE_2000'\n",
    "gjsn_dir = 'geojsons'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "print('____________________________________________________________________________')\n",
    "print('GREENLAND EXTRA')\n",
    "print('____________________________________________________________________________')\n",
    "\n",
    "startday = '05-01'\n",
    "endday = '05-14'\n",
    "for i, gjsn in enumerate(gjsn_list):\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(geojson, '(', i+1, '/', len(gjsn_list), ')')\n",
    "    for yr in np.arange(startyear, endyear+1):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        df = make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(df)\n",
    "\n",
    "startday = '09-16'\n",
    "endday = '09-30'\n",
    "for i, gjsn in enumerate(gjsn_list):\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(geojson, '(', i+1, '/', len(gjsn_list), ')')\n",
    "    for yr in np.arange(startyear, endyear+1):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        df = make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(df)\n",
    "\n",
    "df_all = pd.concat(dflist)\n",
    "df_all.loc[:, 'description'] = df_all.apply(lambda x: x.loc['description'].replace('simplified_','').replace('GRE_','').replace('ANT_',''), axis=1)\n",
    "df_all.reset_index(inplace=True, drop=True)\n",
    "df_all.to_csv(filename_out.replace('.csv', '_size.csv'), header=False, index=False)\n",
    "\n",
    "print('Number of ganules over Greenland:', np.sum(df_all.apply(lambda x: 'GrIS' in x.loc['description'], axis=1)))\n",
    "print('Number of ganules over Antarctica:', np.sum(df_all.apply(lambda x: 'AIS' in x.loc['description'], axis=1)))\n",
    "print('Total number of granules:', len(df_all))\n",
    "maxrow = df_all.loc[np.argmax(df_all.size_mb),:]\n",
    "print('Largest granule: %.1f GB, %s, %s' % (maxrow.size_mb/1000, maxrow.granule, maxrow.geojson))\n",
    "print('Total size: %.2f TB' % (np.sum(df_all.size_mb)/1e6))\n",
    "\n",
    "df_all_nosize = df_all.drop(columns='size_mb').copy()\n",
    "df_all_nosize.to_csv(filename_out, header=False, index=False)\n",
    "df_all_nosize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae89200-6c77-4e58-8042-3be285b0bdb3",
   "metadata": {},
   "source": [
    "# antarctica extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb047bc-3341-45be-9591-7a1c77047512",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist = []\n",
    "filename_out = 'granule_lists/ANT_1000_extraNovMar.csv'\n",
    "\n",
    "startyear = 2018\n",
    "endyear = 2023\n",
    "icesheet = 'AIS'\n",
    "searchfor = 'simplified_ANT_1000'\n",
    "gjsn_dir = 'geojsons'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "\n",
    "print('____________________________________________________________________________')\n",
    "print('ANTARCTICA EXTRA')\n",
    "print('____________________________________________________________________________')\n",
    "\n",
    "startday = '11-01'\n",
    "endday = '11-30'\n",
    "for i, gjsn in enumerate(gjsn_list):\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(geojson, '(', i+1, '/', len(gjsn_list), ')')\n",
    "    for yr in np.arange(startyear, endyear):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        df = make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(df)\n",
    "\n",
    "startday = '03-01'\n",
    "endday = '03-31'\n",
    "for i, gjsn in enumerate(gjsn_list):\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(geojson, '(', i+1, '/', len(gjsn_list), ')')\n",
    "    for yr in np.arange(startyear, endyear):\n",
    "        start_date = '%s-%s' % (yr+1, startday)\n",
    "        end_date = '%s-%s' % (yr+1, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        df = make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(df)\n",
    "\n",
    "df_all = pd.concat(dflist)\n",
    "df_all.loc[:, 'description'] = df_all.apply(lambda x: x.loc['description'].replace('simplified_','').replace('GRE_','').replace('ANT_',''), axis=1)\n",
    "df_all.reset_index(inplace=True, drop=True)\n",
    "df_all.to_csv(filename_out.replace('.csv', '_size.csv'), header=False, index=False)\n",
    "\n",
    "print('Number of ganules over Greenland:', np.sum(df_all.apply(lambda x: 'GrIS' in x.loc['description'], axis=1)))\n",
    "print('Number of ganules over Antarctica:', np.sum(df_all.apply(lambda x: 'AIS' in x.loc['description'], axis=1)))\n",
    "print('Total number of granules:', len(df_all))\n",
    "maxrow = df_all.loc[np.argmax(df_all.size_mb),:]\n",
    "print('Largest granule: %.1f GB, %s, %s' % (maxrow.size_mb/1000, maxrow.granule, maxrow.geojson))\n",
    "print('Total size: %.2f TB' % (np.sum(df_all.size_mb)/1e6))\n",
    "\n",
    "df_all_nosize = df_all.drop(columns='size_mb').copy()\n",
    "df_all_nosize.to_csv(filename_out, header=False, index=False)\n",
    "df_all_nosize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f664f895-b6f9-474e-b6e9-2762dd808ed2",
   "metadata": {},
   "source": [
    "# combine both ANT and GRE extra job inputs for shoulder season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce181e-09c2-45ff-8b5f-aaac76a99ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('granule_lists/GRE_2000_extraMaySep.csv',header=None)\n",
    "df2 = pd.read_csv('granule_lists/ANT_1000_extraNovMar.csv',header=None)\n",
    "dfb = pd.concat((df1,df2))\n",
    "dfb.to_csv('granule_lists/extra_shoulderseason_GRE_2000_ANT_1000.csv', header=False, index=False)\n",
    "\n",
    "nms = ['granule', 'geojson', 'description', 'geojson_clip', 'size_mb']\n",
    "df1 = pd.read_csv('granule_lists/GRE_2000_extraMaySep_size.csv',header=None,names=nms)\n",
    "df2 = pd.read_csv('granule_lists/ANT_1000_extraNovMar_size.csv',header=None,names=nms)\n",
    "dfb = pd.concat((df1,df2)).reset_index(drop=True)\n",
    "dfb.to_csv('granule_lists/extra_shoulderseason_GRE_2000_ANT_1000_size.csv', header=False, index=False)\n",
    "\n",
    "print('Number of ganules over Greenland:', np.sum(dfb.apply(lambda x: 'GrIS' in x.loc['description'], axis=1)))\n",
    "print('Number of ganules over Antarctica:', np.sum(dfb.apply(lambda x: 'AIS' in x.loc['description'], axis=1)))\n",
    "print('Total number of granules:', len(dfb))\n",
    "maxrow = dfb.loc[np.argmax(dfb.size_mb),:]\n",
    "print('Largest granule: %.1f GB, %s, %s' % (maxrow.size_mb/1000, maxrow.granule, maxrow.geojson))\n",
    "print('Total size: %.2f TB' % (np.sum(dfb.size_mb)/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e27a66-c378-4c93-b86b-19d485179517",
   "metadata": {},
   "outputs": [],
   "source": [
    "17305+1762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3ec97-11ba-4927-a0c5-9a61da828967",
   "metadata": {},
   "source": [
    "# stats for all combined inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aafec533-8af3-4003-a25b-5019119e6132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ganules over Greenland: 8762\n",
      "Number of ganules over Antarctica: 43052\n",
      "Total number of granules: 51814\n",
      "Largest granule: 12.2 GB, ATL03_20220511191525_07591505_006_01.h5, geojsons/simplified_GRE_2000_NO.geojson\n",
      "Total size: 132.81 TB\n"
     ]
    }
   ],
   "source": [
    "in_list = [\n",
    "    'granule_lists/GRE_2000_May2019_Jun2023.csv',\n",
    "    'granule_lists/ANT_1000_Dec2018_Mar2021.csv',\n",
    "    'granule_lists/ANT_1000_Dec2021_Mar2023.csv',\n",
    "    'granule_lists/extra_shoulderseason_GRE_2000_ANT_1000.csv']\n",
    "nms = ['granule', 'geojson', 'description', 'geojson_clip', 'size_mb']\n",
    "dfs_all_input = []\n",
    "for grlist in in_list:\n",
    "    dfs_all_input.append(pd.read_csv(grlist.replace('.csv', '_size.csv'), header=None, names=nms))\n",
    "df_all_inputs = pd.concat(dfs_all_input).reset_index(drop=True)\n",
    "\n",
    "print('Number of ganules over Greenland:', np.sum(df_all_inputs.apply(lambda x: 'GrIS' in x.loc['description'], axis=1)))\n",
    "print('Number of ganules over Antarctica:', np.sum(df_all_inputs.apply(lambda x: 'AIS' in x.loc['description'], axis=1)))\n",
    "print('Total number of granules:', len(df_all_inputs))\n",
    "maxrow = df_all_inputs.loc[np.argmax(df_all_inputs.size_mb),:]\n",
    "print('Largest granule: %.1f GB, %s, %s' % (maxrow.size_mb/1000, maxrow.granule, maxrow.geojson))\n",
    "print('Total size: %.2f TB' % (np.sum(df_all_inputs.size_mb)/1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819890e-8166-497f-bc81-8fd7d335b0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382409f0-8a57-4c7e-8489-f4ce229ac506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280922a-30ec-4cec-b430-95e19fca4335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d19743-6106-491d-9550-56be5ff5969e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c5e46-9edc-45e2-9ad4-8072bed2eb61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f8df2-08ea-4401-96f6-2c786ad4065c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee4a35-75c0-496f-b1fc-edbbb35e7b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c0240-ba0c-49fa-ba0c-a8831d24dee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40101fe0-a8b7-46f5-af7c-e622ea15528d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee09b33-3a47-43ae-88c0-ddc4cb75309d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e0480-8f48-4fe1-b633-e2a7738b9d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6f16b-4f3d-4998-8ad8-bf85d7775d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_in = 'granule_lists/GRE_2500_ANT_1500_Oct2018_Mar2023.csv'\n",
    "fn_in = 'granule_lists/GRE_2000_ANT_1000_Oct2018_Mar2023.csv'\n",
    "fn_in = 'granule_lists/GRE_2000_ANT_1000_Oct2018_Jun2023.csv'\n",
    "n_granules = 1000\n",
    "\n",
    "df = pd.read_csv(fn_in, header=None)\n",
    "if n_granules == 1:\n",
    "    df_small = df[df.apply(lambda x: 'ATL03_20220714010847' in x.loc[0], axis=1)]\n",
    "else:\n",
    "    idxs = np.random.choice(np.arange(0,len(df)), size=n_granules, replace=False)\n",
    "    df_small = df.loc[idxs, :]\n",
    "    \n",
    "fn_out = fn_in.replace('.csv', '-%i.csv' % n_granules)\n",
    "print(fn_out)\n",
    "\n",
    "df_small.to_csv(fn_out, header=False, index=False)\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ad108-a9de-47be-8a24-769484935ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the longest polygon to check if query code works with it\n",
    "gjsn_dir = 'geojsons'\n",
    "searchfor = 'simplified_GRE_2500'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "\n",
    "searchfor = 'simplified_ANT_1500'\n",
    "gjsn_list += [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "\n",
    "for geojson_filepath in gjsn_list:\n",
    "    gdf = gpd.read_file(geojson_filepath)\n",
    "    poly = orient(gdf.loc[0].geometry,sign=1.0)\n",
    "    polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "    print('%4i'%len(polygon), geojson_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935d0c9-0ffd-4b37-9357-6f30332196e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 detect_lakes.py --granule ATL03_20200302160852_10220610_006_01.h5 --polygon geojsons/simplified_ANT_1500_East_Dp-E.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be39da8-0b39-43bd-8655-9788a2dbaf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = np.linspace(2,8)\n",
    "xp = np.array([])\n",
    "fp = np.sin(xp)\n",
    "x = np.linspace(0,10)\n",
    "len(xp)\n",
    "#np.interp(x, xp, fp, left=np.nan, right=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3e54d-21af-4b5d-9648-0664113be718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of dataframes\n",
    "dflist = []\n",
    "\n",
    "startyear = 2019\n",
    "endyear = 2022\n",
    "startday = '05-01'\n",
    "endday = '09-30'\n",
    "icesheet = 'GrIS'\n",
    "\n",
    "searchfor = 'simplified_GRE_2500_CW'\n",
    "gjsn_dir = 'geojsons'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "\n",
    "for gjsn in gjsn_list:\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "\n",
    "    # gdf = gpd.read_file(gjsn)\n",
    "    # print(gdf.geometry.loc[0].geom_type, geojson)\n",
    "    \n",
    "    for yr in np.arange(startyear, endyear+1):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(pd.read_csv(outname,header=None))\n",
    "\n",
    "df_all = pd.concat(dflist)\n",
    "df_all.loc[:, 3] = df_all.apply(lambda x: x.loc[1].replace('simplified_', ''), axis=1)\n",
    "df_all.loc[:, 2] = df_all.apply(lambda x: x.loc[2].replace('simplified_','').replace('GRE_','').replace('ANT_',''), axis=1)\n",
    "df_all.to_csv('granule_lists/GRE_2500_CW_2019-22_.csv', header=False, index=False)\n",
    "df_all_nosize = df_all.drop(columns='size_mb').copy()\n",
    "df_all_nosize.to_csv('granule_lists/GRE_2000_ANT_1000_Oct2018_Jun2023.csv', header=False, index=False)\n",
    "df_all_nosize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce22efd-94b3-490a-90b5-a6141e4575bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "polygon = 'geojsons/simplified_GRE_2500_CW.geojson'\n",
    "poly_nonsimplified = polygon.replace('simplified_', '')\n",
    "poly_nonsimplified\n",
    "clip_shape = gpd.read_file(poly_nonsimplified)\n",
    "clip_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065350e-58b8-4ff3-8f74-537d3e072051",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('geojsons/simplified_GRE_2500_NO.geojson')\n",
    "poly = orient(gdf.loc[0].geometry,sign=1.0)\n",
    "    \n",
    "#Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482dfd0f-1e72-4d5e-86ca-c1158ece63e4",
   "metadata": {},
   "source": [
    "# Greenland and Antarctica, all regions, Oct 2018 - March 2022\n",
    "\n",
    "For GRE2500/ANT1500:\n",
    "- Number of ganules over Greenland: 9325\n",
    "- Number of ganules over Antarctica: 43790\n",
    "- Total number of granules: 53115\n",
    "- Largest granule: 12.2 GB, ATL03_20220511191525_07591505_006_01.h5, geojsons/simplified_GRE_2500_NW.geojson\n",
    "- Total size: 141.03 TB 3 TB\n",
    "\n",
    "For GRE2000/ANT1000:\n",
    "- Number of ganules over Greenland: 8068\n",
    "- Number of ganules over Antarctica: 39947\n",
    "- Total number of granules: 48015\n",
    "- Largest granule: 12.2 GB, ATL03_20220511191525_07591505_006_01.h5, geojsons/simplified_GRE_2000_NO.geojson\n",
    "- Total size: 128.30 TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792df27-a652-48e7-ab47-0aca2ba2cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list of dataframes\n",
    "dflist = []\n",
    "\n",
    "startyear = 2019\n",
    "endyear = 2023\n",
    "startday = '05-01'\n",
    "endday = '09-30'\n",
    "icesheet = 'GrIS'\n",
    "\n",
    "searchfor = 'simplified_GRE_2000'\n",
    "gjsn_dir = 'geojsons'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "print('____________________________________________________________________________')\n",
    "print('GREENLAND')\n",
    "print('____________________________________________________________________________')\n",
    "\n",
    "for i, gjsn in enumerate(gjsn_list):\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(geojson, '(', i+1, '/', len(gjsn_list), ')')\n",
    "    for yr in np.arange(startyear, endyear+1):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        df = make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(df)\n",
    "\n",
    "startyear = 2018\n",
    "endyear = 2023\n",
    "startday = '11-01'\n",
    "endday = '03-15'\n",
    "icesheet = 'AIS'\n",
    "\n",
    "searchfor = 'simplified_ANT_1000'\n",
    "gjsn_dir = 'geojsons'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "\n",
    "print('____________________________________________________________________________')\n",
    "print('ANTARCTICA')\n",
    "print('____________________________________________________________________________')\n",
    "for i, gjsn in enumerate(gjsn_list):\n",
    "    geojson = gjsn[gjsn.rfind('/')+1:]\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(geojson, '(', i+1, '/', len(gjsn_list), ')')\n",
    "    for yr in np.arange(startyear, endyear):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr+1, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        df = make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(df)\n",
    "    \n",
    "df_all = pd.concat(dflist)\n",
    "df_all.loc[:, 'description'] = df_all.apply(lambda x: x.loc['description'].replace('simplified_','').replace('GRE_','').replace('ANT_',''), axis=1)\n",
    "df_all.reset_index(inplace=True, drop=True)\n",
    "df_all.to_csv('granule_lists/GRE_2000_ANT_1000_Oct2018_Jun2023_size.csv', header=False, index=False)\n",
    "print('Number of ganules over Greenland:', np.sum(df_all.apply(lambda x: 'GrIS' in x.loc['description'], axis=1)))\n",
    "print('Number of ganules over Antarctica:', np.sum(df_all.apply(lambda x: 'AIS' in x.loc['description'], axis=1)))\n",
    "print('Total number of granules:', len(df_all))\n",
    "maxrow = df_all.loc[np.argmax(df_all.size_mb),:]\n",
    "print('Largest granule: %.1f GB, %s, %s' % (maxrow.size_mb/1000, maxrow.granule, maxrow.geojson))\n",
    "print('Total size: %.2f TB' % (np.sum(df_all.size_mb)/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fe728-6b74-4735-a6bc-40735c2ea8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_nosize = df_all.drop(columns='size_mb').copy()\n",
    "df_all_nosize.to_csv('granule_lists/GRE_2000_ANT_1000_Oct2018_Jun2023.csv', header=False, index=False)\n",
    "df_all_nosize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057643d2-e1fa-4557-9cad-4d98970e506a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ce407-0855-46c1-af5d-72834e971e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e7933-01b4-4066-99cc-c5249b97450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loc[:, 'description'] = df_all.apply(lambda x: x.loc['description'].replace('simplified_','').replace('GRE_','').replace('ANT_',''), axis=1)\n",
    "df_all.to_csv('granule_lists/GRE_2500_ANT_1500_Oct2018_Mar2023_size.csv', header=False, index=False)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c68b1d-09f5-498d-9817-cbdc67fd1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('granule_lists/GRE_2500_ANT_1500_Oct2018_Mar2023_size.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac518a-0058-4afa-b4dd-1e878afe600f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78668653-72f3-4d07-a992-63dfcbdd87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da726668-8da2-4128-9fd8-a1332988936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(df_all.size_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5da94-d28f-43b0-8ec3-9a29468acd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loc[np.argmax(df_all.size_mb),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045019b-963f-4514-9428-8491f8bb466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e86538-730e-4ec1-a380-e89ee91df910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8bb71e-c570-4f1b-b602-7dca389a7c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8f421-58ea-4621-9c31-3f3bf6f5e681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c75dc-48a1-44b5-9d8c-7a6d01985a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson = 'jakobshavn_test.geojson'\n",
    "icesheet = 'GrIS'\n",
    "startyear = 2022\n",
    "endyear = 2022\n",
    "startday = '07-14'\n",
    "endday = '07-14'\n",
    "start_date = '%s-%s' % (startyear, startday)\n",
    "end_date = '%s-%s' % (endyear, endday)\n",
    "\n",
    "\n",
    "meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "outname = 'zzz_test006.csv'\n",
    "\n",
    "make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "pd.read_csv(outname,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e9c0a-4bb2-497d-bead-ad5836db2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that it worked by reading the file into a DataFrame and displaying it\n",
    "import pandas as pd\n",
    "pd.read_csv(outname,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9d6f8-cf58-4651-84c3-201c9bfe3be8",
   "metadata": {},
   "source": [
    "# Jakobshavn + Amery + George VI all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990e69b-515b-44a7-95ec-2432cfa3ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters and make the list\n",
    "geojson = 'jakobshavn_test.geojson'\n",
    "icesheet = 'GrIS'\n",
    "startyear = 2019\n",
    "endyear = 2022\n",
    "startday = '05-15'\n",
    "endday = '09-15'\n",
    "\n",
    "\n",
    "meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "\n",
    "make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4717cb-96b2-4c94-abb0-86be43fd8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters and make the list\n",
    "geojson = 'west_greenland.geojson'\n",
    "icesheet = 'GrIS'\n",
    "startyear = 2019\n",
    "endyear = 2022\n",
    "startday = '05-15'\n",
    "endday = '09-15'\n",
    "\n",
    "dflist = []\n",
    "for yr in np.arange(startyear, endyear+1):\n",
    "    start_date = '%s-%s' % (yr, startday)\n",
    "    end_date = '%s-%s' % (yr, endday)\n",
    "    \n",
    "    meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "    outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "\n",
    "    make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "    dflist.append(pd.read_csv(outname,header=None))\n",
    "    \n",
    "geojson1 = 'george_vi.geojson'\n",
    "geojson2 = 'amery.geojson'\n",
    "icesheet = 'AIS'\n",
    "startyear = 2018\n",
    "endyear = 2021\n",
    "startday = '11-15'\n",
    "endday = '03-15'\n",
    "\n",
    "for yr in np.arange(startyear, endyear+1):\n",
    "    start_date = '%s-%s' % (yr, startday)\n",
    "    end_date = '%s-%s' % (yr+1, endday)\n",
    "    \n",
    "    meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "    outname1 = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson1.replace('.geojson','') + '.csv'\n",
    "    outname2 = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson2.replace('.geojson','') + '.csv'\n",
    "    \n",
    "    make_granule_list(geojson1, start_date, end_date, icesheet, meltseason, outname1)\n",
    "    make_granule_list(geojson2, start_date, end_date, icesheet, meltseason, outname2)\n",
    "    \n",
    "    dflist.append(pd.read_csv(outname1,header=None))\n",
    "    dflist.append(pd.read_csv(outname2,header=None))\n",
    "    \n",
    "df_all = pd.concat(dflist)  \n",
    "df_all.to_csv('granule_lists/wais-areas.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441f66d-202d-47d6-acfd-570b2ec67d65",
   "metadata": {},
   "source": [
    "# granule list from failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f2e92-cf2f-4bdf-917d-f6eb3a5f753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('failed_jobs/jobs_failed.csv', header=None)\n",
    "def get_geo(x): \n",
    "    for area in ['amery', 'west_greenland', 'george_vi']:\n",
    "        if area in x: \n",
    "            return 'geojsons/'+area+'.geojson', x[(x.find('job_')+4) : (x.find(area)+len(area))]\n",
    "df['granule'] = df[0].map(lambda x : x[x.find('ATL03') : (x.find('.h5')+3)])\n",
    "df['geo'], df['desc'] = list(zip(*df[0].map(get_geo)))\n",
    "df.drop([0],inplace=True,axis=1)\n",
    "df.to_csv('granule_lists/wais-areas_failed1.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9764710-5db8-454f-8bc5-79d34cd30e39",
   "metadata": {},
   "source": [
    "# granule list for all of WAIS melt regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da34078-d587-470b-bc5e-4884a190e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "icesheet = 'AIS'\n",
    "startyear = 2018\n",
    "endyear = 2021\n",
    "startday = '11-01'\n",
    "endday = '03-15'\n",
    "\n",
    "searchfor = 'waismeltregions'\n",
    "gjsn_dir = 'geojsons'\n",
    "gjsn_list = [gjsn_dir+'/'+f for f in os.listdir(gjsn_dir) \\\n",
    "            if os.path.isfile(os.path.join(gjsn_dir, f)) & (searchfor in f)]\n",
    "\n",
    "dflist = []\n",
    "for gjsn in gjsn_list:\n",
    "    geojson = gjsn[gjsn.find('/')+1:]\n",
    "    for yr in np.arange(startyear, endyear+1):\n",
    "        start_date = '%s-%s' % (yr, startday)\n",
    "        end_date = '%s-%s' % (yr+1, endday)\n",
    "        meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "        outname = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson.replace('.geojson','') + '.csv'\n",
    "        make_granule_list(geojson, start_date, end_date, icesheet, meltseason, outname)\n",
    "        dflist.append(pd.read_csv(outname,header=None))\n",
    "    \n",
    "df_all = pd.concat(dflist)\n",
    "df_all.to_csv('granule_lists/waismeltregions.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d6ad8-9368-4991-9689-3d2b70bda3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['granule'] = 'granule'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5220212-79d1-4f4b-90ee-48d862b44d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cbd496-d5c1-4bb4-b8ed-42f6851028c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c45d58-1c6a-4dae-9c98-58b3aba1d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters and make the list\n",
    "geojson1 = 'george_vi.geojson'\n",
    "geojson2 = 'amery.geojson'\n",
    "icesheet = 'AIS'\n",
    "startyear = 2018\n",
    "endyear = 2021\n",
    "startday = '11-15'\n",
    "endday = '03-15'\n",
    "\n",
    "dflist = []\n",
    "for yr in np.arange(startyear, endyear+1):\n",
    "    start_date = '%s-%s' % (yr, startday)\n",
    "    end_date = '%s-%s' % (yr+1, endday)\n",
    "    print(start_date, end_date)\n",
    "    \n",
    "    meltseason = start_date[:4] if start_date[:4]==end_date[:4] else start_date[:4] + '-' + end_date[2:4]\n",
    "    outname1 = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson1.replace('.geojson','') + '.csv'\n",
    "    outname2 = 'granule_lists/' + icesheet + '_' + meltseason + '_' + geojson2.replace('.geojson','') + '.csv'\n",
    "    \n",
    "    make_granule_list(geojson1, start_date, end_date, icesheet, meltseason, outname1)\n",
    "    make_granule_list(geojson2, start_date, end_date, icesheet, meltseason, outname2)\n",
    "    \n",
    "    dflist.append(pd.read_csv(outname1,header=None))\n",
    "    dflist.append(pd.read_csv(outname2,header=None))\n",
    "    \n",
    "df_all = pd.concat(dflist)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9cf51-478c-4063-a647-d208099f468a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepyx-env",
   "language": "python",
   "name": "icepyx-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
